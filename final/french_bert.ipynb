{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfWiL6vlIX7u"
      },
      "source": [
        "#import every needed Library \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM2KmqWmMbwy",
        "outputId": "232ef623-197b-4b6f-9476-bdcf192af853"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 26.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from langdetect) (1.15.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=18f714baadf659adb58162bcabefc933361735cb8dca4d15589ca290c3017796\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgY4DzJwINO9"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.feature_selection import r_regression\n",
        "\n",
        "# from langdetect import detect\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAmIPsfNIjJ1"
      },
      "source": [
        "#loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQaR5-z6JeN0",
        "outputId": "b84ca447-d342-4960-f7fd-b33a3519ea95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fxak0dQUuC-"
      },
      "outputs": [],
      "source": [
        "CURR_PATH = !pwd\n",
        "# PARAMETERS\n",
        "PATH_DATA = CURR_PATH[0]\n",
        "PATH_GDRIVE_TMP = \"/content/drive/MyDrive/ML/project2\"  # Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyKD1z4KI2sU"
      },
      "source": [
        "##detect language "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2ReFTiYI1nF"
      },
      "outputs": [],
      "source": [
        "def detect2(x):\n",
        "  try:\n",
        "    return detect(x)\n",
        "  except:\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDjXCkjVn2yX"
      },
      "source": [
        "##loading training list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV_au_NiJBZd"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "with open('/content/drive/MyDrive/ML/project2/listings.pkl', 'rb') as f:\n",
        "    data = pickle.load(f) \n",
        "#combine description\n",
        "data['Listing Description'] = data['Listing Title'] + '.' + data['Listing Description']\n",
        "i = data['Listing Description'].isna()\n",
        "data.loc[i,'Listing Description']= data.loc[i,'Listing Title']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmzxpM_FzUFG",
        "outputId": "dcb45ef1-eb46-4316-eb19-79b49d3e5317"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "de    16361\n",
              "fr     3206\n",
              "it      425\n",
              "pt        2\n",
              "hr        1\n",
              "en        1\n",
              "ca        1\n",
              "da        1\n",
              "so        1\n",
              "sl        1\n",
              "Name: Description Langage, dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#language classify\n",
        "data['Description Langage']=data['Listing Description'].apply(lambda x: detect2(x))\n",
        "print(len(data))\n",
        "#save data with language tag\n",
        "pickle.dump(data, open(PATH_GDRIVE_TMP + \"/data_with_language.pkl\", \"wb\"))\n",
        "data['Description Langage'].value_counts ()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQmANUpeZSJ-"
      },
      "source": [
        "###for building model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pCfPVRaJF-g",
        "outputId": "d18e3d44-f079-43d6-fa55-5861ffeba744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fr1: 3206\n",
            "16361\n",
            "fr2: 3195\n",
            "425\n",
            "19981\n"
          ]
        }
      ],
      "source": [
        "#only use description length more than 100 characters\n",
        "data = pickle.load(open(PATH_GDRIVE_TMP+\"/data_with_language.pkl\", \"rb\"))\n",
        "data_de = data.loc[data['Description Langage'] == 'de', (\"Listing Description\",\"Listing Title\",\"Demand\")]\n",
        "data_fr = data.loc[data['Description Langage'] == 'fr', (\"Listing Description\",\"Listing Title\",\"Demand\")]\n",
        "data_it = data.loc[data['Description Langage'] == 'it', (\"Listing Description\",\"Listing Title\",\"Demand\")]\n",
        "print(\"before filter:\",len(data_fr))\n",
        "data_fr = data_fr[data_fr[\"Listing Description\"].apply(len)>100]\n",
        "print(len(data_de))\n",
        "print(\"after filter:\",len(data_fr))\n",
        "print(len(data_it))\n",
        "print(len(data_de)+len(data_fr)+len(data_it))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEV8LtrKCLu3"
      },
      "outputs": [],
      "source": [
        "data_de.columns = ['description', 'title', 'demand']\n",
        "data_fr.columns = ['description', 'title', 'demand']\n",
        "data_it.columns = ['description', 'title', 'demand']\n",
        "df_de, val_data_de = train_test_split(data_de, test_size=0.2, random_state=18)\n",
        "df_fr, val_data_fr = train_test_split(data_fr, test_size=0.2, random_state=18)\n",
        "df_it, val_data_it = train_test_split(data_it, test_size=0.2, random_state=18)\n",
        "# print(df_de.head(2))\n",
        "# print(df_fr.head(2))\n",
        "# print(df_it.head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1T7NXX2nugF"
      },
      "source": [
        "##loading testing listing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kK6YCaentjP"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "# pickle.dump(data, open(PATH_GDRIVE_TMP + \"/data_with_language.pkl\", \"wb\"))\n",
        "with open('/content/drive/MyDrive/ML/project2/test_listings_overall.pkl', 'rb') as f:\n",
        "    data = pickle.load(f) \n",
        "data['Listing Description'] = data['Listing Title'] + '.' + data['Listing Description']\n",
        "i = data['Listing Description'].isna()\n",
        "data.loc[i,'Listing Description']= data.loc[i,'Listing Title']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHQ4NRknKYL3",
        "outputId": "06c659d0-fe64-418d-e52b-d5f054624a73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "de    8211\n",
              "fr    1554\n",
              "it     234\n",
              "Name: Description Langage, dtype: int64"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['Description Langage']=data['Listing Description'].apply(lambda x: detect2(x))\n",
        "print(len(data))\n",
        "pickle.dump(data, open(PATH_GDRIVE_TMP + \"/test_data_with_language.pkl\", \"wb\"))\n",
        "data['Description Langage'].value_counts ()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7px9rtuZYKB"
      },
      "source": [
        "###for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wp7fjneXoyfM",
        "outputId": "39334096-8ddb-46ca-865c-10b048908513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1554\n",
            "8212\n",
            "1554\n",
            "234\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "# data = pickle.load(open(PATH_GDRIVE_TMP+\"/test_data_with_language.pkl\", \"rb\"))\n",
        "# data = pickle.load(open(PATH_GDRIVE_TMP+\"/test_overall_data_with_language.pkl\", \"rb\"))\n",
        "data_de = data.loc[data['Description Langage'] == 'de', (\"Listing Description\",\"Listing Title\",\"Prediction\")]\n",
        "data_fr = data.loc[data['Description Langage'] == 'fr', (\"Listing Description\",\"Listing Title\",\"Prediction\")]\n",
        "data_it = data.loc[data['Description Langage'] == 'it', (\"Listing Description\",\"Listing Title\",\"Prediction\")]\n",
        "print(len(data_fr))\n",
        "data_fr = data_fr[data_fr[\"Listing Description\"].apply(len)>100]\n",
        "print(len(data_de))\n",
        "print(len(data_fr))\n",
        "print(len(data_it))\n",
        "print(len(data_de)+len(data_fr)+len(data_it))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEsN6jYdZaEu"
      },
      "outputs": [],
      "source": [
        "data_de.columns = ['description', 'title', 'Prediction']\n",
        "data_fr.columns = ['description', 'title', 'Prediction']\n",
        "data_it.columns = ['description', 'title', 'Prediction']\n",
        "df_de = data_de\n",
        "df_fr = data_fr\n",
        "df_it = data_it\n",
        "print(df_de.head(2))\n",
        "print(df_fr.head(2))\n",
        "print(df_it.head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXHe-rkCJveI"
      },
      "source": [
        "#data processing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHsLMEXQKaVD"
      },
      "source": [
        "##clean1\n",
        "\n",
        "*   Converting text to lower-case\n",
        "*   Standardising representations of a same entity such as “€”, “euro” and “euros” or “m2” and “m²”,\n",
        "*   Cleaning out certain patterns that are unlikely to be meaningful such as URLs, phone numbers, emails and bank account references."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7v6YY56J2Yh"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def treat_euro(text):\n",
        "    text = re.sub(r'(euro[^s])|(euros)|(€)', ' euros', text)\n",
        "    return text\n",
        "def treat_m2(text):\n",
        "    text = re.sub(r'(m2)|(m²)', ' m²', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7DDEjetKXPD"
      },
      "outputs": [],
      "source": [
        "def filter_ibans(text):\n",
        "    pattern = r'fr\\d{2}[ ]\\d{4}[ ]\\d{4}[ ]\\d{4}[ ]\\d{4}[ ]\\d{2}|fr\\d{20}|fr[ ]\\d{2}[ ]\\d{3}[ ]\\d{3}[ ]\\d{3}[ ]\\d{5}'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "def remove_space_between_numbers(text):\n",
        "    text = re.sub(r'(\\d)\\s+(\\d)', r'\\1\\2', text)\n",
        "    return text\n",
        "def filter_emails(text):\n",
        "    pattern = r'(?:(?!.*?[.]{2})[a-zA-Z0-9](?:[a-zA-Z0-9.+!%-]{1,64}|)|\\\"[a-zA-Z0-9.+!% -]{1,64}\\\")@[a-zA-Z0-9][a-zA-Z0-9.-]+(.[a-z]{2,}|.[0-9]{1,})'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "def filter_ref(text):\n",
        "    pattern = r'(\\(*)(ref|réf)(\\.|[ ])\\d+(\\)*)'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "def filter_websites(text):\n",
        "    pattern = r'(http\\:\\/\\/|https\\:\\/\\/)?([a-z0-9][a-z0-9\\-]*\\.)+[a-z][a-z\\-]*'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "def filter_phone_numbers(text):\n",
        "    pattern = r'(?:(?:\\+|00)33[\\s.-]{0,3}(?:\\(0\\)[\\s.-]{0,3})?|0)[1-9](?:(?:[\\s.-]?\\d{2}){4}|\\d{2}(?:[\\s.-]?\\d{3}){2})|(\\d{2}[ ]\\d{2}[ ]\\d{3}[ ]\\d{3})'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "def filter_tag(text):\n",
        "    pattern = r'<[^>]+>'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBvJf5xpKiZZ"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace(u'\\xa0', u' ')\n",
        "    text = treat_m2(text)\n",
        "    text = treat_euro(text)\n",
        "    text = filter_phone_numbers(text)\n",
        "    text = filter_emails(text)\n",
        "    text = filter_ibans(text)\n",
        "    text = filter_ref(text)\n",
        "    text = filter_websites(text)\n",
        "    text = remove_space_between_numbers(text)\n",
        "    text = filter_tag(text)\n",
        "    return text\n",
        "df_fr['cleaned_description'] = df_fr.description.apply(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hocMNlkgYcTj"
      },
      "source": [
        "##clean2\n",
        "only in case of dealing stop word (which is not necessary in this task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEe7RJQcKknj"
      },
      "source": [
        "###for french"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euUn8_QvYM6x",
        "outputId": "9247d984-6cb4-4ec2-d601-2bce3a7ef3e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "stemmer = SnowballStemmer(\"french\")\n",
        "stop_words = set(stopwords.words(\"french\"))\n",
        "\n",
        "\n",
        "def clean_text_fr(text, for_embedding=False):\n",
        "    \"\"\"\n",
        "        - remove any html tags (< /br> often found)\n",
        "        - Keep only ASCII + European Chars and whitespace, no digits\n",
        "        - remove single letter chars\n",
        "        - convert all whitespaces (tabs etc.) to single wspace\n",
        "        if not for embedding (but e.g. tdf-idf):\n",
        "        - all lowercase\n",
        "        - remove stopwords, punctuation and stemm\n",
        "    \"\"\"\n",
        "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
        "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
        "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
        "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
        "    if for_embedding:\n",
        "        # Keep punctuation\n",
        "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
        "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
        "\n",
        "    text = re.sub(RE_TAGS, \" \", text)\n",
        "    text = re.sub(RE_ASCII, \" \", text)\n",
        "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
        "    text = re.sub(RE_WSPACE, \" \", text)\n",
        "\n",
        "    word_tokens = word_tokenize(text)\n",
        "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
        "\n",
        "    if for_embedding:\n",
        "        # no stemming, lowering and punctuation / stop words removal\n",
        "        words_filtered = word_tokens\n",
        "    else:\n",
        "        words_filtered = [\n",
        "            stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n",
        "        ]\n",
        "\n",
        "    text_clean = \" \".join(words_filtered)\n",
        "    return text_clean\n",
        "\n",
        "df_fr['cleaned_description'] = df_fr.description.apply(clean_text_fr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA6ADupyMSQX",
        "outputId": "103e528b-78af-494c-e2d8-f22de06d9fb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1554"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df_fr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb0BBj4NZIV_"
      },
      "outputs": [],
      "source": [
        "df_fr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTsl8fMh-fjF"
      },
      "source": [
        "###for Italian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVhW1PrS4-nG",
        "outputId": "6745b398-7d66-4c47-ccc2-33c25a820358"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "stemmer = SnowballStemmer(\"italian\")\n",
        "stop_words = set(stopwords.words(\"italian\"))\n",
        "\n",
        "\n",
        "def clean_text_it(text, for_embedding=False):\n",
        "    \"\"\"\n",
        "        - remove any html tags (< /br> often found)\n",
        "        - Keep only ASCII + European Chars and whitespace, no digits\n",
        "        - remove single letter chars\n",
        "        - convert all whitespaces (tabs etc.) to single wspace\n",
        "        if not for embedding (but e.g. tdf-idf):\n",
        "        - all lowercase\n",
        "        - remove stopwords, punctuation and stemm\n",
        "    \"\"\"\n",
        "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
        "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
        "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
        "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
        "    if for_embedding:\n",
        "        # Keep punctuation\n",
        "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
        "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
        "\n",
        "    text = re.sub(RE_TAGS, \" \", text)\n",
        "    text = re.sub(RE_ASCII, \" \", text)\n",
        "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
        "    text = re.sub(RE_WSPACE, \" \", text)\n",
        "\n",
        "    word_tokens = word_tokenize(text)\n",
        "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
        "\n",
        "    if for_embedding:\n",
        "        # no stemming, lowering and punctuation / stop words removal\n",
        "        words_filtered = word_tokens\n",
        "    else:\n",
        "        words_filtered = [\n",
        "            stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n",
        "        ]\n",
        "\n",
        "    text_clean = \" \".join(words_filtered)\n",
        "    return text_clean\n",
        "\n",
        "df_it['cleaned_description'] = df_it.description.apply(clean_text_it)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W6EIQPmW_LQ"
      },
      "source": [
        "###for german"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePf0FD0iXCLF",
        "outputId": "c88c6111-e072-450b-ea33-c97e25e89382"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "stemmer = SnowballStemmer(\"german\")\n",
        "stop_words = set(stopwords.words(\"german\"))\n",
        "\n",
        "def clean_text_de(text, for_embedding=False):\n",
        "    \"\"\"\n",
        "        - remove any html tags (< /br> often found)\n",
        "        - Keep only ASCII + European Chars and whitespace, no digits\n",
        "        - remove single letter chars\n",
        "        - convert all whitespaces (tabs etc.) to single wspace\n",
        "        if not for embedding (but e.g. tdf-idf):\n",
        "        - all lowercase\n",
        "        - remove stopwords, punctuation and stemm\n",
        "    \"\"\"\n",
        "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
        "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
        "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
        "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
        "    if for_embedding:\n",
        "        # Keep punctuation\n",
        "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
        "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
        "\n",
        "    text = re.sub(RE_TAGS, \" \", text)\n",
        "    text = re.sub(RE_ASCII, \" \", text)\n",
        "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
        "    text = re.sub(RE_WSPACE, \" \", text)\n",
        "\n",
        "    word_tokens = word_tokenize(text)\n",
        "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
        "\n",
        "    if for_embedding:\n",
        "        # no stemming, lowering and punctuation / stop words removal\n",
        "        words_filtered = word_tokens\n",
        "    else:\n",
        "        words_filtered = [\n",
        "            stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n",
        "        ]\n",
        "\n",
        "    text_clean = \" \".join(words_filtered)\n",
        "    return text_clean\n",
        "\n",
        "df_de['cleaned_description'] = df_de.description.apply(clean_text_de)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mby9rdOGcJxM"
      },
      "source": [
        "#French model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7z8BAyRKvU8"
      },
      "source": [
        "##Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoET8vZjOSZm"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-OGslPmO4ds"
      },
      "outputs": [],
      "source": [
        "from transformers import CamembertModel, CamembertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbpSLQ8IPWl8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "93393b5ebcca437e98e8768dc1672e48",
            "1d46bc0aaa634a20a5de046c8b06a62e",
            "9391594ee7e4473bb171f43f1cb14bc6",
            "ba6bd4f8692341cda4d7cd9bbd367526",
            "05230496cade47d3ac62d679f7fbe958",
            "f15789be7bfb43f2a1b2d01ed04c196f",
            "b1daf5f445a84090b9f7c5a6d45aa17f",
            "58379656e53d4c80bdda1185cb2998ae",
            "2768e3bc9e8848f19d60a4be2b58f051",
            "76a033af8f44460684b1b79c399c092d",
            "bc14021acf2c46868f2f2ddeb9a5866d",
            "1a61ac57486f4fef853e054f964a5b8e",
            "3903740f4b2a41a0a5a88c6adf1656bd",
            "4320a2f4d9fa4f77a9839738c3b55933",
            "59856030b3bf4279bb66cb2ab50472c4",
            "b431a6fb46d246599cbc14ef9dd293b9",
            "dda0da8d32b24a4c9a7b6af7e7699d58",
            "8147d1c90cce41fabfa00c691836b2fb",
            "af54e8971a9f46c4b1a6fc95aecffa10",
            "9e6221838e764147a0d756c79df54a35",
            "ff051fe525e24ba59bf6cce708cfb48a",
            "e5fec491d4ea432c9471c440d974beb0"
          ]
        },
        "outputId": "841995eb-b6e9-4359-fdeb-54195829ec19"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/811k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93393b5ebcca437e98e8768dc1672e48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/508 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a61ac57486f4fef853e054f964a5b8e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZ-SKp2mKupY"
      },
      "outputs": [],
      "source": [
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "encoded_corpus = tokenizer(text=df_fr.cleaned_description.tolist(),\n",
        "                            add_special_tokens=True,\n",
        "                            padding='max_length',\n",
        "                            truncation='longest_first',\n",
        "                            max_length=300,\n",
        "                            return_attention_mask=True)\n",
        "input_ids = encoded_corpus['input_ids']\n",
        "attention_mask = encoded_corpus['attention_mask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDyIimwLK5Sc",
        "outputId": "35b9a1f5-7ef7-4d88-c3eb-89182e3b6e2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (775 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "def filter_long_descriptions(tokenizer, descriptions, max_len):\n",
        "    indices = []\n",
        "    lengths = tokenizer(descriptions, padding=False, \n",
        "                     truncation=False, return_length=True)['length']\n",
        "    for i in range(len(descriptions)):\n",
        "        if lengths[i] <= max_len-2:\n",
        "            indices.append(i)\n",
        "    return indices\n",
        "short_descriptions = filter_long_descriptions(tokenizer, \n",
        "                               df_fr.cleaned_description.tolist(), 300)\n",
        "input_ids = np.array(input_ids)[short_descriptions]\n",
        "attention_mask = np.array(attention_mask)[short_descriptions]\n",
        "labels = df_fr.demand.to_numpy()[short_descriptions]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH9xCgHELBCS"
      },
      "source": [
        "##Input formatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-tszNYZrMBi"
      },
      "source": [
        "###for train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZQ1MFMOLEBC"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "test_size = 0.1\n",
        "seed = 64\n",
        "train_inputs, test_inputs, train_labels, test_labels = \\\n",
        "            train_test_split(input_ids, labels, test_size=test_size, \n",
        "                             random_state=seed)\n",
        "train_masks, test_masks, _, _ = train_test_split(attention_mask, \n",
        "                                        labels, test_size=test_size, \n",
        "                                        random_state=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IObbtnVirQ35"
      },
      "source": [
        "###for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ_fM2rcrTzT"
      },
      "outputs": [],
      "source": [
        "train_inputs = input_ids\n",
        "train_masks = attention_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMlDBFRF5pWw"
      },
      "source": [
        "##create_dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lcas6k6Rrd0M"
      },
      "source": [
        "###train dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXeaOiBcLMA6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "batch_size = 32\n",
        "def create_dataloaders(inputs, masks, labels, batch_size):\n",
        "    input_tensor = torch.tensor(inputs)\n",
        "    mask_tensor = torch.tensor(masks)\n",
        "    labels_tensor = torch.tensor(labels)\n",
        "    dataset = TensorDataset(input_tensor, mask_tensor, \n",
        "                            labels_tensor)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, \n",
        "                            shuffle=True)\n",
        "    return dataloader\n",
        "train_dataloader = create_dataloaders(train_inputs, train_masks, \n",
        "                                      train_labels, batch_size)\n",
        "test_dataloader = create_dataloaders(test_inputs, test_masks, \n",
        "                                     test_labels, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1cY8rUJriNQ"
      },
      "source": [
        "###test dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSZkyiucrcek"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "batch_size = 32\n",
        "def create_dataloaders(inputs, masks, batch_size):\n",
        "    input_tensor = torch.tensor(inputs)\n",
        "    mask_tensor = torch.tensor(masks)\n",
        "    dataset = TensorDataset(input_tensor, mask_tensor)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, \n",
        "                            shuffle=True)\n",
        "    return dataloader\n",
        "train_dataloader = create_dataloaders(train_inputs, train_masks, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTLKK7qYLQSu"
      },
      "source": [
        "##Implementing the model in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2vWVmIyLXEM",
        "outputId": "8baa7287-9842-4788-d0c2-135186311ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "from transformers import CamembertModel\n",
        "class CamembertRegressor(nn.Module):\n",
        "    \n",
        "    def __init__(self, drop_rate=0.2, freeze_camembert=False):\n",
        "        \n",
        "        super(CamembertRegressor, self).__init__()\n",
        "        D_in, D_out = 768, 1\n",
        "        # D_in2, D_out2 = 1536 ,1\n",
        "        self.camembert = \\\n",
        "                   CamembertModel.from_pretrained('camembert-base')\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Dropout(drop_rate),\n",
        "            nn.Linear(D_in, D_out),\n",
        "            # nn.Sigmoid(),\n",
        "            # nn.Dropout(drop_rate),\n",
        "            # nn.Linear(D_in2, D_out2),\n",
        "            # nn.ReLU()\n",
        "            )\n",
        "    def forward(self, input_ids, attention_masks):\n",
        "        \n",
        "        outputs = self.camembert(input_ids, attention_masks)\n",
        "        class_label_output = outputs[1]\n",
        "        outputs = self.regressor(class_label_output)\n",
        "        return outputs\n",
        "model = CamembertRegressor(drop_rate=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maXTtsQJLb1N"
      },
      "source": [
        "##Setting up the training environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYHqHNYrLaCW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU.\")\n",
        "else:\n",
        "    print(\"No GPU available, using the CPU instead.\")\n",
        "    device = torch.device(\"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3x9KepPLjDv"
      },
      "source": [
        "##Optimizer, scheduler and loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k31eQSR5Lk8f"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "# from torch.optim import RAdam\n",
        "# from torch.optim import SGD\n",
        "# optimizer = SGD(model.parameters(), lr=1e-5)\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=5e-5,\n",
        "                  eps=1e-8)\n",
        "# optimizer = RAdam(model.parameters(),\n",
        "#                   lr=2e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLdQdVENLoVP"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "epochs = 50\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,       \n",
        "                 num_warmup_steps=0, num_training_steps=total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWNH6_3mLq7T"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "loss_function = nn.MSELoss()\n",
        "# loss_function = nn.PoissonNLLLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWtGr2SLLtLb"
      },
      "source": [
        "##Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xDHKBjKLuWJ",
        "outputId": "e335d3f0-aa77-4212-b62c-c01784f94c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "-----\n",
            "step:0, loss:58.077796936035156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-be6f69454809>:20: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  clip_grad_norm(model.parameters(), clip_value)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step:1, loss:80.8313980102539\n",
            "step:2, loss:60.55558776855469\n",
            "step:3, loss:91.92435455322266\n",
            "step:4, loss:96.04283142089844\n",
            "step:5, loss:14.005131721496582\n",
            "step:6, loss:42.97001647949219\n",
            "step:7, loss:50.27099609375\n",
            "step:8, loss:117.72444152832031\n",
            "step:9, loss:215.45034790039062\n",
            "step:10, loss:115.28955841064453\n",
            "step:11, loss:19.153955459594727\n",
            "step:12, loss:17.362031936645508\n",
            "step:13, loss:45.41661834716797\n",
            "step:14, loss:39.768638610839844\n",
            "step:15, loss:13.67514705657959\n",
            "step:16, loss:20.485519409179688\n",
            "step:17, loss:36.66682815551758\n",
            "step:18, loss:40.55812072753906\n",
            "step:19, loss:105.08653259277344\n",
            "step:20, loss:45.955162048339844\n",
            "step:21, loss:23.11153221130371\n",
            "step:22, loss:16.624839782714844\n",
            "step:23, loss:21.738676071166992\n",
            "step:24, loss:135.15516662597656\n",
            "step:25, loss:24.59214973449707\n",
            "step:26, loss:16.41219711303711\n",
            "step:27, loss:12.522788047790527\n",
            "step:28, loss:56.47816467285156\n",
            "step:29, loss:14.219366073608398\n",
            "step:30, loss:29.098634719848633\n",
            "step:31, loss:59.12055969238281\n",
            "step:32, loss:25.298931121826172\n",
            "step:33, loss:25.303730010986328\n",
            "step:34, loss:38.70135498046875\n",
            "step:35, loss:34.97686767578125\n",
            "step:36, loss:9.777029991149902\n",
            "step:37, loss:11.938678741455078\n",
            "step:38, loss:39.46349334716797\n",
            "step:39, loss:21.892269134521484\n",
            "step:40, loss:12.167064666748047\n",
            "step:41, loss:40.594688415527344\n",
            "step:42, loss:27.37404441833496\n",
            "step:43, loss:48.690399169921875\n",
            "step:44, loss:73.1259765625\n",
            "step:45, loss:58.67961502075195\n",
            "step:46, loss:21.579845428466797\n",
            "step:47, loss:11.814247131347656\n",
            "step:48, loss:72.06707000732422\n",
            "step:49, loss:54.535274505615234\n",
            "step:50, loss:19.009170532226562\n",
            "step:51, loss:25.876916885375977\n",
            "step:52, loss:68.10523223876953\n",
            "step:53, loss:18.459680557250977\n",
            "step:54, loss:21.351520538330078\n",
            "step:55, loss:22.956653594970703\n",
            "step:56, loss:29.8447265625\n",
            "step:57, loss:42.40040969848633\n",
            "step:58, loss:12.490036010742188\n",
            "step:59, loss:50.85900115966797\n",
            "step:60, loss:45.31214141845703\n",
            "step:61, loss:15.078031539916992\n",
            "step:62, loss:18.52915382385254\n",
            "step:63, loss:22.07547378540039\n",
            "step:64, loss:14.878459930419922\n",
            "step:65, loss:67.40161895751953\n",
            "step:66, loss:13.349428176879883\n",
            "step:67, loss:179.61703491210938\n",
            "step:68, loss:110.1365966796875\n",
            "step:69, loss:19.33747673034668\n",
            "step:70, loss:54.13258361816406\n",
            "step:71, loss:17.09130859375\n",
            "step:72, loss:26.674610137939453\n",
            "step:73, loss:33.998966217041016\n",
            "step:74, loss:26.09847640991211\n",
            "step:75, loss:30.024974822998047\n",
            "step:76, loss:18.79684829711914\n",
            "1\n",
            "-----\n",
            "step:0, loss:49.048805236816406\n",
            "step:1, loss:60.01909255981445\n",
            "step:2, loss:33.239112854003906\n",
            "step:3, loss:18.214900970458984\n",
            "step:4, loss:31.203943252563477\n",
            "step:5, loss:41.310585021972656\n",
            "step:6, loss:34.030826568603516\n",
            "step:7, loss:232.83135986328125\n",
            "step:8, loss:23.29848289489746\n",
            "step:9, loss:45.230072021484375\n",
            "step:10, loss:31.823287963867188\n",
            "step:11, loss:27.11229705810547\n",
            "step:12, loss:35.03556823730469\n",
            "step:13, loss:74.28045654296875\n",
            "step:14, loss:87.1174545288086\n",
            "step:15, loss:23.8922119140625\n",
            "step:16, loss:33.915122985839844\n",
            "step:17, loss:32.398162841796875\n",
            "step:18, loss:11.24123477935791\n",
            "step:19, loss:14.822154998779297\n",
            "step:20, loss:12.694368362426758\n",
            "step:21, loss:44.57837677001953\n",
            "step:22, loss:14.40250301361084\n",
            "step:23, loss:25.50780487060547\n",
            "step:24, loss:42.88924789428711\n",
            "step:25, loss:30.551284790039062\n",
            "step:26, loss:26.970218658447266\n",
            "step:27, loss:26.51258087158203\n",
            "step:28, loss:23.666942596435547\n",
            "step:29, loss:39.51222229003906\n",
            "step:30, loss:24.065853118896484\n",
            "step:31, loss:11.003748893737793\n",
            "step:32, loss:46.217132568359375\n",
            "step:33, loss:45.85904312133789\n",
            "step:34, loss:169.875244140625\n",
            "step:35, loss:28.592166900634766\n",
            "step:36, loss:85.22503662109375\n",
            "step:37, loss:12.786223411560059\n",
            "step:38, loss:26.971508026123047\n",
            "step:39, loss:26.627958297729492\n",
            "step:40, loss:14.980978965759277\n",
            "step:41, loss:40.11128234863281\n",
            "step:42, loss:61.78486633300781\n",
            "step:43, loss:65.9868392944336\n",
            "step:44, loss:41.82756042480469\n",
            "step:45, loss:14.450386047363281\n",
            "step:46, loss:57.829307556152344\n",
            "step:47, loss:58.273868560791016\n",
            "step:48, loss:22.183387756347656\n",
            "step:49, loss:16.28021240234375\n",
            "step:50, loss:11.64631462097168\n",
            "step:51, loss:13.525911331176758\n",
            "step:52, loss:27.068880081176758\n",
            "step:53, loss:36.14934158325195\n",
            "step:54, loss:40.625221252441406\n",
            "step:55, loss:14.374119758605957\n",
            "step:56, loss:19.626590728759766\n",
            "step:57, loss:26.595726013183594\n",
            "step:58, loss:21.67864227294922\n",
            "step:59, loss:68.6820068359375\n",
            "step:60, loss:21.1959228515625\n",
            "step:61, loss:64.74041748046875\n",
            "step:62, loss:43.89997100830078\n",
            "step:63, loss:96.3349380493164\n",
            "step:64, loss:28.77237319946289\n",
            "step:65, loss:26.25214385986328\n",
            "step:66, loss:33.70962905883789\n",
            "step:67, loss:12.253618240356445\n",
            "step:68, loss:43.949859619140625\n",
            "step:69, loss:41.23906707763672\n",
            "step:70, loss:24.125072479248047\n",
            "step:71, loss:13.467893600463867\n",
            "step:72, loss:36.00596618652344\n",
            "step:73, loss:82.98062133789062\n",
            "step:74, loss:25.118955612182617\n",
            "step:75, loss:26.72508430480957\n",
            "step:76, loss:22.848920822143555\n",
            "2\n",
            "-----\n",
            "step:0, loss:31.948509216308594\n",
            "step:1, loss:20.233360290527344\n",
            "step:2, loss:16.63575553894043\n",
            "step:3, loss:50.045928955078125\n",
            "step:4, loss:18.803544998168945\n",
            "step:5, loss:67.44856262207031\n",
            "step:6, loss:23.357162475585938\n",
            "step:7, loss:40.37834930419922\n",
            "step:8, loss:9.62668228149414\n",
            "step:9, loss:32.09166717529297\n",
            "step:10, loss:26.932374954223633\n",
            "step:11, loss:31.061668395996094\n",
            "step:12, loss:12.549715042114258\n",
            "step:13, loss:52.21396255493164\n",
            "step:14, loss:22.88051986694336\n",
            "step:15, loss:17.325458526611328\n",
            "step:16, loss:21.339677810668945\n",
            "step:17, loss:9.086311340332031\n",
            "step:18, loss:61.612430572509766\n",
            "step:19, loss:21.71408462524414\n",
            "step:20, loss:13.282429695129395\n",
            "step:21, loss:62.841575622558594\n",
            "step:22, loss:25.749252319335938\n",
            "step:23, loss:99.62745666503906\n",
            "step:24, loss:24.418067932128906\n",
            "step:25, loss:16.027830123901367\n",
            "step:26, loss:23.384599685668945\n",
            "step:27, loss:162.3893585205078\n",
            "step:28, loss:84.77159881591797\n",
            "step:29, loss:18.2262020111084\n",
            "step:30, loss:73.09339904785156\n",
            "step:31, loss:18.82998275756836\n",
            "step:32, loss:28.30742835998535\n",
            "step:33, loss:16.038484573364258\n",
            "step:34, loss:30.98965072631836\n",
            "step:35, loss:28.267963409423828\n",
            "step:36, loss:70.23656463623047\n",
            "step:37, loss:43.01713562011719\n",
            "step:38, loss:37.40581130981445\n",
            "step:39, loss:34.41559600830078\n",
            "step:40, loss:17.342655181884766\n",
            "step:41, loss:12.138171195983887\n",
            "step:42, loss:31.951271057128906\n",
            "step:43, loss:23.927902221679688\n",
            "step:44, loss:194.775390625\n",
            "step:45, loss:19.42733383178711\n",
            "step:46, loss:17.456592559814453\n",
            "step:47, loss:29.619140625\n",
            "step:48, loss:58.306556701660156\n",
            "step:49, loss:41.812355041503906\n",
            "step:50, loss:47.594276428222656\n",
            "step:51, loss:39.86192321777344\n",
            "step:52, loss:11.398870468139648\n",
            "step:53, loss:69.90757751464844\n",
            "step:54, loss:36.63365173339844\n",
            "step:55, loss:25.576885223388672\n",
            "step:56, loss:50.701148986816406\n",
            "step:57, loss:23.81462860107422\n",
            "step:58, loss:47.68824005126953\n",
            "step:59, loss:7.398441791534424\n",
            "step:60, loss:32.801055908203125\n",
            "step:61, loss:13.819109916687012\n",
            "step:62, loss:46.28569030761719\n",
            "step:63, loss:17.180198669433594\n",
            "step:64, loss:21.366724014282227\n",
            "step:65, loss:32.621246337890625\n",
            "step:66, loss:44.886680603027344\n",
            "step:67, loss:17.546398162841797\n",
            "step:68, loss:26.57404327392578\n",
            "step:69, loss:13.079150199890137\n",
            "step:70, loss:53.78715896606445\n",
            "step:71, loss:22.44200897216797\n",
            "step:72, loss:124.36626434326172\n",
            "step:73, loss:19.727561950683594\n",
            "step:74, loss:32.58005142211914\n",
            "step:75, loss:21.230281829833984\n",
            "step:76, loss:34.4005126953125\n",
            "3\n",
            "-----\n",
            "step:0, loss:98.0484390258789\n",
            "step:1, loss:57.689552307128906\n",
            "step:2, loss:87.61217498779297\n",
            "step:3, loss:152.1040802001953\n",
            "step:4, loss:37.70315933227539\n",
            "step:5, loss:32.32539749145508\n",
            "step:6, loss:35.544471740722656\n",
            "step:7, loss:55.114742279052734\n",
            "step:8, loss:8.00886344909668\n",
            "step:9, loss:14.837919235229492\n",
            "step:10, loss:79.67711639404297\n",
            "step:11, loss:26.495590209960938\n",
            "step:12, loss:25.757362365722656\n",
            "step:13, loss:59.465248107910156\n",
            "step:14, loss:24.405433654785156\n",
            "step:15, loss:26.971498489379883\n",
            "step:16, loss:19.566883087158203\n",
            "step:17, loss:99.58171081542969\n",
            "step:18, loss:64.26518249511719\n",
            "step:19, loss:17.28898048400879\n",
            "step:20, loss:27.855247497558594\n",
            "step:21, loss:17.55089569091797\n",
            "step:22, loss:31.60308265686035\n",
            "step:23, loss:33.496883392333984\n",
            "step:24, loss:45.51169967651367\n",
            "step:25, loss:41.420982360839844\n",
            "step:26, loss:16.729938507080078\n",
            "step:27, loss:53.1322021484375\n",
            "step:28, loss:18.832828521728516\n",
            "step:29, loss:13.063802719116211\n",
            "step:30, loss:30.56753921508789\n",
            "step:31, loss:28.07030487060547\n",
            "step:32, loss:9.810016632080078\n",
            "step:33, loss:21.574310302734375\n",
            "step:34, loss:21.26217269897461\n",
            "step:35, loss:17.423377990722656\n",
            "step:36, loss:35.46200942993164\n",
            "step:37, loss:30.8710880279541\n",
            "step:38, loss:9.89552116394043\n",
            "step:39, loss:16.90718650817871\n",
            "step:40, loss:18.598037719726562\n",
            "step:41, loss:6.81173849105835\n",
            "step:42, loss:5.7919464111328125\n",
            "step:43, loss:32.97743606567383\n",
            "step:44, loss:25.274024963378906\n",
            "step:45, loss:43.51124572753906\n",
            "step:46, loss:43.56010055541992\n",
            "step:47, loss:97.699462890625\n",
            "step:48, loss:29.09430694580078\n",
            "step:49, loss:31.6669921875\n",
            "step:50, loss:17.447988510131836\n",
            "step:51, loss:19.2828426361084\n",
            "step:52, loss:32.75907897949219\n",
            "step:53, loss:105.99012756347656\n",
            "step:54, loss:16.91499137878418\n",
            "step:55, loss:11.049575805664062\n",
            "step:56, loss:32.18579864501953\n",
            "step:57, loss:17.325807571411133\n",
            "step:58, loss:22.040328979492188\n",
            "step:59, loss:29.612911224365234\n",
            "step:60, loss:21.774059295654297\n",
            "step:61, loss:11.687512397766113\n",
            "step:62, loss:36.89483642578125\n",
            "step:63, loss:53.83751678466797\n",
            "step:64, loss:18.69295883178711\n",
            "step:65, loss:22.824779510498047\n",
            "step:66, loss:22.678157806396484\n",
            "step:67, loss:37.68437957763672\n",
            "step:68, loss:148.4438018798828\n",
            "step:69, loss:20.886390686035156\n",
            "step:70, loss:24.171632766723633\n",
            "step:71, loss:30.256202697753906\n",
            "step:72, loss:19.519514083862305\n",
            "step:73, loss:15.548929214477539\n",
            "step:74, loss:94.54846954345703\n",
            "step:75, loss:17.08414077758789\n",
            "step:76, loss:12.680948257446289\n",
            "4\n",
            "-----\n",
            "step:0, loss:22.086639404296875\n",
            "step:1, loss:30.34876823425293\n",
            "step:2, loss:24.468759536743164\n",
            "step:3, loss:36.16590881347656\n",
            "step:4, loss:49.685546875\n",
            "step:5, loss:22.896244049072266\n",
            "step:6, loss:18.30265235900879\n",
            "step:7, loss:17.203508377075195\n",
            "step:8, loss:43.269691467285156\n",
            "step:9, loss:16.98563003540039\n",
            "step:10, loss:10.017660140991211\n",
            "step:11, loss:28.121793746948242\n",
            "step:12, loss:11.694162368774414\n",
            "step:13, loss:13.948609352111816\n",
            "step:14, loss:35.38022994995117\n",
            "step:15, loss:80.65343475341797\n",
            "step:16, loss:57.00086212158203\n",
            "step:17, loss:29.40898895263672\n",
            "step:18, loss:27.172914505004883\n",
            "step:19, loss:18.46251678466797\n",
            "step:20, loss:11.891069412231445\n",
            "step:21, loss:23.7404727935791\n",
            "step:22, loss:15.228456497192383\n",
            "step:23, loss:29.547168731689453\n",
            "step:24, loss:21.12078094482422\n",
            "step:25, loss:28.686059951782227\n",
            "step:26, loss:12.409431457519531\n",
            "step:27, loss:32.67401123046875\n",
            "step:28, loss:36.91908645629883\n",
            "step:29, loss:12.684978485107422\n",
            "step:30, loss:20.573963165283203\n",
            "step:31, loss:21.928926467895508\n",
            "step:32, loss:20.816041946411133\n",
            "step:33, loss:39.141727447509766\n",
            "step:34, loss:18.801992416381836\n",
            "step:35, loss:39.003448486328125\n",
            "step:36, loss:32.0186767578125\n",
            "step:37, loss:18.93280029296875\n",
            "step:38, loss:84.08197784423828\n",
            "step:39, loss:34.9699592590332\n",
            "step:40, loss:82.06189727783203\n",
            "step:41, loss:32.328086853027344\n",
            "step:42, loss:13.327400207519531\n",
            "step:43, loss:48.34501647949219\n",
            "step:44, loss:39.401344299316406\n",
            "step:45, loss:21.334579467773438\n",
            "step:46, loss:23.403993606567383\n",
            "step:47, loss:23.475570678710938\n",
            "step:48, loss:20.87694549560547\n",
            "step:49, loss:41.42827606201172\n",
            "step:50, loss:61.95078659057617\n",
            "step:51, loss:35.74927520751953\n",
            "step:52, loss:153.3436737060547\n",
            "step:53, loss:25.46946907043457\n",
            "step:54, loss:13.801352500915527\n",
            "step:55, loss:17.447710037231445\n",
            "step:56, loss:13.390094757080078\n",
            "step:57, loss:26.407974243164062\n",
            "step:58, loss:17.421459197998047\n",
            "step:59, loss:213.50233459472656\n",
            "step:60, loss:20.54604148864746\n",
            "step:61, loss:14.329858779907227\n",
            "step:62, loss:28.98686981201172\n",
            "step:63, loss:29.255084991455078\n",
            "step:64, loss:17.959442138671875\n",
            "step:65, loss:36.365501403808594\n",
            "step:66, loss:93.02134704589844\n",
            "step:67, loss:17.686424255371094\n",
            "step:68, loss:48.44325256347656\n",
            "step:69, loss:66.04098510742188\n",
            "step:70, loss:25.593358993530273\n",
            "step:71, loss:20.868385314941406\n",
            "step:72, loss:37.1969108581543\n",
            "step:73, loss:30.517066955566406\n",
            "step:74, loss:78.54779052734375\n",
            "step:75, loss:24.487483978271484\n",
            "step:76, loss:24.404970169067383\n",
            "5\n",
            "-----\n",
            "step:0, loss:33.34082794189453\n",
            "step:1, loss:12.9411039352417\n",
            "step:2, loss:51.9689826965332\n",
            "step:3, loss:11.807236671447754\n",
            "step:4, loss:12.964985847473145\n",
            "step:5, loss:21.282636642456055\n",
            "step:6, loss:52.006370544433594\n",
            "step:7, loss:29.66853141784668\n",
            "step:8, loss:45.968894958496094\n",
            "step:9, loss:16.379596710205078\n",
            "step:10, loss:54.995357513427734\n",
            "step:11, loss:27.850418090820312\n",
            "step:12, loss:23.59433937072754\n",
            "step:13, loss:8.584802627563477\n",
            "step:14, loss:5.533529281616211\n",
            "step:15, loss:15.91988754272461\n",
            "step:16, loss:24.94275665283203\n",
            "step:17, loss:104.08129119873047\n",
            "step:18, loss:18.449420928955078\n",
            "step:19, loss:11.396566390991211\n",
            "step:20, loss:17.711437225341797\n",
            "step:21, loss:155.38558959960938\n",
            "step:22, loss:71.30875396728516\n",
            "step:23, loss:23.26351547241211\n",
            "step:24, loss:16.820520401000977\n",
            "step:25, loss:25.14291763305664\n",
            "step:26, loss:46.13544464111328\n",
            "step:27, loss:68.77549743652344\n",
            "step:28, loss:24.894304275512695\n",
            "step:29, loss:17.633968353271484\n",
            "step:30, loss:45.34571075439453\n",
            "step:31, loss:15.407072067260742\n",
            "step:32, loss:9.032766342163086\n",
            "step:33, loss:70.99055480957031\n",
            "step:34, loss:33.810462951660156\n",
            "step:35, loss:18.617279052734375\n",
            "step:36, loss:15.427623748779297\n",
            "step:37, loss:61.319732666015625\n",
            "step:38, loss:25.197446823120117\n",
            "step:39, loss:28.45852279663086\n",
            "step:40, loss:10.2883882522583\n",
            "step:41, loss:11.693077087402344\n",
            "step:42, loss:19.102121353149414\n",
            "step:43, loss:27.334070205688477\n",
            "step:44, loss:56.147674560546875\n",
            "step:45, loss:24.04905891418457\n",
            "step:46, loss:27.563400268554688\n",
            "step:47, loss:31.313983917236328\n",
            "step:48, loss:28.02166748046875\n",
            "step:49, loss:12.997421264648438\n",
            "step:50, loss:20.782758712768555\n",
            "step:51, loss:8.782264709472656\n",
            "step:52, loss:16.653289794921875\n",
            "step:53, loss:17.01220703125\n",
            "step:54, loss:54.01698303222656\n",
            "step:55, loss:38.711761474609375\n",
            "step:56, loss:49.85029220581055\n",
            "step:57, loss:39.317543029785156\n",
            "step:58, loss:24.098691940307617\n",
            "step:59, loss:36.340450286865234\n",
            "step:60, loss:12.461223602294922\n",
            "step:61, loss:154.9949493408203\n",
            "step:62, loss:9.226714134216309\n",
            "step:63, loss:42.22802734375\n",
            "step:64, loss:14.714385032653809\n",
            "step:65, loss:6.348468780517578\n",
            "step:66, loss:28.210899353027344\n",
            "step:67, loss:6.49283504486084\n",
            "step:68, loss:16.7908935546875\n",
            "step:69, loss:57.289031982421875\n",
            "step:70, loss:25.84619903564453\n",
            "step:71, loss:56.300804138183594\n",
            "step:72, loss:75.99162292480469\n",
            "step:73, loss:39.32960510253906\n",
            "step:74, loss:22.03863525390625\n",
            "step:75, loss:89.76012420654297\n",
            "step:76, loss:22.292741775512695\n",
            "6\n",
            "-----\n",
            "step:0, loss:20.177350997924805\n",
            "step:1, loss:30.157466888427734\n",
            "step:2, loss:15.471160888671875\n",
            "step:3, loss:22.770477294921875\n",
            "step:4, loss:34.945396423339844\n",
            "step:5, loss:20.509639739990234\n",
            "step:6, loss:28.341482162475586\n",
            "step:7, loss:38.80219268798828\n",
            "step:8, loss:16.900272369384766\n",
            "step:9, loss:14.369412422180176\n",
            "step:10, loss:11.683423042297363\n",
            "step:11, loss:30.24005126953125\n",
            "step:12, loss:34.332183837890625\n",
            "step:13, loss:39.667076110839844\n",
            "step:14, loss:40.39002227783203\n",
            "step:15, loss:14.553043365478516\n",
            "step:16, loss:24.247241973876953\n",
            "step:17, loss:20.501888275146484\n",
            "step:18, loss:16.19271469116211\n",
            "step:19, loss:32.97859191894531\n",
            "step:20, loss:12.695398330688477\n",
            "step:21, loss:14.453880310058594\n",
            "step:22, loss:35.012054443359375\n",
            "step:23, loss:13.100235939025879\n",
            "step:24, loss:59.35820770263672\n",
            "step:25, loss:16.81720542907715\n",
            "step:26, loss:7.72403621673584\n",
            "step:27, loss:33.534542083740234\n",
            "step:28, loss:78.98654174804688\n",
            "step:29, loss:13.39434814453125\n",
            "step:30, loss:18.102787017822266\n",
            "step:31, loss:13.996427536010742\n",
            "step:32, loss:53.86949157714844\n",
            "step:33, loss:16.576194763183594\n",
            "step:34, loss:18.04971694946289\n",
            "step:35, loss:14.107303619384766\n",
            "step:36, loss:54.577606201171875\n",
            "step:37, loss:20.357559204101562\n",
            "step:38, loss:18.294389724731445\n",
            "step:39, loss:123.4104995727539\n",
            "step:40, loss:28.764808654785156\n",
            "step:41, loss:16.621110916137695\n",
            "step:42, loss:27.708229064941406\n",
            "step:43, loss:58.70561218261719\n",
            "step:44, loss:175.94227600097656\n",
            "step:45, loss:9.463903427124023\n",
            "step:46, loss:36.482933044433594\n",
            "step:47, loss:12.835969924926758\n",
            "step:48, loss:16.450010299682617\n",
            "step:49, loss:24.208824157714844\n",
            "step:50, loss:43.98078155517578\n",
            "step:51, loss:54.87438201904297\n",
            "step:52, loss:80.30210876464844\n",
            "step:53, loss:9.773076057434082\n",
            "step:54, loss:64.17706298828125\n",
            "step:55, loss:27.114227294921875\n",
            "step:56, loss:25.164077758789062\n",
            "step:57, loss:175.5859832763672\n",
            "step:58, loss:14.836366653442383\n",
            "step:59, loss:71.65835571289062\n",
            "step:60, loss:40.123924255371094\n",
            "step:61, loss:21.53946304321289\n",
            "step:62, loss:19.079566955566406\n",
            "step:63, loss:16.584522247314453\n",
            "step:64, loss:21.166330337524414\n",
            "step:65, loss:18.706539154052734\n",
            "step:66, loss:10.806314468383789\n",
            "step:67, loss:10.756208419799805\n",
            "step:68, loss:33.70499038696289\n",
            "step:69, loss:11.327735900878906\n",
            "step:70, loss:48.506282806396484\n",
            "step:71, loss:22.169147491455078\n",
            "step:72, loss:17.74300765991211\n",
            "step:73, loss:27.179073333740234\n",
            "step:74, loss:16.984498977661133\n",
            "step:75, loss:31.427520751953125\n",
            "step:76, loss:32.39411544799805\n",
            "7\n",
            "-----\n",
            "step:0, loss:32.14854049682617\n",
            "step:1, loss:31.911481857299805\n",
            "step:2, loss:12.465385437011719\n",
            "step:3, loss:13.625393867492676\n",
            "step:4, loss:25.072895050048828\n",
            "step:5, loss:17.709396362304688\n",
            "step:6, loss:9.908167839050293\n",
            "step:7, loss:36.96834182739258\n",
            "step:8, loss:16.65656280517578\n",
            "step:9, loss:14.15402889251709\n",
            "step:10, loss:29.689712524414062\n",
            "step:11, loss:19.00586700439453\n",
            "step:12, loss:25.926105499267578\n",
            "step:13, loss:17.650043487548828\n",
            "step:14, loss:20.9403076171875\n",
            "step:15, loss:11.751625061035156\n",
            "step:16, loss:68.50459289550781\n",
            "step:17, loss:30.268177032470703\n",
            "step:18, loss:16.142314910888672\n",
            "step:19, loss:15.492673873901367\n",
            "step:20, loss:10.89377212524414\n",
            "step:21, loss:55.21863555908203\n",
            "step:22, loss:18.110639572143555\n",
            "step:23, loss:13.005529403686523\n",
            "step:24, loss:12.579663276672363\n",
            "step:25, loss:38.0388069152832\n",
            "step:26, loss:15.035697937011719\n",
            "step:27, loss:9.980101585388184\n",
            "step:28, loss:10.776296615600586\n",
            "step:29, loss:25.253719329833984\n",
            "step:30, loss:19.04800033569336\n",
            "step:31, loss:10.68027114868164\n",
            "step:32, loss:20.53125\n",
            "step:33, loss:95.687255859375\n",
            "step:34, loss:35.502113342285156\n",
            "step:35, loss:28.02188491821289\n",
            "step:36, loss:7.232723712921143\n",
            "step:37, loss:29.48961067199707\n",
            "step:38, loss:29.3259220123291\n",
            "step:39, loss:41.687557220458984\n",
            "step:40, loss:22.896604537963867\n",
            "step:41, loss:90.22516632080078\n",
            "step:42, loss:56.680484771728516\n",
            "step:43, loss:25.66213607788086\n",
            "step:44, loss:10.03883171081543\n",
            "step:45, loss:57.24324417114258\n",
            "step:46, loss:22.050247192382812\n",
            "step:47, loss:21.78018569946289\n",
            "step:48, loss:175.97747802734375\n",
            "step:49, loss:9.377328872680664\n",
            "step:50, loss:38.38412094116211\n",
            "step:51, loss:55.00332260131836\n",
            "step:52, loss:11.999613761901855\n",
            "step:53, loss:13.208481788635254\n",
            "step:54, loss:25.35036849975586\n",
            "step:55, loss:32.946136474609375\n",
            "step:56, loss:10.795177459716797\n",
            "step:57, loss:136.5435333251953\n",
            "step:58, loss:69.69192504882812\n",
            "step:59, loss:23.350181579589844\n",
            "step:60, loss:28.007564544677734\n",
            "step:61, loss:44.678287506103516\n",
            "step:62, loss:19.260452270507812\n",
            "step:63, loss:18.022918701171875\n",
            "step:64, loss:14.896899223327637\n",
            "step:65, loss:26.9583797454834\n",
            "step:66, loss:38.8492546081543\n",
            "step:67, loss:11.151073455810547\n",
            "step:68, loss:21.97344207763672\n",
            "step:69, loss:13.47427749633789\n",
            "step:70, loss:46.814964294433594\n",
            "step:71, loss:19.40088653564453\n",
            "step:72, loss:31.381755828857422\n",
            "step:73, loss:23.883098602294922\n",
            "step:74, loss:18.806371688842773\n",
            "step:75, loss:21.9906063079834\n",
            "step:76, loss:7.191746711730957\n",
            "8\n",
            "-----\n",
            "step:0, loss:7.843857288360596\n",
            "step:1, loss:12.535355567932129\n",
            "step:2, loss:35.32185745239258\n",
            "step:3, loss:65.83869171142578\n",
            "step:4, loss:13.810152053833008\n",
            "step:5, loss:7.389190673828125\n",
            "step:6, loss:23.4547119140625\n",
            "step:7, loss:11.58735179901123\n",
            "step:8, loss:46.423465728759766\n",
            "step:9, loss:19.73196029663086\n",
            "step:10, loss:9.625956535339355\n",
            "step:11, loss:27.022485733032227\n",
            "step:12, loss:16.64628028869629\n",
            "step:13, loss:52.568946838378906\n",
            "step:14, loss:11.014022827148438\n",
            "step:15, loss:24.437217712402344\n",
            "step:16, loss:35.00312042236328\n",
            "step:17, loss:15.993795394897461\n",
            "step:18, loss:33.880149841308594\n",
            "step:19, loss:10.669480323791504\n",
            "step:20, loss:8.565084457397461\n",
            "step:21, loss:14.500190734863281\n",
            "step:22, loss:29.164941787719727\n",
            "step:23, loss:13.668455123901367\n",
            "step:24, loss:32.50055694580078\n",
            "step:25, loss:55.75041580200195\n",
            "step:26, loss:46.616722106933594\n",
            "step:27, loss:12.356586456298828\n",
            "step:28, loss:14.501914978027344\n",
            "step:29, loss:23.230863571166992\n",
            "step:30, loss:6.358484268188477\n",
            "step:31, loss:55.97578811645508\n",
            "step:32, loss:11.836736679077148\n",
            "step:33, loss:3.8208789825439453\n",
            "step:34, loss:139.0017852783203\n",
            "step:35, loss:47.040306091308594\n",
            "step:36, loss:18.479339599609375\n",
            "step:37, loss:57.658512115478516\n",
            "step:38, loss:11.48724365234375\n",
            "step:39, loss:40.66318130493164\n",
            "step:40, loss:9.697301864624023\n",
            "step:41, loss:65.22355651855469\n",
            "step:42, loss:39.11054992675781\n",
            "step:43, loss:25.625675201416016\n",
            "step:44, loss:19.55832862854004\n",
            "step:45, loss:18.873302459716797\n",
            "step:46, loss:19.869016647338867\n",
            "step:47, loss:22.00298309326172\n",
            "step:48, loss:14.496500968933105\n",
            "step:49, loss:16.819007873535156\n",
            "step:50, loss:19.670494079589844\n",
            "step:51, loss:9.892219543457031\n",
            "step:52, loss:19.603477478027344\n",
            "step:53, loss:18.538042068481445\n",
            "step:54, loss:7.026568412780762\n",
            "step:55, loss:26.4600830078125\n",
            "step:56, loss:8.25487995147705\n",
            "step:57, loss:10.154006958007812\n",
            "step:58, loss:49.76154708862305\n",
            "step:59, loss:9.494664192199707\n",
            "step:60, loss:21.837203979492188\n",
            "step:61, loss:17.682918548583984\n",
            "step:62, loss:38.63764953613281\n",
            "step:63, loss:18.259119033813477\n",
            "step:64, loss:22.855459213256836\n",
            "step:65, loss:149.82327270507812\n",
            "step:66, loss:11.52792739868164\n",
            "step:67, loss:19.39620590209961\n",
            "step:68, loss:24.495887756347656\n",
            "step:69, loss:7.950404167175293\n",
            "step:70, loss:7.747669219970703\n",
            "step:71, loss:37.77320861816406\n",
            "step:72, loss:10.68405532836914\n",
            "step:73, loss:26.844451904296875\n",
            "step:74, loss:68.8249740600586\n",
            "step:75, loss:15.949453353881836\n",
            "step:76, loss:76.16993713378906\n",
            "9\n",
            "-----\n",
            "step:0, loss:8.467233657836914\n",
            "step:1, loss:13.187994956970215\n",
            "step:2, loss:12.33238410949707\n",
            "step:3, loss:13.591181755065918\n",
            "step:4, loss:9.626470565795898\n",
            "step:5, loss:34.712074279785156\n",
            "step:6, loss:142.19046020507812\n",
            "step:7, loss:48.18999099731445\n",
            "step:8, loss:16.316791534423828\n",
            "step:9, loss:24.49347686767578\n",
            "step:10, loss:59.89917755126953\n",
            "step:11, loss:6.170316219329834\n",
            "step:12, loss:31.069236755371094\n",
            "step:13, loss:77.72262573242188\n",
            "step:14, loss:146.88279724121094\n",
            "step:15, loss:17.93332290649414\n",
            "step:16, loss:32.20042037963867\n",
            "step:17, loss:16.471969604492188\n",
            "step:18, loss:17.81664276123047\n",
            "step:19, loss:18.331668853759766\n",
            "step:20, loss:54.08244323730469\n",
            "step:21, loss:21.941808700561523\n",
            "step:22, loss:6.9273834228515625\n",
            "step:23, loss:18.451457977294922\n",
            "step:24, loss:19.044902801513672\n",
            "step:25, loss:18.88545036315918\n",
            "step:26, loss:16.894746780395508\n",
            "step:27, loss:44.954498291015625\n",
            "step:28, loss:17.410850524902344\n",
            "step:29, loss:14.274971008300781\n",
            "step:30, loss:26.423961639404297\n",
            "step:31, loss:11.952031135559082\n",
            "step:32, loss:11.242096900939941\n",
            "step:33, loss:35.95641326904297\n",
            "step:34, loss:10.110910415649414\n",
            "step:35, loss:23.905729293823242\n",
            "step:36, loss:4.724998950958252\n",
            "step:37, loss:7.260601043701172\n",
            "step:38, loss:6.574744701385498\n",
            "step:39, loss:27.630943298339844\n",
            "step:40, loss:9.521383285522461\n",
            "step:41, loss:25.818965911865234\n",
            "step:42, loss:23.09435272216797\n",
            "step:43, loss:40.07175827026367\n",
            "step:44, loss:20.482234954833984\n",
            "step:45, loss:13.25231647491455\n",
            "step:46, loss:19.162471771240234\n",
            "step:47, loss:17.661911010742188\n",
            "step:48, loss:26.339075088500977\n",
            "step:49, loss:10.008659362792969\n",
            "step:50, loss:7.684347629547119\n",
            "step:51, loss:9.775629043579102\n",
            "step:52, loss:20.309293746948242\n",
            "step:53, loss:25.944305419921875\n",
            "step:54, loss:4.411159038543701\n",
            "step:55, loss:24.606327056884766\n",
            "step:56, loss:18.71872329711914\n",
            "step:57, loss:24.376771926879883\n",
            "step:58, loss:129.59967041015625\n",
            "step:59, loss:11.88568115234375\n",
            "step:60, loss:10.817094802856445\n",
            "step:61, loss:12.65447998046875\n",
            "step:62, loss:38.241607666015625\n",
            "step:63, loss:14.953882217407227\n",
            "step:64, loss:5.62006950378418\n",
            "step:65, loss:34.14888381958008\n",
            "step:66, loss:18.965301513671875\n",
            "step:67, loss:12.085052490234375\n",
            "step:68, loss:21.057912826538086\n",
            "step:69, loss:20.86178207397461\n",
            "step:70, loss:7.998295307159424\n",
            "step:71, loss:41.36054229736328\n",
            "step:72, loss:7.411397457122803\n",
            "step:73, loss:30.880474090576172\n",
            "step:74, loss:24.05437469482422\n",
            "step:75, loss:11.832598686218262\n",
            "step:76, loss:32.20906448364258\n",
            "10\n",
            "-----\n",
            "step:0, loss:22.22182273864746\n",
            "step:1, loss:18.146377563476562\n",
            "step:2, loss:12.885461807250977\n",
            "step:3, loss:13.28193473815918\n",
            "step:4, loss:14.873490333557129\n",
            "step:5, loss:7.144009590148926\n",
            "step:6, loss:32.639713287353516\n",
            "step:7, loss:21.037443161010742\n",
            "step:8, loss:29.374486923217773\n",
            "step:9, loss:30.56290054321289\n",
            "step:10, loss:77.24119567871094\n",
            "step:11, loss:17.760900497436523\n",
            "step:12, loss:13.381400108337402\n",
            "step:13, loss:19.361465454101562\n",
            "step:14, loss:12.157800674438477\n",
            "step:15, loss:39.92691421508789\n",
            "step:16, loss:82.5207290649414\n",
            "step:17, loss:5.01263427734375\n",
            "step:18, loss:11.235161781311035\n",
            "step:19, loss:17.902103424072266\n",
            "step:20, loss:9.455554008483887\n",
            "step:21, loss:28.51502227783203\n",
            "step:22, loss:7.729604244232178\n",
            "step:23, loss:18.847434997558594\n",
            "step:24, loss:13.394386291503906\n",
            "step:25, loss:19.463008880615234\n",
            "step:26, loss:12.662156105041504\n",
            "step:27, loss:6.4066314697265625\n",
            "step:28, loss:20.3774471282959\n",
            "step:29, loss:11.55601692199707\n",
            "step:30, loss:14.79226303100586\n",
            "step:31, loss:146.2338104248047\n",
            "step:32, loss:12.388991355895996\n",
            "step:33, loss:13.477514266967773\n",
            "step:34, loss:30.87297821044922\n",
            "step:35, loss:134.11672973632812\n",
            "step:36, loss:9.494033813476562\n",
            "step:37, loss:11.557685852050781\n",
            "step:38, loss:6.486806869506836\n",
            "step:39, loss:24.429946899414062\n",
            "step:40, loss:35.27093505859375\n",
            "step:41, loss:7.163616180419922\n",
            "step:42, loss:16.898818969726562\n",
            "step:43, loss:7.62916898727417\n",
            "step:44, loss:8.296052932739258\n",
            "step:45, loss:12.597769737243652\n",
            "step:46, loss:7.985131740570068\n",
            "step:47, loss:11.11168098449707\n",
            "step:48, loss:52.00714111328125\n",
            "step:49, loss:26.036075592041016\n",
            "step:50, loss:21.661457061767578\n",
            "step:51, loss:37.66313934326172\n",
            "step:52, loss:27.39374542236328\n",
            "step:53, loss:9.00605297088623\n",
            "step:54, loss:34.29359817504883\n",
            "step:55, loss:9.516160011291504\n",
            "step:56, loss:27.744625091552734\n",
            "step:57, loss:19.599048614501953\n",
            "step:58, loss:11.130393028259277\n",
            "step:59, loss:33.21720886230469\n",
            "step:60, loss:54.75057601928711\n",
            "step:61, loss:23.197345733642578\n",
            "step:62, loss:9.438518524169922\n",
            "step:63, loss:6.893666744232178\n",
            "step:64, loss:16.449995040893555\n",
            "step:65, loss:14.40958309173584\n",
            "step:66, loss:23.457839965820312\n",
            "step:67, loss:19.35987091064453\n",
            "step:68, loss:9.637189865112305\n",
            "step:69, loss:74.64682006835938\n",
            "step:70, loss:7.910007476806641\n",
            "step:71, loss:39.39905548095703\n",
            "step:72, loss:17.362998962402344\n",
            "step:73, loss:18.96355628967285\n",
            "step:74, loss:25.071863174438477\n",
            "step:75, loss:5.666406631469727\n",
            "step:76, loss:16.777128219604492\n",
            "11\n",
            "-----\n",
            "step:0, loss:124.4566650390625\n",
            "step:1, loss:15.428350448608398\n",
            "step:2, loss:8.749150276184082\n",
            "step:3, loss:15.687347412109375\n",
            "step:4, loss:22.755475997924805\n",
            "step:5, loss:19.12136459350586\n",
            "step:6, loss:10.150976181030273\n",
            "step:7, loss:11.788613319396973\n",
            "step:8, loss:15.318251609802246\n",
            "step:9, loss:11.932510375976562\n",
            "step:10, loss:15.660675048828125\n",
            "step:11, loss:17.35434341430664\n",
            "step:12, loss:13.546844482421875\n",
            "step:13, loss:63.186100006103516\n",
            "step:14, loss:25.210479736328125\n",
            "step:15, loss:13.213247299194336\n",
            "step:16, loss:5.1232781410217285\n",
            "step:17, loss:8.213275909423828\n",
            "step:18, loss:5.783071517944336\n",
            "step:19, loss:15.817455291748047\n",
            "step:20, loss:10.845449447631836\n",
            "step:21, loss:15.449721336364746\n",
            "step:22, loss:31.77422332763672\n",
            "step:23, loss:23.985061645507812\n",
            "step:24, loss:19.3615665435791\n",
            "step:25, loss:12.590192794799805\n",
            "step:26, loss:9.470873832702637\n",
            "step:27, loss:8.343526840209961\n",
            "step:28, loss:10.102320671081543\n",
            "step:29, loss:17.842283248901367\n",
            "step:30, loss:5.505009174346924\n",
            "step:31, loss:26.2944393157959\n",
            "step:32, loss:77.11740112304688\n",
            "step:33, loss:7.457745552062988\n",
            "step:34, loss:14.922531127929688\n",
            "step:35, loss:4.786153793334961\n",
            "step:36, loss:161.43641662597656\n",
            "step:37, loss:39.60561752319336\n",
            "step:38, loss:57.837989807128906\n",
            "step:39, loss:18.639240264892578\n",
            "step:40, loss:8.128171920776367\n",
            "step:41, loss:6.18879508972168\n",
            "step:42, loss:8.059011459350586\n",
            "step:43, loss:9.713693618774414\n",
            "step:44, loss:9.055858612060547\n",
            "step:45, loss:55.64435958862305\n",
            "step:46, loss:26.994041442871094\n",
            "step:47, loss:8.162670135498047\n",
            "step:48, loss:31.961666107177734\n",
            "step:49, loss:9.007393836975098\n",
            "step:50, loss:8.385401725769043\n",
            "step:51, loss:42.437294006347656\n",
            "step:52, loss:8.08525276184082\n",
            "step:53, loss:11.272917747497559\n",
            "step:54, loss:12.505485534667969\n",
            "step:55, loss:10.190427780151367\n",
            "step:56, loss:22.44057846069336\n",
            "step:57, loss:9.445281982421875\n",
            "step:58, loss:13.871027946472168\n",
            "step:59, loss:31.788576126098633\n",
            "step:60, loss:14.571556091308594\n",
            "step:61, loss:7.1939568519592285\n",
            "step:62, loss:14.281877517700195\n",
            "step:63, loss:11.32504653930664\n",
            "step:64, loss:16.64196014404297\n",
            "step:65, loss:14.593430519104004\n",
            "step:66, loss:8.893495559692383\n",
            "step:67, loss:7.876331329345703\n",
            "step:68, loss:20.40811538696289\n",
            "step:69, loss:10.3215913772583\n",
            "step:70, loss:52.57432174682617\n",
            "step:71, loss:5.772470474243164\n",
            "step:72, loss:49.68378829956055\n",
            "step:73, loss:16.858068466186523\n",
            "step:74, loss:14.242528915405273\n",
            "step:75, loss:7.293501377105713\n",
            "step:76, loss:54.528358459472656\n",
            "12\n",
            "-----\n",
            "step:0, loss:16.58033561706543\n",
            "step:1, loss:13.833155632019043\n",
            "step:2, loss:33.33583068847656\n",
            "step:3, loss:19.036056518554688\n",
            "step:4, loss:64.53961181640625\n",
            "step:5, loss:20.452959060668945\n",
            "step:6, loss:14.937960624694824\n",
            "step:7, loss:12.891112327575684\n",
            "step:8, loss:30.474897384643555\n",
            "step:9, loss:20.082317352294922\n",
            "step:10, loss:7.159404754638672\n",
            "step:11, loss:8.1464262008667\n",
            "step:12, loss:5.957493782043457\n",
            "step:13, loss:16.8391056060791\n",
            "step:14, loss:5.677314758300781\n",
            "step:15, loss:21.576507568359375\n",
            "step:16, loss:15.683576583862305\n",
            "step:17, loss:9.999573707580566\n",
            "step:18, loss:2.5237560272216797\n",
            "step:19, loss:51.63159942626953\n",
            "step:20, loss:20.14798927307129\n",
            "step:21, loss:6.512784004211426\n",
            "step:22, loss:12.89140510559082\n",
            "step:23, loss:7.370551109313965\n",
            "step:24, loss:28.848846435546875\n",
            "step:25, loss:17.831298828125\n",
            "step:26, loss:6.2467041015625\n",
            "step:27, loss:9.084217071533203\n",
            "step:28, loss:5.963227272033691\n",
            "step:29, loss:5.50137996673584\n",
            "step:30, loss:4.7635698318481445\n",
            "step:31, loss:6.640870571136475\n",
            "step:32, loss:9.90289306640625\n",
            "step:33, loss:14.074110984802246\n",
            "step:34, loss:120.58763122558594\n",
            "step:35, loss:17.74625015258789\n",
            "step:36, loss:20.018508911132812\n",
            "step:37, loss:7.306143760681152\n",
            "step:38, loss:6.736300945281982\n",
            "step:39, loss:36.257286071777344\n",
            "step:40, loss:11.786436080932617\n",
            "step:41, loss:21.437923431396484\n",
            "step:42, loss:38.48736572265625\n",
            "step:43, loss:11.576505661010742\n",
            "step:44, loss:32.419395446777344\n",
            "step:45, loss:29.296466827392578\n",
            "step:46, loss:5.429880142211914\n",
            "step:47, loss:14.333415985107422\n",
            "step:48, loss:9.07162857055664\n",
            "step:49, loss:39.85386657714844\n",
            "step:50, loss:8.481561660766602\n",
            "step:51, loss:8.141119956970215\n",
            "step:52, loss:39.206214904785156\n",
            "step:53, loss:8.807930946350098\n",
            "step:54, loss:9.106195449829102\n",
            "step:55, loss:57.6214485168457\n",
            "step:56, loss:26.980361938476562\n",
            "step:57, loss:31.290546417236328\n",
            "step:58, loss:7.068889617919922\n",
            "step:59, loss:7.479330539703369\n",
            "step:60, loss:120.68897247314453\n",
            "step:61, loss:10.50090503692627\n",
            "step:62, loss:8.99951457977295\n",
            "step:63, loss:16.784177780151367\n",
            "step:64, loss:56.62531280517578\n",
            "step:65, loss:15.58478832244873\n",
            "step:66, loss:10.400915145874023\n",
            "step:67, loss:16.192855834960938\n",
            "step:68, loss:13.729713439941406\n",
            "step:69, loss:8.941915512084961\n",
            "step:70, loss:7.18798828125\n",
            "step:71, loss:10.038640022277832\n",
            "step:72, loss:8.064762115478516\n",
            "step:73, loss:25.811153411865234\n",
            "step:74, loss:11.896039009094238\n",
            "step:75, loss:17.405656814575195\n",
            "step:76, loss:6.465262413024902\n",
            "13\n",
            "-----\n",
            "step:0, loss:26.303417205810547\n",
            "step:1, loss:9.251336097717285\n",
            "step:2, loss:9.080485343933105\n",
            "step:3, loss:19.55486297607422\n",
            "step:4, loss:7.261872291564941\n",
            "step:5, loss:17.555736541748047\n",
            "step:6, loss:12.145916938781738\n",
            "step:7, loss:53.63898849487305\n",
            "step:8, loss:15.806622505187988\n",
            "step:9, loss:8.386714935302734\n",
            "step:10, loss:31.867584228515625\n",
            "step:11, loss:16.065364837646484\n",
            "step:12, loss:38.22539138793945\n",
            "step:13, loss:11.443873405456543\n",
            "step:14, loss:6.032010078430176\n",
            "step:15, loss:9.354511260986328\n",
            "step:16, loss:21.27399253845215\n",
            "step:17, loss:9.149504661560059\n",
            "step:18, loss:8.792001724243164\n",
            "step:19, loss:18.177749633789062\n",
            "step:20, loss:9.797657012939453\n",
            "step:21, loss:10.250329971313477\n",
            "step:22, loss:9.233377456665039\n",
            "step:23, loss:6.550526142120361\n",
            "step:24, loss:28.214065551757812\n",
            "step:25, loss:11.410934448242188\n",
            "step:26, loss:14.043989181518555\n",
            "step:27, loss:21.92853546142578\n",
            "step:28, loss:12.09050178527832\n",
            "step:29, loss:22.5294189453125\n",
            "step:30, loss:25.331207275390625\n",
            "step:31, loss:53.840545654296875\n",
            "step:32, loss:14.349142074584961\n",
            "step:33, loss:22.628284454345703\n",
            "step:34, loss:53.22795486450195\n",
            "step:35, loss:3.963827610015869\n",
            "step:36, loss:27.39373779296875\n",
            "step:37, loss:7.784063339233398\n",
            "step:38, loss:18.18531036376953\n",
            "step:39, loss:9.035865783691406\n",
            "step:40, loss:6.478188514709473\n",
            "step:41, loss:13.257476806640625\n",
            "step:42, loss:9.607258796691895\n",
            "step:43, loss:16.20834732055664\n",
            "step:44, loss:5.374717712402344\n",
            "step:45, loss:41.54655075073242\n",
            "step:46, loss:7.7173309326171875\n",
            "step:47, loss:14.61667537689209\n",
            "step:48, loss:2.884753465652466\n",
            "step:49, loss:10.804590225219727\n",
            "step:50, loss:15.282587051391602\n",
            "step:51, loss:34.6517333984375\n",
            "step:52, loss:10.625165939331055\n",
            "step:53, loss:6.112712383270264\n",
            "step:54, loss:118.05279541015625\n",
            "step:55, loss:5.843517780303955\n",
            "step:56, loss:8.380870819091797\n",
            "step:57, loss:32.82673645019531\n",
            "step:58, loss:18.336233139038086\n",
            "step:59, loss:7.601412773132324\n",
            "step:60, loss:11.785511016845703\n",
            "step:61, loss:5.548906326293945\n",
            "step:62, loss:8.948534965515137\n",
            "step:63, loss:117.52247619628906\n",
            "step:64, loss:4.123815536499023\n",
            "step:65, loss:45.13410186767578\n",
            "step:66, loss:34.25817108154297\n",
            "step:67, loss:5.005002021789551\n",
            "step:68, loss:3.579258441925049\n",
            "step:69, loss:8.023137092590332\n",
            "step:70, loss:14.656163215637207\n",
            "step:71, loss:14.963016510009766\n",
            "step:72, loss:19.63833999633789\n",
            "step:73, loss:5.316723823547363\n",
            "step:74, loss:10.05937671661377\n",
            "step:75, loss:7.469429969787598\n",
            "step:76, loss:7.675681114196777\n",
            "14\n",
            "-----\n",
            "step:0, loss:7.937684535980225\n",
            "step:1, loss:30.945207595825195\n",
            "step:2, loss:5.3320441246032715\n",
            "step:3, loss:7.639128684997559\n",
            "step:4, loss:8.081507682800293\n",
            "step:5, loss:4.68593692779541\n",
            "step:6, loss:27.096698760986328\n",
            "step:7, loss:12.262057304382324\n",
            "step:8, loss:9.035979270935059\n",
            "step:9, loss:63.23759078979492\n",
            "step:10, loss:9.186273574829102\n",
            "step:11, loss:6.393561363220215\n",
            "step:12, loss:11.383140563964844\n",
            "step:13, loss:4.133662223815918\n",
            "step:14, loss:120.7998046875\n",
            "step:15, loss:10.812928199768066\n",
            "step:16, loss:7.389444828033447\n",
            "step:17, loss:5.664872169494629\n",
            "step:18, loss:12.425868034362793\n",
            "step:19, loss:13.079635620117188\n",
            "step:20, loss:19.89792823791504\n",
            "step:21, loss:16.77707862854004\n",
            "step:22, loss:15.188192367553711\n",
            "step:23, loss:24.20201301574707\n",
            "step:24, loss:22.029266357421875\n",
            "step:25, loss:7.189487457275391\n",
            "step:26, loss:8.63558578491211\n",
            "step:27, loss:5.026533603668213\n",
            "step:28, loss:12.041374206542969\n",
            "step:29, loss:57.11145782470703\n",
            "step:30, loss:13.73248291015625\n",
            "step:31, loss:10.391408920288086\n",
            "step:32, loss:12.800971984863281\n",
            "step:33, loss:6.687955379486084\n",
            "step:34, loss:7.252467155456543\n",
            "step:35, loss:25.840450286865234\n",
            "step:36, loss:35.615421295166016\n",
            "step:37, loss:9.96983814239502\n",
            "step:38, loss:10.590896606445312\n",
            "step:39, loss:5.726113319396973\n",
            "step:40, loss:12.284204483032227\n",
            "step:41, loss:22.02659034729004\n",
            "step:42, loss:6.412778854370117\n",
            "step:43, loss:4.047389984130859\n",
            "step:44, loss:13.006511688232422\n",
            "step:45, loss:16.425209045410156\n",
            "step:46, loss:15.231878280639648\n",
            "step:47, loss:13.00460433959961\n",
            "step:48, loss:8.016304016113281\n",
            "step:49, loss:5.566649913787842\n",
            "step:50, loss:52.12176513671875\n",
            "step:51, loss:14.52364730834961\n",
            "step:52, loss:9.950857162475586\n",
            "step:53, loss:12.47138500213623\n",
            "step:54, loss:16.852664947509766\n",
            "step:55, loss:14.03462028503418\n",
            "step:56, loss:5.1845703125\n",
            "step:57, loss:8.816574096679688\n",
            "step:58, loss:14.026752471923828\n",
            "step:59, loss:27.346708297729492\n",
            "step:60, loss:21.169418334960938\n",
            "step:61, loss:13.544910430908203\n",
            "step:62, loss:47.337615966796875\n",
            "step:63, loss:9.02450942993164\n",
            "step:64, loss:6.970132350921631\n",
            "step:65, loss:31.865800857543945\n",
            "step:66, loss:20.115211486816406\n",
            "step:67, loss:9.923507690429688\n",
            "step:68, loss:9.824312210083008\n",
            "step:69, loss:6.016436576843262\n",
            "step:70, loss:23.18549346923828\n",
            "step:71, loss:5.066527366638184\n",
            "step:72, loss:13.653057098388672\n",
            "step:73, loss:10.370233535766602\n",
            "step:74, loss:109.02394104003906\n",
            "step:75, loss:6.773797035217285\n",
            "step:76, loss:11.403654098510742\n",
            "15\n",
            "-----\n",
            "step:0, loss:2.86293625831604\n",
            "step:1, loss:7.163582801818848\n",
            "step:2, loss:9.143924713134766\n",
            "step:3, loss:3.1522927284240723\n",
            "step:4, loss:22.587703704833984\n",
            "step:5, loss:11.945791244506836\n",
            "step:6, loss:60.01786422729492\n",
            "step:7, loss:15.055615425109863\n",
            "step:8, loss:14.160685539245605\n",
            "step:9, loss:6.215702533721924\n",
            "step:10, loss:7.015779495239258\n",
            "step:11, loss:25.02446937561035\n",
            "step:12, loss:5.402020454406738\n",
            "step:13, loss:5.85323429107666\n",
            "step:14, loss:3.916171073913574\n",
            "step:15, loss:5.885675430297852\n",
            "step:16, loss:4.368766784667969\n",
            "step:17, loss:16.152128219604492\n",
            "step:18, loss:3.8248448371887207\n",
            "step:19, loss:14.502358436584473\n",
            "step:20, loss:7.475577354431152\n",
            "step:21, loss:16.62038803100586\n",
            "step:22, loss:32.241905212402344\n",
            "step:23, loss:6.617161750793457\n",
            "step:24, loss:4.8660888671875\n",
            "step:25, loss:43.43100357055664\n",
            "step:26, loss:8.521634101867676\n",
            "step:27, loss:6.302504539489746\n",
            "step:28, loss:47.00110626220703\n",
            "step:29, loss:6.351334095001221\n",
            "step:30, loss:138.7544708251953\n",
            "step:31, loss:6.29581356048584\n",
            "step:32, loss:7.185179710388184\n",
            "step:33, loss:9.350204467773438\n",
            "step:34, loss:8.958831787109375\n",
            "step:35, loss:9.746553421020508\n",
            "step:36, loss:12.922268867492676\n",
            "step:37, loss:17.900936126708984\n",
            "step:38, loss:21.196640014648438\n",
            "step:39, loss:12.030061721801758\n",
            "step:40, loss:7.281550407409668\n",
            "step:41, loss:22.814451217651367\n",
            "step:42, loss:16.557668685913086\n",
            "step:43, loss:9.862382888793945\n",
            "step:44, loss:38.03632736206055\n",
            "step:45, loss:10.765033721923828\n",
            "step:46, loss:13.554848670959473\n",
            "step:47, loss:5.213813781738281\n",
            "step:48, loss:6.297203063964844\n",
            "step:49, loss:10.942394256591797\n",
            "step:50, loss:3.5501561164855957\n",
            "step:51, loss:8.149681091308594\n",
            "step:52, loss:9.111042022705078\n",
            "step:53, loss:9.142788887023926\n",
            "step:54, loss:10.383706092834473\n",
            "step:55, loss:5.97403621673584\n",
            "step:56, loss:47.52836990356445\n",
            "step:57, loss:9.17150592803955\n",
            "step:58, loss:4.802314281463623\n",
            "step:59, loss:16.644519805908203\n",
            "step:60, loss:7.771522045135498\n",
            "step:61, loss:32.19987487792969\n",
            "step:62, loss:14.425214767456055\n",
            "step:63, loss:7.3220367431640625\n",
            "step:64, loss:7.04878044128418\n",
            "step:65, loss:32.110939025878906\n",
            "step:66, loss:7.846721649169922\n",
            "step:67, loss:8.997137069702148\n",
            "step:68, loss:121.38521575927734\n",
            "step:69, loss:4.780255317687988\n",
            "step:70, loss:19.63937759399414\n",
            "step:71, loss:11.083300590515137\n",
            "step:72, loss:4.612576961517334\n",
            "step:73, loss:4.684072971343994\n",
            "step:74, loss:28.40607261657715\n",
            "step:75, loss:12.643852233886719\n",
            "step:76, loss:9.76865291595459\n",
            "16\n",
            "-----\n",
            "step:0, loss:7.437453269958496\n",
            "step:1, loss:11.219856262207031\n",
            "step:2, loss:17.233478546142578\n",
            "step:3, loss:5.029532432556152\n",
            "step:4, loss:25.101638793945312\n",
            "step:5, loss:18.843690872192383\n",
            "step:6, loss:12.107736587524414\n",
            "step:7, loss:6.247832298278809\n",
            "step:8, loss:8.204615592956543\n",
            "step:9, loss:18.48213005065918\n",
            "step:10, loss:45.380584716796875\n",
            "step:11, loss:13.965731620788574\n",
            "step:12, loss:14.615312576293945\n",
            "step:13, loss:4.608242034912109\n",
            "step:14, loss:10.211517333984375\n",
            "step:15, loss:6.109150409698486\n",
            "step:16, loss:8.692780494689941\n",
            "step:17, loss:3.541940689086914\n",
            "step:18, loss:17.493406295776367\n",
            "step:19, loss:20.865020751953125\n",
            "step:20, loss:6.526335716247559\n",
            "step:21, loss:108.96065521240234\n",
            "step:22, loss:9.762067794799805\n",
            "step:23, loss:13.46214485168457\n",
            "step:24, loss:9.818046569824219\n",
            "step:25, loss:8.422931671142578\n",
            "step:26, loss:3.571092128753662\n",
            "step:27, loss:11.656834602355957\n",
            "step:28, loss:4.46408224105835\n",
            "step:29, loss:104.41320037841797\n",
            "step:30, loss:6.562263011932373\n",
            "step:31, loss:18.69559097290039\n",
            "step:32, loss:3.6996560096740723\n",
            "step:33, loss:16.561229705810547\n",
            "step:34, loss:11.123470306396484\n",
            "step:35, loss:3.929981231689453\n",
            "step:36, loss:5.5944013595581055\n",
            "step:37, loss:7.767988681793213\n",
            "step:38, loss:19.173019409179688\n",
            "step:39, loss:6.643889427185059\n",
            "step:40, loss:16.322364807128906\n",
            "step:41, loss:5.776792526245117\n",
            "step:42, loss:10.793848037719727\n",
            "step:43, loss:6.095273971557617\n",
            "step:44, loss:14.155004501342773\n",
            "step:45, loss:12.20943832397461\n",
            "step:46, loss:3.2945306301116943\n",
            "step:47, loss:38.65400695800781\n",
            "step:48, loss:5.857795238494873\n",
            "step:49, loss:15.797394752502441\n",
            "step:50, loss:42.863807678222656\n",
            "step:51, loss:8.21044921875\n",
            "step:52, loss:24.10434341430664\n",
            "step:53, loss:4.895895957946777\n",
            "step:54, loss:8.045110702514648\n",
            "step:55, loss:7.176598072052002\n",
            "step:56, loss:11.02528190612793\n",
            "step:57, loss:1.9363120794296265\n",
            "step:58, loss:18.530902862548828\n",
            "step:59, loss:8.066328048706055\n",
            "step:60, loss:9.622137069702148\n",
            "step:61, loss:5.57270622253418\n",
            "step:62, loss:16.23102569580078\n",
            "step:63, loss:5.904231071472168\n",
            "step:64, loss:17.326438903808594\n",
            "step:65, loss:46.64439392089844\n",
            "step:66, loss:6.9876580238342285\n",
            "step:67, loss:4.612312316894531\n",
            "step:68, loss:27.167673110961914\n",
            "step:69, loss:13.22178840637207\n",
            "step:70, loss:17.360183715820312\n",
            "step:71, loss:8.47513484954834\n",
            "step:72, loss:6.57103157043457\n",
            "step:73, loss:14.19651985168457\n",
            "step:74, loss:42.86961364746094\n",
            "step:75, loss:4.364441871643066\n",
            "step:76, loss:8.910303115844727\n",
            "17\n",
            "-----\n",
            "step:0, loss:8.098748207092285\n",
            "step:1, loss:10.345427513122559\n",
            "step:2, loss:13.284797668457031\n",
            "step:3, loss:6.619589805603027\n",
            "step:4, loss:6.118633270263672\n",
            "step:5, loss:19.790184020996094\n",
            "step:6, loss:12.222068786621094\n",
            "step:7, loss:6.847485065460205\n",
            "step:8, loss:11.796976089477539\n",
            "step:9, loss:7.829275608062744\n",
            "step:10, loss:8.57632064819336\n",
            "step:11, loss:14.649614334106445\n",
            "step:12, loss:3.9956674575805664\n",
            "step:13, loss:6.227056980133057\n",
            "step:14, loss:43.41556930541992\n",
            "step:15, loss:56.09615707397461\n",
            "step:16, loss:4.555843830108643\n",
            "step:17, loss:4.531148910522461\n",
            "step:18, loss:6.195684432983398\n",
            "step:19, loss:34.01736068725586\n",
            "step:20, loss:42.77693176269531\n",
            "step:21, loss:19.985666275024414\n",
            "step:22, loss:6.276555061340332\n",
            "step:23, loss:10.323677062988281\n",
            "step:24, loss:10.6217622756958\n",
            "step:25, loss:3.3197178840637207\n",
            "step:26, loss:14.290971755981445\n",
            "step:27, loss:2.9865870475769043\n",
            "step:28, loss:7.612456798553467\n",
            "step:29, loss:7.244408130645752\n",
            "step:30, loss:6.448640823364258\n",
            "step:31, loss:4.876645088195801\n",
            "step:32, loss:4.573223114013672\n",
            "step:33, loss:13.255777359008789\n",
            "step:34, loss:5.264247417449951\n",
            "step:35, loss:6.4527788162231445\n",
            "step:36, loss:11.048255920410156\n",
            "step:37, loss:7.202823638916016\n",
            "step:38, loss:9.765122413635254\n",
            "step:39, loss:8.285511016845703\n",
            "step:40, loss:4.491529941558838\n",
            "step:41, loss:13.180180549621582\n",
            "step:42, loss:7.162513732910156\n",
            "step:43, loss:6.486791610717773\n",
            "step:44, loss:6.189830303192139\n",
            "step:45, loss:16.86908721923828\n",
            "step:46, loss:14.192485809326172\n",
            "step:47, loss:8.76400089263916\n",
            "step:48, loss:6.097282409667969\n",
            "step:49, loss:11.450167655944824\n",
            "step:50, loss:102.77493286132812\n",
            "step:51, loss:15.819403648376465\n",
            "step:52, loss:5.598696231842041\n",
            "step:53, loss:30.721328735351562\n",
            "step:54, loss:8.059738159179688\n",
            "step:55, loss:5.1645026206970215\n",
            "step:56, loss:3.47324275970459\n",
            "step:57, loss:3.6853580474853516\n",
            "step:58, loss:4.766530513763428\n",
            "step:59, loss:4.016068935394287\n",
            "step:60, loss:11.0709810256958\n",
            "step:61, loss:20.001415252685547\n",
            "step:62, loss:15.701434135437012\n",
            "step:63, loss:4.151230812072754\n",
            "step:64, loss:5.130653381347656\n",
            "step:65, loss:103.51532745361328\n",
            "step:66, loss:26.567676544189453\n",
            "step:67, loss:5.567567825317383\n",
            "step:68, loss:14.559109687805176\n",
            "step:69, loss:12.303153991699219\n",
            "step:70, loss:19.574129104614258\n",
            "step:71, loss:5.191361427307129\n",
            "step:72, loss:22.16575813293457\n",
            "step:73, loss:23.210906982421875\n",
            "step:74, loss:16.493560791015625\n",
            "step:75, loss:42.50151062011719\n",
            "step:76, loss:2.408871650695801\n",
            "18\n",
            "-----\n",
            "step:0, loss:18.722137451171875\n",
            "step:1, loss:7.06392765045166\n",
            "step:2, loss:12.727361679077148\n",
            "step:3, loss:4.571614742279053\n",
            "step:4, loss:12.66382884979248\n",
            "step:5, loss:13.759448051452637\n",
            "step:6, loss:4.510859489440918\n",
            "step:7, loss:15.927310943603516\n",
            "step:8, loss:10.633487701416016\n",
            "step:9, loss:8.089725494384766\n",
            "step:10, loss:13.826979637145996\n",
            "step:11, loss:3.619128704071045\n",
            "step:12, loss:3.9335999488830566\n",
            "step:13, loss:35.4495735168457\n",
            "step:14, loss:6.576172351837158\n",
            "step:15, loss:7.089615345001221\n",
            "step:16, loss:5.484587669372559\n",
            "step:17, loss:26.543386459350586\n",
            "step:18, loss:4.987611770629883\n",
            "step:19, loss:2.9045681953430176\n",
            "step:20, loss:4.648144245147705\n",
            "step:21, loss:5.91910457611084\n",
            "step:22, loss:8.30302906036377\n",
            "step:23, loss:41.260047912597656\n",
            "step:24, loss:16.503768920898438\n",
            "step:25, loss:9.558120727539062\n",
            "step:26, loss:3.2936229705810547\n",
            "step:27, loss:8.777389526367188\n",
            "step:28, loss:9.730489730834961\n",
            "step:29, loss:8.661714553833008\n",
            "step:30, loss:19.68841552734375\n",
            "step:31, loss:8.35874080657959\n",
            "step:32, loss:9.660652160644531\n",
            "step:33, loss:11.57999038696289\n",
            "step:34, loss:25.76525115966797\n",
            "step:35, loss:30.62322998046875\n",
            "step:36, loss:1.6914868354797363\n",
            "step:37, loss:5.964256286621094\n",
            "step:38, loss:4.740287780761719\n",
            "step:39, loss:35.331974029541016\n",
            "step:40, loss:103.65702819824219\n",
            "step:41, loss:2.5999937057495117\n",
            "step:42, loss:4.043588638305664\n",
            "step:43, loss:4.865571022033691\n",
            "step:44, loss:4.100637435913086\n",
            "step:45, loss:18.719661712646484\n",
            "step:46, loss:18.668088912963867\n",
            "step:47, loss:6.047566890716553\n",
            "step:48, loss:19.572032928466797\n",
            "step:49, loss:14.98147201538086\n",
            "step:50, loss:4.955547332763672\n",
            "step:51, loss:15.209030151367188\n",
            "step:52, loss:10.121187210083008\n",
            "step:53, loss:6.156402587890625\n",
            "step:54, loss:3.430087089538574\n",
            "step:55, loss:54.492156982421875\n",
            "step:56, loss:5.244451522827148\n",
            "step:57, loss:5.794170379638672\n",
            "step:58, loss:14.718521118164062\n",
            "step:59, loss:6.502274036407471\n",
            "step:60, loss:8.654542922973633\n",
            "step:61, loss:4.618667125701904\n",
            "step:62, loss:6.6561150550842285\n",
            "step:63, loss:3.2051165103912354\n",
            "step:64, loss:4.362135887145996\n",
            "step:65, loss:103.88996887207031\n",
            "step:66, loss:6.167872905731201\n",
            "step:67, loss:12.788223266601562\n",
            "step:68, loss:15.522457122802734\n",
            "step:69, loss:5.186357498168945\n",
            "step:70, loss:6.066739082336426\n",
            "step:71, loss:7.0120649337768555\n",
            "step:72, loss:6.142336368560791\n",
            "step:73, loss:5.941600322723389\n",
            "step:74, loss:18.714374542236328\n",
            "step:75, loss:12.358187675476074\n",
            "step:76, loss:9.470778465270996\n",
            "19\n",
            "-----\n",
            "step:0, loss:5.763963222503662\n",
            "step:1, loss:10.760486602783203\n",
            "step:2, loss:4.374933242797852\n",
            "step:3, loss:19.55952262878418\n",
            "step:4, loss:7.676972389221191\n",
            "step:5, loss:13.627898216247559\n",
            "step:6, loss:35.33147430419922\n",
            "step:7, loss:140.28262329101562\n",
            "step:8, loss:5.570302486419678\n",
            "step:9, loss:15.353565216064453\n",
            "step:10, loss:3.8960964679718018\n",
            "step:11, loss:3.8605780601501465\n",
            "step:12, loss:4.593953609466553\n",
            "step:13, loss:13.212989807128906\n",
            "step:14, loss:10.14288330078125\n",
            "step:15, loss:6.187865257263184\n",
            "step:16, loss:7.093138217926025\n",
            "step:17, loss:7.314704418182373\n",
            "step:18, loss:9.2654447555542\n",
            "step:19, loss:44.80909729003906\n",
            "step:20, loss:5.042244911193848\n",
            "step:21, loss:5.995579719543457\n",
            "step:22, loss:114.49358367919922\n",
            "step:23, loss:9.893354415893555\n",
            "step:24, loss:4.258652210235596\n",
            "step:25, loss:3.1777756214141846\n",
            "step:26, loss:18.189533233642578\n",
            "step:27, loss:6.577764511108398\n",
            "step:28, loss:5.795485973358154\n",
            "step:29, loss:25.1369571685791\n",
            "step:30, loss:6.475832939147949\n",
            "step:31, loss:3.714865207672119\n",
            "step:32, loss:3.148707628250122\n",
            "step:33, loss:4.135660648345947\n",
            "step:34, loss:3.5343034267425537\n",
            "step:35, loss:4.281902313232422\n",
            "step:36, loss:2.569028377532959\n",
            "step:37, loss:5.422042369842529\n",
            "step:38, loss:4.501948356628418\n",
            "step:39, loss:3.9272830486297607\n",
            "step:40, loss:9.603479385375977\n",
            "step:41, loss:9.715173721313477\n",
            "step:42, loss:21.001209259033203\n",
            "step:43, loss:6.126064300537109\n",
            "step:44, loss:14.774007797241211\n",
            "step:45, loss:8.325238227844238\n",
            "step:46, loss:6.763985633850098\n",
            "step:47, loss:18.083457946777344\n",
            "step:48, loss:18.711627960205078\n",
            "step:49, loss:6.831292152404785\n",
            "step:50, loss:2.077399253845215\n",
            "step:51, loss:11.96371841430664\n",
            "step:52, loss:5.469484329223633\n",
            "step:53, loss:16.56490707397461\n",
            "step:54, loss:21.675561904907227\n",
            "step:55, loss:9.272016525268555\n",
            "step:56, loss:6.462804794311523\n",
            "step:57, loss:4.444384574890137\n",
            "step:58, loss:5.34876823425293\n",
            "step:59, loss:11.042396545410156\n",
            "step:60, loss:6.256390571594238\n",
            "step:61, loss:4.63763427734375\n",
            "step:62, loss:9.39317512512207\n",
            "step:63, loss:10.087705612182617\n",
            "step:64, loss:36.48855972290039\n",
            "step:65, loss:11.613058090209961\n",
            "step:66, loss:14.02200698852539\n",
            "step:67, loss:10.21355152130127\n",
            "step:68, loss:4.375443458557129\n",
            "step:69, loss:7.812100887298584\n",
            "step:70, loss:5.561861515045166\n",
            "step:71, loss:11.255170822143555\n",
            "step:72, loss:5.546439170837402\n",
            "step:73, loss:29.18950080871582\n",
            "step:74, loss:5.166272163391113\n",
            "step:75, loss:9.815786361694336\n",
            "step:76, loss:12.624553680419922\n",
            "20\n",
            "-----\n",
            "step:0, loss:3.527202606201172\n",
            "step:1, loss:7.354169845581055\n",
            "step:2, loss:8.16415786743164\n",
            "step:3, loss:8.923019409179688\n",
            "step:4, loss:4.838951110839844\n",
            "step:5, loss:11.644646644592285\n",
            "step:6, loss:9.059795379638672\n",
            "step:7, loss:107.21627044677734\n",
            "step:8, loss:11.981443405151367\n",
            "step:9, loss:6.2154541015625\n",
            "step:10, loss:3.3302340507507324\n",
            "step:11, loss:4.5149126052856445\n",
            "step:12, loss:20.215696334838867\n",
            "step:13, loss:7.212515354156494\n",
            "step:14, loss:4.074368953704834\n",
            "step:15, loss:2.2281653881073\n",
            "step:16, loss:3.8268561363220215\n",
            "step:17, loss:6.943492889404297\n",
            "step:18, loss:8.124567031860352\n",
            "step:19, loss:5.150507926940918\n",
            "step:20, loss:5.989604949951172\n",
            "step:21, loss:4.00154447555542\n",
            "step:22, loss:3.131265640258789\n",
            "step:23, loss:3.499067544937134\n",
            "step:24, loss:1.9003479480743408\n",
            "step:25, loss:8.558900833129883\n",
            "step:26, loss:5.320134162902832\n",
            "step:27, loss:4.928400993347168\n",
            "step:28, loss:2.3905134201049805\n",
            "step:29, loss:4.879120826721191\n",
            "step:30, loss:10.337594985961914\n",
            "step:31, loss:10.024080276489258\n",
            "step:32, loss:13.017390251159668\n",
            "step:33, loss:4.560399055480957\n",
            "step:34, loss:10.534345626831055\n",
            "step:35, loss:38.551544189453125\n",
            "step:36, loss:5.208224296569824\n",
            "step:37, loss:7.168487548828125\n",
            "step:38, loss:32.093658447265625\n",
            "step:39, loss:5.241912841796875\n",
            "step:40, loss:11.335041046142578\n",
            "step:41, loss:4.875757217407227\n",
            "step:42, loss:34.69143295288086\n",
            "step:43, loss:3.641366481781006\n",
            "step:44, loss:7.089147567749023\n",
            "step:45, loss:24.551870346069336\n",
            "step:46, loss:4.4824042320251465\n",
            "step:47, loss:6.322154998779297\n",
            "step:48, loss:12.07260799407959\n",
            "step:49, loss:16.73400115966797\n",
            "step:50, loss:3.9698519706726074\n",
            "step:51, loss:48.90411376953125\n",
            "step:52, loss:8.866628646850586\n",
            "step:53, loss:4.657993316650391\n",
            "step:54, loss:4.437087535858154\n",
            "step:55, loss:6.851706504821777\n",
            "step:56, loss:6.1049885749816895\n",
            "step:57, loss:41.87788391113281\n",
            "step:58, loss:2.7681057453155518\n",
            "step:59, loss:9.508112907409668\n",
            "step:60, loss:2.1932075023651123\n",
            "step:61, loss:8.170726776123047\n",
            "step:62, loss:6.955526351928711\n",
            "step:63, loss:2.644719123840332\n",
            "step:64, loss:9.172830581665039\n",
            "step:65, loss:124.77812194824219\n",
            "step:66, loss:3.068096160888672\n",
            "step:67, loss:23.526803970336914\n",
            "step:68, loss:4.910158157348633\n",
            "step:69, loss:18.86199951171875\n",
            "step:70, loss:7.565101146697998\n",
            "step:71, loss:6.973183631896973\n",
            "step:72, loss:6.530886650085449\n",
            "step:73, loss:16.21154022216797\n",
            "step:74, loss:16.52166175842285\n",
            "step:75, loss:8.233139991760254\n",
            "step:76, loss:3.0508062839508057\n",
            "21\n",
            "-----\n",
            "step:0, loss:3.980984926223755\n",
            "step:1, loss:13.008628845214844\n",
            "step:2, loss:6.327585220336914\n",
            "step:3, loss:8.771097183227539\n",
            "step:4, loss:3.6465816497802734\n",
            "step:5, loss:9.796531677246094\n",
            "step:6, loss:5.327877044677734\n",
            "step:7, loss:5.381100654602051\n",
            "step:8, loss:4.3672261238098145\n",
            "step:9, loss:3.1892895698547363\n",
            "step:10, loss:5.1793060302734375\n",
            "step:11, loss:3.492356300354004\n",
            "step:12, loss:5.602026462554932\n",
            "step:13, loss:4.8254876136779785\n",
            "step:14, loss:6.721355438232422\n",
            "step:15, loss:6.18822717666626\n",
            "step:16, loss:20.572895050048828\n",
            "step:17, loss:20.169628143310547\n",
            "step:18, loss:3.517308235168457\n",
            "step:19, loss:6.895175933837891\n",
            "step:20, loss:5.391533374786377\n",
            "step:21, loss:4.808981418609619\n",
            "step:22, loss:8.663095474243164\n",
            "step:23, loss:3.3682007789611816\n",
            "step:24, loss:5.2819952964782715\n",
            "step:25, loss:23.14455223083496\n",
            "step:26, loss:10.202680587768555\n",
            "step:27, loss:8.602156639099121\n",
            "step:28, loss:3.1579103469848633\n",
            "step:29, loss:6.313054084777832\n",
            "step:30, loss:14.577286720275879\n",
            "step:31, loss:2.254908561706543\n",
            "step:32, loss:3.9195446968078613\n",
            "step:33, loss:5.059159278869629\n",
            "step:34, loss:4.317803382873535\n",
            "step:35, loss:7.92027473449707\n",
            "step:36, loss:14.374019622802734\n",
            "step:37, loss:3.041506767272949\n",
            "step:38, loss:4.295519828796387\n",
            "step:39, loss:9.193227767944336\n",
            "step:40, loss:5.065364360809326\n",
            "step:41, loss:14.37912654876709\n",
            "step:42, loss:53.41755676269531\n",
            "step:43, loss:31.679615020751953\n",
            "step:44, loss:4.561352729797363\n",
            "step:45, loss:2.315309524536133\n",
            "step:46, loss:6.707425594329834\n",
            "step:47, loss:8.61209774017334\n",
            "step:48, loss:4.093996524810791\n",
            "step:49, loss:7.975170135498047\n",
            "step:50, loss:3.315779209136963\n",
            "step:51, loss:4.680927753448486\n",
            "step:52, loss:107.02982330322266\n",
            "step:53, loss:4.477004528045654\n",
            "step:54, loss:13.939115524291992\n",
            "step:55, loss:15.12319564819336\n",
            "step:56, loss:9.540826797485352\n",
            "step:57, loss:2.7521533966064453\n",
            "step:58, loss:14.487092971801758\n",
            "step:59, loss:4.027503490447998\n",
            "step:60, loss:5.606029510498047\n",
            "step:61, loss:8.845009803771973\n",
            "step:62, loss:2.9872007369995117\n",
            "step:63, loss:2.2486462593078613\n",
            "step:64, loss:38.36252975463867\n",
            "step:65, loss:6.6724443435668945\n",
            "step:66, loss:4.79014253616333\n",
            "step:67, loss:3.2425448894500732\n",
            "step:68, loss:3.7194604873657227\n",
            "step:69, loss:160.95709228515625\n",
            "step:70, loss:5.4082441329956055\n",
            "step:71, loss:21.13174819946289\n",
            "step:72, loss:3.2407073974609375\n",
            "step:73, loss:17.022136688232422\n",
            "step:74, loss:12.73759651184082\n",
            "step:75, loss:8.3955078125\n",
            "step:76, loss:3.740915298461914\n",
            "22\n",
            "-----\n",
            "step:0, loss:13.25961685180664\n",
            "step:1, loss:12.778350830078125\n",
            "step:2, loss:5.23629093170166\n",
            "step:3, loss:2.4131712913513184\n",
            "step:4, loss:11.953754425048828\n",
            "step:5, loss:4.4084086418151855\n",
            "step:6, loss:15.89826774597168\n",
            "step:7, loss:11.64022445678711\n",
            "step:8, loss:7.922652721405029\n",
            "step:9, loss:3.7594661712646484\n",
            "step:10, loss:4.165499687194824\n",
            "step:11, loss:16.0343074798584\n",
            "step:12, loss:32.637149810791016\n",
            "step:13, loss:2.954043388366699\n",
            "step:14, loss:16.89816665649414\n",
            "step:15, loss:4.30999231338501\n",
            "step:16, loss:41.17698669433594\n",
            "step:17, loss:5.224662780761719\n",
            "step:18, loss:18.178173065185547\n",
            "step:19, loss:6.522452354431152\n",
            "step:20, loss:3.547736883163452\n",
            "step:21, loss:7.161614894866943\n",
            "step:22, loss:4.10933256149292\n",
            "step:23, loss:2.9147214889526367\n",
            "step:24, loss:8.329713821411133\n",
            "step:25, loss:3.59763240814209\n",
            "step:26, loss:3.411496639251709\n",
            "step:27, loss:3.9927682876586914\n",
            "step:28, loss:3.0069265365600586\n",
            "step:29, loss:26.654525756835938\n",
            "step:30, loss:12.670705795288086\n",
            "step:31, loss:2.6850342750549316\n",
            "step:32, loss:4.945971488952637\n",
            "step:33, loss:27.157442092895508\n",
            "step:34, loss:23.002126693725586\n",
            "step:35, loss:16.53540802001953\n",
            "step:36, loss:188.08900451660156\n",
            "step:37, loss:5.094628810882568\n",
            "step:38, loss:11.6241455078125\n",
            "step:39, loss:2.8664886951446533\n",
            "step:40, loss:6.023556709289551\n",
            "step:41, loss:3.714787006378174\n",
            "step:42, loss:5.768196105957031\n",
            "step:43, loss:4.633517742156982\n",
            "step:44, loss:8.413029670715332\n",
            "step:45, loss:2.496450424194336\n",
            "step:46, loss:4.287747383117676\n",
            "step:47, loss:35.206886291503906\n",
            "step:48, loss:4.247483730316162\n",
            "step:49, loss:4.3185343742370605\n",
            "step:50, loss:32.496978759765625\n",
            "step:51, loss:3.0615854263305664\n",
            "step:52, loss:15.105863571166992\n",
            "step:53, loss:7.245229244232178\n",
            "step:54, loss:2.659201145172119\n",
            "step:55, loss:7.262146949768066\n",
            "step:56, loss:4.26472282409668\n",
            "step:57, loss:3.0992507934570312\n",
            "step:58, loss:2.261882781982422\n",
            "step:59, loss:22.487485885620117\n",
            "step:60, loss:8.333805084228516\n",
            "step:61, loss:9.905338287353516\n",
            "step:62, loss:7.700411796569824\n",
            "step:63, loss:4.613574028015137\n",
            "step:64, loss:3.9539976119995117\n",
            "step:65, loss:5.01369571685791\n",
            "step:66, loss:9.005531311035156\n",
            "step:67, loss:3.4858341217041016\n",
            "step:68, loss:8.337177276611328\n",
            "step:69, loss:7.925274848937988\n",
            "step:70, loss:3.35476016998291\n",
            "step:71, loss:9.527596473693848\n",
            "step:72, loss:3.283249855041504\n",
            "step:73, loss:9.244473457336426\n",
            "step:74, loss:4.4656853675842285\n",
            "step:75, loss:13.32010555267334\n",
            "step:76, loss:11.5817289352417\n",
            "23\n",
            "-----\n",
            "step:0, loss:3.675414562225342\n",
            "step:1, loss:4.1539459228515625\n",
            "step:2, loss:8.874669075012207\n",
            "step:3, loss:32.4020881652832\n",
            "step:4, loss:33.494285583496094\n",
            "step:5, loss:8.693389892578125\n",
            "step:6, loss:4.402827739715576\n",
            "step:7, loss:4.253643989562988\n",
            "step:8, loss:7.104389667510986\n",
            "step:9, loss:3.174793243408203\n",
            "step:10, loss:5.110171794891357\n",
            "step:11, loss:1.3485454320907593\n",
            "step:12, loss:3.3585739135742188\n",
            "step:13, loss:5.034135341644287\n",
            "step:14, loss:11.857335090637207\n",
            "step:15, loss:3.4549989700317383\n",
            "step:16, loss:2.391212224960327\n",
            "step:17, loss:3.3551323413848877\n",
            "step:18, loss:6.5763444900512695\n",
            "step:19, loss:26.784812927246094\n",
            "step:20, loss:12.179072380065918\n",
            "step:21, loss:12.470952987670898\n",
            "step:22, loss:5.9807305335998535\n",
            "step:23, loss:4.35801362991333\n",
            "step:24, loss:3.622385025024414\n",
            "step:25, loss:3.8518290519714355\n",
            "step:26, loss:106.21049499511719\n",
            "step:27, loss:14.994129180908203\n",
            "step:28, loss:17.320133209228516\n",
            "step:29, loss:5.930665493011475\n",
            "step:30, loss:5.847728252410889\n",
            "step:31, loss:30.569591522216797\n",
            "step:32, loss:5.489381790161133\n",
            "step:33, loss:16.598508834838867\n",
            "step:34, loss:8.402246475219727\n",
            "step:35, loss:3.9939472675323486\n",
            "step:36, loss:2.644016742706299\n",
            "step:37, loss:2.7033820152282715\n",
            "step:38, loss:26.792251586914062\n",
            "step:39, loss:7.33695125579834\n",
            "step:40, loss:7.745245933532715\n",
            "step:41, loss:5.157814979553223\n",
            "step:42, loss:1.9389272928237915\n",
            "step:43, loss:5.847226142883301\n",
            "step:44, loss:3.210493564605713\n",
            "step:45, loss:3.40144681930542\n",
            "step:46, loss:17.4625244140625\n",
            "step:47, loss:39.751258850097656\n",
            "step:48, loss:6.903913497924805\n",
            "step:49, loss:9.727261543273926\n",
            "step:50, loss:2.341604709625244\n",
            "step:51, loss:18.25295639038086\n",
            "step:52, loss:10.580240249633789\n",
            "step:53, loss:4.421905517578125\n",
            "step:54, loss:12.827017784118652\n",
            "step:55, loss:3.819350242614746\n",
            "step:56, loss:8.353468894958496\n",
            "step:57, loss:7.42557954788208\n",
            "step:58, loss:5.212256908416748\n",
            "step:59, loss:2.91422438621521\n",
            "step:60, loss:5.393856525421143\n",
            "step:61, loss:2.133273124694824\n",
            "step:62, loss:32.321510314941406\n",
            "step:63, loss:7.95709228515625\n",
            "step:64, loss:3.3076024055480957\n",
            "step:65, loss:97.84893035888672\n",
            "step:66, loss:2.242189407348633\n",
            "step:67, loss:4.94593620300293\n",
            "step:68, loss:3.4751501083374023\n",
            "step:69, loss:3.298966884613037\n",
            "step:70, loss:6.2310099601745605\n",
            "step:71, loss:6.821310997009277\n",
            "step:72, loss:2.504852771759033\n",
            "step:73, loss:13.534043312072754\n",
            "step:74, loss:2.6047887802124023\n",
            "step:75, loss:12.644793510437012\n",
            "step:76, loss:21.757036209106445\n",
            "24\n",
            "-----\n",
            "step:0, loss:37.13258361816406\n",
            "step:1, loss:4.089537620544434\n",
            "step:2, loss:5.792574882507324\n",
            "step:3, loss:5.473026752471924\n",
            "step:4, loss:4.734142303466797\n",
            "step:5, loss:2.5681278705596924\n",
            "step:6, loss:1.8718717098236084\n",
            "step:7, loss:17.703310012817383\n",
            "step:8, loss:3.3365166187286377\n",
            "step:9, loss:7.077527046203613\n",
            "step:10, loss:60.50145721435547\n",
            "step:11, loss:13.408086776733398\n",
            "step:12, loss:3.6543092727661133\n",
            "step:13, loss:1.840165138244629\n",
            "step:14, loss:3.4166367053985596\n",
            "step:15, loss:5.118135452270508\n",
            "step:16, loss:4.556646347045898\n",
            "step:17, loss:4.842055797576904\n",
            "step:18, loss:8.827889442443848\n",
            "step:19, loss:9.256816864013672\n",
            "step:20, loss:14.532970428466797\n",
            "step:21, loss:2.7348155975341797\n",
            "step:22, loss:6.4308881759643555\n",
            "step:23, loss:4.564138412475586\n",
            "step:24, loss:6.183164596557617\n",
            "step:25, loss:3.3289451599121094\n",
            "step:26, loss:4.413759231567383\n",
            "step:27, loss:6.1408514976501465\n",
            "step:28, loss:5.258021831512451\n",
            "step:29, loss:2.191467523574829\n",
            "step:30, loss:11.394975662231445\n",
            "step:31, loss:8.599508285522461\n",
            "step:32, loss:31.909208297729492\n",
            "step:33, loss:1.7071008682250977\n",
            "step:34, loss:2.1813817024230957\n",
            "step:35, loss:102.91206359863281\n",
            "step:36, loss:2.448087692260742\n",
            "step:37, loss:5.070718288421631\n",
            "step:38, loss:5.316032886505127\n",
            "step:39, loss:3.5753660202026367\n",
            "step:40, loss:3.8517045974731445\n",
            "step:41, loss:6.187556743621826\n",
            "step:42, loss:2.0162501335144043\n",
            "step:43, loss:99.57903289794922\n",
            "step:44, loss:5.417115211486816\n",
            "step:45, loss:2.42946720123291\n",
            "step:46, loss:4.577003002166748\n",
            "step:47, loss:11.640944480895996\n",
            "step:48, loss:3.2363600730895996\n",
            "step:49, loss:7.697998046875\n",
            "step:50, loss:7.307530879974365\n",
            "step:51, loss:29.803503036499023\n",
            "step:52, loss:3.6379470825195312\n",
            "step:53, loss:2.097851514816284\n",
            "step:54, loss:2.6024227142333984\n",
            "step:55, loss:11.374658584594727\n",
            "step:56, loss:2.993025541305542\n",
            "step:57, loss:7.288238048553467\n",
            "step:58, loss:4.790493965148926\n",
            "step:59, loss:3.7488555908203125\n",
            "step:60, loss:3.604888916015625\n",
            "step:61, loss:3.5047950744628906\n",
            "step:62, loss:13.720391273498535\n",
            "step:63, loss:10.364461898803711\n",
            "step:64, loss:6.648078441619873\n",
            "step:65, loss:1.2146291732788086\n",
            "step:66, loss:3.7930994033813477\n",
            "step:67, loss:2.6065378189086914\n",
            "step:68, loss:4.848389625549316\n",
            "step:69, loss:13.518123626708984\n",
            "step:70, loss:10.704424858093262\n",
            "step:71, loss:13.259452819824219\n",
            "step:72, loss:4.910724639892578\n",
            "step:73, loss:3.562434673309326\n",
            "step:74, loss:9.533915519714355\n",
            "step:75, loss:2.0385022163391113\n",
            "step:76, loss:9.209066390991211\n",
            "25\n",
            "-----\n",
            "step:0, loss:29.819917678833008\n",
            "step:1, loss:3.629115104675293\n",
            "step:2, loss:6.486294746398926\n",
            "step:3, loss:12.921087265014648\n",
            "step:4, loss:3.1882925033569336\n",
            "step:5, loss:7.896442413330078\n",
            "step:6, loss:2.7593367099761963\n",
            "step:7, loss:2.266887903213501\n",
            "step:8, loss:23.542407989501953\n",
            "step:9, loss:8.29012393951416\n",
            "step:10, loss:29.166318893432617\n",
            "step:11, loss:7.36215877532959\n",
            "step:12, loss:4.053428649902344\n",
            "step:13, loss:3.8813109397888184\n",
            "step:14, loss:8.102832794189453\n",
            "step:15, loss:4.811588287353516\n",
            "step:16, loss:4.587067604064941\n",
            "step:17, loss:15.581392288208008\n",
            "step:18, loss:3.785994529724121\n",
            "step:19, loss:178.63616943359375\n",
            "step:20, loss:5.9850664138793945\n",
            "step:21, loss:7.595869541168213\n",
            "step:22, loss:33.03539276123047\n",
            "step:23, loss:4.844125747680664\n",
            "step:24, loss:3.7908382415771484\n",
            "step:25, loss:9.360071182250977\n",
            "step:26, loss:1.8526184558868408\n",
            "step:27, loss:2.998324155807495\n",
            "step:28, loss:2.839020252227783\n",
            "step:29, loss:2.148250102996826\n",
            "step:30, loss:11.55022144317627\n",
            "step:31, loss:8.564491271972656\n",
            "step:32, loss:4.631590843200684\n",
            "step:33, loss:1.689954161643982\n",
            "step:34, loss:6.480132102966309\n",
            "step:35, loss:2.0072193145751953\n",
            "step:36, loss:14.148378372192383\n",
            "step:37, loss:2.255823850631714\n",
            "step:38, loss:30.89964485168457\n",
            "step:39, loss:2.788247585296631\n",
            "step:40, loss:3.7561426162719727\n",
            "step:41, loss:10.658670425415039\n",
            "step:42, loss:6.001389980316162\n",
            "step:43, loss:6.4486236572265625\n",
            "step:44, loss:4.1587018966674805\n",
            "step:45, loss:7.523553371429443\n",
            "step:46, loss:15.95802116394043\n",
            "step:47, loss:8.967357635498047\n",
            "step:48, loss:2.2346653938293457\n",
            "step:49, loss:7.571753025054932\n",
            "step:50, loss:4.154150009155273\n",
            "step:51, loss:6.0457634925842285\n",
            "step:52, loss:2.8630425930023193\n",
            "step:53, loss:2.7842845916748047\n",
            "step:54, loss:3.1397910118103027\n",
            "step:55, loss:1.9504185914993286\n",
            "step:56, loss:29.11610984802246\n",
            "step:57, loss:5.461024761199951\n",
            "step:58, loss:3.151461362838745\n",
            "step:59, loss:10.320182800292969\n",
            "step:60, loss:8.384308815002441\n",
            "step:61, loss:15.046972274780273\n",
            "step:62, loss:4.742403030395508\n",
            "step:63, loss:2.998775005340576\n",
            "step:64, loss:5.9323811531066895\n",
            "step:65, loss:10.342683792114258\n",
            "step:66, loss:7.274484634399414\n",
            "step:67, loss:6.3308258056640625\n",
            "step:68, loss:4.4665632247924805\n",
            "step:69, loss:6.853182315826416\n",
            "step:70, loss:3.3430681228637695\n",
            "step:71, loss:3.467334270477295\n",
            "step:72, loss:2.5805740356445312\n",
            "step:73, loss:4.26231050491333\n",
            "step:74, loss:2.8280551433563232\n",
            "step:75, loss:2.04186749458313\n",
            "step:76, loss:7.4263224601745605\n",
            "26\n",
            "-----\n",
            "step:0, loss:7.139729976654053\n",
            "step:1, loss:2.0534186363220215\n",
            "step:2, loss:7.935899257659912\n",
            "step:3, loss:88.35333251953125\n",
            "step:4, loss:34.250553131103516\n",
            "step:5, loss:0.9679839015007019\n",
            "step:6, loss:2.2184500694274902\n",
            "step:7, loss:3.5037636756896973\n",
            "step:8, loss:26.824140548706055\n",
            "step:9, loss:4.629598140716553\n",
            "step:10, loss:1.847807765007019\n",
            "step:11, loss:7.3899102210998535\n",
            "step:12, loss:2.9693217277526855\n",
            "step:13, loss:12.33211612701416\n",
            "step:14, loss:5.4386749267578125\n",
            "step:15, loss:4.057178497314453\n",
            "step:16, loss:8.753003120422363\n",
            "step:17, loss:11.059433937072754\n",
            "step:18, loss:3.093660354614258\n",
            "step:19, loss:1.7597630023956299\n",
            "step:20, loss:8.11735725402832\n",
            "step:21, loss:86.85506439208984\n",
            "step:22, loss:2.5771422386169434\n",
            "step:23, loss:3.978275775909424\n",
            "step:24, loss:2.6113743782043457\n",
            "step:25, loss:2.9969756603240967\n",
            "step:26, loss:2.5568227767944336\n",
            "step:27, loss:6.380186080932617\n",
            "step:28, loss:12.517928123474121\n",
            "step:29, loss:9.577021598815918\n",
            "step:30, loss:3.095465660095215\n",
            "step:31, loss:6.953182220458984\n",
            "step:32, loss:5.648552894592285\n",
            "step:33, loss:2.9050512313842773\n",
            "step:34, loss:3.2033157348632812\n",
            "step:35, loss:3.9754786491394043\n",
            "step:36, loss:10.377284049987793\n",
            "step:37, loss:6.901791572570801\n",
            "step:38, loss:7.041323661804199\n",
            "step:39, loss:5.095282077789307\n",
            "step:40, loss:4.35959529876709\n",
            "step:41, loss:5.016254425048828\n",
            "step:42, loss:3.7958319187164307\n",
            "step:43, loss:6.7466325759887695\n",
            "step:44, loss:6.313617706298828\n",
            "step:45, loss:4.19844388961792\n",
            "step:46, loss:3.9374520778656006\n",
            "step:47, loss:5.748244762420654\n",
            "step:48, loss:9.408526420593262\n",
            "step:49, loss:5.701048851013184\n",
            "step:50, loss:15.396125793457031\n",
            "step:51, loss:27.487892150878906\n",
            "step:52, loss:11.666111946105957\n",
            "step:53, loss:3.5254502296447754\n",
            "step:54, loss:2.8448915481567383\n",
            "step:55, loss:8.97574234008789\n",
            "step:56, loss:3.2570197582244873\n",
            "step:57, loss:5.257961750030518\n",
            "step:58, loss:3.1008670330047607\n",
            "step:59, loss:27.955373764038086\n",
            "step:60, loss:2.110525369644165\n",
            "step:61, loss:9.586935997009277\n",
            "step:62, loss:11.183494567871094\n",
            "step:63, loss:2.8766636848449707\n",
            "step:64, loss:3.156989574432373\n",
            "step:65, loss:2.3260903358459473\n",
            "step:66, loss:4.954998016357422\n",
            "step:67, loss:2.849642753601074\n",
            "step:68, loss:2.3556175231933594\n",
            "step:69, loss:3.0384724140167236\n",
            "step:70, loss:2.9170002937316895\n",
            "step:71, loss:2.7327499389648438\n",
            "step:72, loss:5.0295209884643555\n",
            "step:73, loss:2.970381259918213\n",
            "step:74, loss:32.570674896240234\n",
            "step:75, loss:2.5677356719970703\n",
            "step:76, loss:11.315829277038574\n",
            "27\n",
            "-----\n",
            "step:0, loss:2.418429374694824\n",
            "step:1, loss:2.4073777198791504\n",
            "step:2, loss:9.267049789428711\n",
            "step:3, loss:1.6607345342636108\n",
            "step:4, loss:23.867109298706055\n",
            "step:5, loss:3.3219361305236816\n",
            "step:6, loss:2.3231887817382812\n",
            "step:7, loss:5.995899677276611\n",
            "step:8, loss:2.1681323051452637\n",
            "step:9, loss:1.3124938011169434\n",
            "step:10, loss:11.546385765075684\n",
            "step:11, loss:3.3683338165283203\n",
            "step:12, loss:2.878408908843994\n",
            "step:13, loss:2.4743831157684326\n",
            "step:14, loss:27.533977508544922\n",
            "step:15, loss:4.505321979522705\n",
            "step:16, loss:2.010223388671875\n",
            "step:17, loss:3.044649600982666\n",
            "step:18, loss:3.503054618835449\n",
            "step:19, loss:2.534827709197998\n",
            "step:20, loss:3.3519093990325928\n",
            "step:21, loss:2.997976541519165\n",
            "step:22, loss:2.4810526371002197\n",
            "step:23, loss:3.065260887145996\n",
            "step:24, loss:3.0449905395507812\n",
            "step:25, loss:7.0982842445373535\n",
            "step:26, loss:5.6775360107421875\n",
            "step:27, loss:6.105679512023926\n",
            "step:28, loss:2.0730154514312744\n",
            "step:29, loss:6.150525093078613\n",
            "step:30, loss:3.0260767936706543\n",
            "step:31, loss:3.559579849243164\n",
            "step:32, loss:11.804511070251465\n",
            "step:33, loss:84.8449478149414\n",
            "step:34, loss:3.091487407684326\n",
            "step:35, loss:3.7478275299072266\n",
            "step:36, loss:7.048203468322754\n",
            "step:37, loss:2.473170757293701\n",
            "step:38, loss:5.121620178222656\n",
            "step:39, loss:3.6544086933135986\n",
            "step:40, loss:5.449454307556152\n",
            "step:41, loss:13.730213165283203\n",
            "step:42, loss:32.15640640258789\n",
            "step:43, loss:2.260040044784546\n",
            "step:44, loss:5.312481880187988\n",
            "step:45, loss:88.06600189208984\n",
            "step:46, loss:2.506643772125244\n",
            "step:47, loss:1.2551153898239136\n",
            "step:48, loss:4.579821586608887\n",
            "step:49, loss:5.141829967498779\n",
            "step:50, loss:7.781495094299316\n",
            "step:51, loss:7.541682243347168\n",
            "step:52, loss:4.451737403869629\n",
            "step:53, loss:25.89002227783203\n",
            "step:54, loss:9.423295021057129\n",
            "step:55, loss:7.538507461547852\n",
            "step:56, loss:17.66876983642578\n",
            "step:57, loss:10.142165184020996\n",
            "step:58, loss:2.1245641708374023\n",
            "step:59, loss:8.323843955993652\n",
            "step:60, loss:11.063972473144531\n",
            "step:61, loss:10.641008377075195\n",
            "step:62, loss:7.787590980529785\n",
            "step:63, loss:2.924683094024658\n",
            "step:64, loss:9.725406646728516\n",
            "step:65, loss:3.7198963165283203\n",
            "step:66, loss:5.302004814147949\n",
            "step:67, loss:26.939590454101562\n",
            "step:68, loss:2.41343092918396\n",
            "step:69, loss:4.117318630218506\n",
            "step:70, loss:10.934837341308594\n",
            "step:71, loss:3.798715114593506\n",
            "step:72, loss:2.9384984970092773\n",
            "step:73, loss:4.865850448608398\n",
            "step:74, loss:10.929910659790039\n",
            "step:75, loss:6.547104358673096\n",
            "step:76, loss:15.319625854492188\n",
            "28\n",
            "-----\n",
            "step:0, loss:3.163522958755493\n",
            "step:1, loss:3.2791361808776855\n",
            "step:2, loss:3.221066951751709\n",
            "step:3, loss:3.9296975135803223\n",
            "step:4, loss:2.570136070251465\n",
            "step:5, loss:1.7814857959747314\n",
            "step:6, loss:6.1076202392578125\n",
            "step:7, loss:16.98871421813965\n",
            "step:8, loss:4.056278705596924\n",
            "step:9, loss:6.6830291748046875\n",
            "step:10, loss:10.425650596618652\n",
            "step:11, loss:4.099716663360596\n",
            "step:12, loss:2.2412033081054688\n",
            "step:13, loss:5.2901082038879395\n",
            "step:14, loss:2.572075843811035\n",
            "step:15, loss:2.6128218173980713\n",
            "step:16, loss:1.64253830909729\n",
            "step:17, loss:1.7944226264953613\n",
            "step:18, loss:1.5546928644180298\n",
            "step:19, loss:1.9837639331817627\n",
            "step:20, loss:4.014466285705566\n",
            "step:21, loss:1.9872992038726807\n",
            "step:22, loss:2.7366855144500732\n",
            "step:23, loss:1.879186749458313\n",
            "step:24, loss:8.040650367736816\n",
            "step:25, loss:3.231875419616699\n",
            "step:26, loss:29.998830795288086\n",
            "step:27, loss:5.492037773132324\n",
            "step:28, loss:4.520101547241211\n",
            "step:29, loss:1.796600341796875\n",
            "step:30, loss:29.21019744873047\n",
            "step:31, loss:3.5022735595703125\n",
            "step:32, loss:14.222355842590332\n",
            "step:33, loss:2.2817158699035645\n",
            "step:34, loss:4.518594264984131\n",
            "step:35, loss:7.705822467803955\n",
            "step:36, loss:2.903632640838623\n",
            "step:37, loss:28.36328125\n",
            "step:38, loss:3.7321062088012695\n",
            "step:39, loss:3.547905445098877\n",
            "step:40, loss:2.8593389987945557\n",
            "step:41, loss:3.8141448497772217\n",
            "step:42, loss:96.58780670166016\n",
            "step:43, loss:28.900115966796875\n",
            "step:44, loss:4.803861141204834\n",
            "step:45, loss:1.4811534881591797\n",
            "step:46, loss:1.967128038406372\n",
            "step:47, loss:8.00631332397461\n",
            "step:48, loss:7.199416160583496\n",
            "step:49, loss:3.688321590423584\n",
            "step:50, loss:2.5606346130371094\n",
            "step:51, loss:1.1062928438186646\n",
            "step:52, loss:2.7305190563201904\n",
            "step:53, loss:2.797671318054199\n",
            "step:54, loss:2.639282703399658\n",
            "step:55, loss:7.750617027282715\n",
            "step:56, loss:5.319234848022461\n",
            "step:57, loss:4.914134979248047\n",
            "step:58, loss:2.5086793899536133\n",
            "step:59, loss:4.318331241607666\n",
            "step:60, loss:4.048116683959961\n",
            "step:61, loss:5.690768718719482\n",
            "step:62, loss:4.241090774536133\n",
            "step:63, loss:84.22274780273438\n",
            "step:64, loss:5.79926872253418\n",
            "step:65, loss:1.3770997524261475\n",
            "step:66, loss:2.743269681930542\n",
            "step:67, loss:4.524669170379639\n",
            "step:68, loss:18.402652740478516\n",
            "step:69, loss:33.528839111328125\n",
            "step:70, loss:5.262466907501221\n",
            "step:71, loss:4.871532440185547\n",
            "step:72, loss:2.340005874633789\n",
            "step:73, loss:3.0242490768432617\n",
            "step:74, loss:14.518092155456543\n",
            "step:75, loss:10.482462882995605\n",
            "step:76, loss:1.7047533988952637\n",
            "29\n",
            "-----\n",
            "step:0, loss:5.662423133850098\n",
            "step:1, loss:2.2285590171813965\n",
            "step:2, loss:6.687922954559326\n",
            "step:3, loss:4.024514198303223\n",
            "step:4, loss:9.149693489074707\n",
            "step:5, loss:5.801236629486084\n",
            "step:6, loss:7.219453811645508\n",
            "step:7, loss:6.088549613952637\n",
            "step:8, loss:1.09517240524292\n",
            "step:9, loss:2.90964937210083\n",
            "step:10, loss:3.3610754013061523\n",
            "step:11, loss:1.115351915359497\n",
            "step:12, loss:5.052713394165039\n",
            "step:13, loss:2.6584644317626953\n",
            "step:14, loss:2.2509243488311768\n",
            "step:15, loss:5.398423194885254\n",
            "step:16, loss:2.124574899673462\n",
            "step:17, loss:2.096433639526367\n",
            "step:18, loss:3.1087093353271484\n",
            "step:19, loss:3.2511441707611084\n",
            "step:20, loss:5.432501792907715\n",
            "step:21, loss:2.566622734069824\n",
            "step:22, loss:2.703864812850952\n",
            "step:23, loss:3.256610870361328\n",
            "step:24, loss:2.672511577606201\n",
            "step:25, loss:8.53930377960205\n",
            "step:26, loss:3.2206051349639893\n",
            "step:27, loss:1.7647991180419922\n",
            "step:28, loss:9.744951248168945\n",
            "step:29, loss:4.7502121925354\n",
            "step:30, loss:3.4508886337280273\n",
            "step:31, loss:5.224821090698242\n",
            "step:32, loss:24.839683532714844\n",
            "step:33, loss:17.856966018676758\n",
            "step:34, loss:10.82759952545166\n",
            "step:35, loss:27.793197631835938\n",
            "step:36, loss:4.666037559509277\n",
            "step:37, loss:15.595834732055664\n",
            "step:38, loss:11.127923011779785\n",
            "step:39, loss:4.013686180114746\n",
            "step:40, loss:85.14588165283203\n",
            "step:41, loss:4.194213390350342\n",
            "step:42, loss:6.154473781585693\n",
            "step:43, loss:7.392662048339844\n",
            "step:44, loss:10.422013282775879\n",
            "step:45, loss:4.145206451416016\n",
            "step:46, loss:87.6561050415039\n",
            "step:47, loss:3.259739398956299\n",
            "step:48, loss:3.248697519302368\n",
            "step:49, loss:1.742824673652649\n",
            "step:50, loss:2.285207509994507\n",
            "step:51, loss:4.957851409912109\n",
            "step:52, loss:4.326831817626953\n",
            "step:53, loss:6.401918411254883\n",
            "step:54, loss:3.852304458618164\n",
            "step:55, loss:9.82628345489502\n",
            "step:56, loss:9.173789978027344\n",
            "step:57, loss:25.06082534790039\n",
            "step:58, loss:2.8366780281066895\n",
            "step:59, loss:6.040484428405762\n",
            "step:60, loss:9.784674644470215\n",
            "step:61, loss:9.545433044433594\n",
            "step:62, loss:4.923924446105957\n",
            "step:63, loss:9.687299728393555\n",
            "step:64, loss:3.3736724853515625\n",
            "step:65, loss:7.886438846588135\n",
            "step:66, loss:7.262871265411377\n",
            "step:67, loss:3.6086859703063965\n",
            "step:68, loss:4.159771919250488\n",
            "step:69, loss:6.7230095863342285\n",
            "step:70, loss:23.61884880065918\n",
            "step:71, loss:1.2749683856964111\n",
            "step:72, loss:6.8583149909973145\n",
            "step:73, loss:3.104658365249634\n",
            "step:74, loss:3.72770094871521\n",
            "step:75, loss:19.046894073486328\n",
            "step:76, loss:2.2594707012176514\n",
            "30\n",
            "-----\n",
            "step:0, loss:4.481388092041016\n",
            "step:1, loss:2.0735301971435547\n",
            "step:2, loss:3.068305730819702\n",
            "step:3, loss:16.772655487060547\n",
            "step:4, loss:2.5459651947021484\n",
            "step:5, loss:2.8114676475524902\n",
            "step:6, loss:80.86809539794922\n",
            "step:7, loss:1.2280093431472778\n",
            "step:8, loss:3.152202844619751\n",
            "step:9, loss:4.555208683013916\n",
            "step:10, loss:2.7692148685455322\n",
            "step:11, loss:2.7325117588043213\n",
            "step:12, loss:5.0752739906311035\n",
            "step:13, loss:10.309640884399414\n",
            "step:14, loss:1.3533964157104492\n",
            "step:15, loss:2.4104995727539062\n",
            "step:16, loss:8.538618087768555\n",
            "step:17, loss:5.61189079284668\n",
            "step:18, loss:2.0375583171844482\n",
            "step:19, loss:2.451401710510254\n",
            "step:20, loss:6.259280681610107\n",
            "step:21, loss:16.594606399536133\n",
            "step:22, loss:1.8470919132232666\n",
            "step:23, loss:2.3884499073028564\n",
            "step:24, loss:1.902968406677246\n",
            "step:25, loss:2.628946542739868\n",
            "step:26, loss:6.2403459548950195\n",
            "step:27, loss:2.0838944911956787\n",
            "step:28, loss:31.510324478149414\n",
            "step:29, loss:9.82400894165039\n",
            "step:30, loss:29.887968063354492\n",
            "step:31, loss:5.261750221252441\n",
            "step:32, loss:2.0120885372161865\n",
            "step:33, loss:8.070396423339844\n",
            "step:34, loss:8.061171531677246\n",
            "step:35, loss:84.29983520507812\n",
            "step:36, loss:2.5913195610046387\n",
            "step:37, loss:4.39725399017334\n",
            "step:38, loss:2.8751349449157715\n",
            "step:39, loss:2.7359800338745117\n",
            "step:40, loss:3.328335762023926\n",
            "step:41, loss:8.141459465026855\n",
            "step:42, loss:3.4691619873046875\n",
            "step:43, loss:2.9683971405029297\n",
            "step:44, loss:2.9067087173461914\n",
            "step:45, loss:2.6524667739868164\n",
            "step:46, loss:7.153128147125244\n",
            "step:47, loss:2.8380861282348633\n",
            "step:48, loss:4.959262847900391\n",
            "step:49, loss:7.298478126525879\n",
            "step:50, loss:1.9957060813903809\n",
            "step:51, loss:1.8389801979064941\n",
            "step:52, loss:7.2580718994140625\n",
            "step:53, loss:1.9635999202728271\n",
            "step:54, loss:3.6392202377319336\n",
            "step:55, loss:11.357799530029297\n",
            "step:56, loss:1.5451792478561401\n",
            "step:57, loss:5.329611301422119\n",
            "step:58, loss:25.77858543395996\n",
            "step:59, loss:6.27747917175293\n",
            "step:60, loss:2.268465518951416\n",
            "step:61, loss:1.9021389484405518\n",
            "step:62, loss:23.327817916870117\n",
            "step:63, loss:2.9556665420532227\n",
            "step:64, loss:4.112143039703369\n",
            "step:65, loss:1.4567058086395264\n",
            "step:66, loss:4.525259494781494\n",
            "step:67, loss:1.3681734800338745\n",
            "step:68, loss:17.88840675354004\n",
            "step:69, loss:3.3841865062713623\n",
            "step:70, loss:10.118301391601562\n",
            "step:71, loss:5.577608585357666\n",
            "step:72, loss:8.378763198852539\n",
            "step:73, loss:2.872304916381836\n",
            "step:74, loss:8.44746208190918\n",
            "step:75, loss:8.544540405273438\n",
            "step:76, loss:3.5322415828704834\n",
            "31\n",
            "-----\n",
            "step:0, loss:3.5436038970947266\n",
            "step:1, loss:1.6419955492019653\n",
            "step:2, loss:8.204449653625488\n",
            "step:3, loss:5.628178596496582\n",
            "step:4, loss:3.4278500080108643\n",
            "step:5, loss:3.5646603107452393\n",
            "step:6, loss:23.912696838378906\n",
            "step:7, loss:3.293720245361328\n",
            "step:8, loss:3.9825263023376465\n",
            "step:9, loss:1.3594248294830322\n",
            "step:10, loss:2.566110849380493\n",
            "step:11, loss:1.2633450031280518\n",
            "step:12, loss:5.040069103240967\n",
            "step:13, loss:1.8568660020828247\n",
            "step:14, loss:4.586799621582031\n",
            "step:15, loss:1.9370133876800537\n",
            "step:16, loss:1.9199774265289307\n",
            "step:17, loss:5.170668125152588\n",
            "step:18, loss:3.9474525451660156\n",
            "step:19, loss:1.3218096494674683\n",
            "step:20, loss:2.8202781677246094\n",
            "step:21, loss:9.839349746704102\n",
            "step:22, loss:9.810518264770508\n",
            "step:23, loss:2.400540828704834\n",
            "step:24, loss:4.292139530181885\n",
            "step:25, loss:4.153256416320801\n",
            "step:26, loss:8.809247016906738\n",
            "step:27, loss:3.0516104698181152\n",
            "step:28, loss:1.899221658706665\n",
            "step:29, loss:8.103672981262207\n",
            "step:30, loss:14.555887222290039\n",
            "step:31, loss:2.9858882427215576\n",
            "step:32, loss:4.2714715003967285\n",
            "step:33, loss:4.0817155838012695\n",
            "step:34, loss:2.6998772621154785\n",
            "step:35, loss:83.63542938232422\n",
            "step:36, loss:3.096370220184326\n",
            "step:37, loss:83.06559753417969\n",
            "step:38, loss:8.18020248413086\n",
            "step:39, loss:24.731456756591797\n",
            "step:40, loss:9.723089218139648\n",
            "step:41, loss:2.72232985496521\n",
            "step:42, loss:3.8263158798217773\n",
            "step:43, loss:29.778772354125977\n",
            "step:44, loss:6.800130844116211\n",
            "step:45, loss:2.2234039306640625\n",
            "step:46, loss:2.388411283493042\n",
            "step:47, loss:4.115192890167236\n",
            "step:48, loss:28.387664794921875\n",
            "step:49, loss:1.4340145587921143\n",
            "step:50, loss:3.4558792114257812\n",
            "step:51, loss:6.609859466552734\n",
            "step:52, loss:2.377458095550537\n",
            "step:53, loss:5.054162979125977\n",
            "step:54, loss:2.3436059951782227\n",
            "step:55, loss:13.050434112548828\n",
            "step:56, loss:2.095918655395508\n",
            "step:57, loss:5.475743293762207\n",
            "step:58, loss:1.6245123147964478\n",
            "step:59, loss:2.614130735397339\n",
            "step:60, loss:3.413390874862671\n",
            "step:61, loss:4.9040846824646\n",
            "step:62, loss:3.3757505416870117\n",
            "step:63, loss:22.84180450439453\n",
            "step:64, loss:1.7314575910568237\n",
            "step:65, loss:10.289108276367188\n",
            "step:66, loss:3.2482714653015137\n",
            "step:67, loss:2.7018942832946777\n",
            "step:68, loss:2.932404041290283\n",
            "step:69, loss:4.506354331970215\n",
            "step:70, loss:1.9706313610076904\n",
            "step:71, loss:3.956958293914795\n",
            "step:72, loss:2.2557926177978516\n",
            "step:73, loss:5.388755798339844\n",
            "step:74, loss:1.1559934616088867\n",
            "step:75, loss:5.174684047698975\n",
            "step:76, loss:10.826790809631348\n",
            "32\n",
            "-----\n",
            "step:0, loss:1.5491135120391846\n",
            "step:1, loss:77.97726440429688\n",
            "step:2, loss:2.0225729942321777\n",
            "step:3, loss:1.98771333694458\n",
            "step:4, loss:1.5723426342010498\n",
            "step:5, loss:2.4199886322021484\n",
            "step:6, loss:2.059413433074951\n",
            "step:7, loss:2.4340057373046875\n",
            "step:8, loss:2.7778334617614746\n",
            "step:9, loss:1.865844488143921\n",
            "step:10, loss:8.433502197265625\n",
            "step:11, loss:2.696019172668457\n",
            "step:12, loss:52.8201904296875\n",
            "step:13, loss:8.615952491760254\n",
            "step:14, loss:3.3186099529266357\n",
            "step:15, loss:1.6436970233917236\n",
            "step:16, loss:1.9176636934280396\n",
            "step:17, loss:2.8457858562469482\n",
            "step:18, loss:7.909170627593994\n",
            "step:19, loss:2.9073691368103027\n",
            "step:20, loss:2.4195680618286133\n",
            "step:21, loss:3.1737866401672363\n",
            "step:22, loss:3.2001075744628906\n",
            "step:23, loss:2.4951722621917725\n",
            "step:24, loss:3.570408344268799\n",
            "step:25, loss:3.1223247051239014\n",
            "step:26, loss:4.4738359451293945\n",
            "step:27, loss:4.702750205993652\n",
            "step:28, loss:21.650951385498047\n",
            "step:29, loss:10.73208999633789\n",
            "step:30, loss:1.8129184246063232\n",
            "step:31, loss:13.055418968200684\n",
            "step:32, loss:20.236217498779297\n",
            "step:33, loss:10.103925704956055\n",
            "step:34, loss:3.201399803161621\n",
            "step:35, loss:2.1109204292297363\n",
            "step:36, loss:3.951770782470703\n",
            "step:37, loss:2.243995428085327\n",
            "step:38, loss:2.5021209716796875\n",
            "step:39, loss:7.5152587890625\n",
            "step:40, loss:3.6662540435791016\n",
            "step:41, loss:1.00103759765625\n",
            "step:42, loss:2.2094364166259766\n",
            "step:43, loss:2.2191333770751953\n",
            "step:44, loss:2.6313095092773438\n",
            "step:45, loss:3.6988468170166016\n",
            "step:46, loss:2.2476401329040527\n",
            "step:47, loss:10.876519203186035\n",
            "step:48, loss:8.602049827575684\n",
            "step:49, loss:3.8243696689605713\n",
            "step:50, loss:2.133587121963501\n",
            "step:51, loss:80.75899505615234\n",
            "step:52, loss:3.6728529930114746\n",
            "step:53, loss:8.824100494384766\n",
            "step:54, loss:13.141959190368652\n",
            "step:55, loss:9.412323951721191\n",
            "step:56, loss:3.80424165725708\n",
            "step:57, loss:2.8246474266052246\n",
            "step:58, loss:1.8571407794952393\n",
            "step:59, loss:7.157485008239746\n",
            "step:60, loss:11.024539947509766\n",
            "step:61, loss:5.258283615112305\n",
            "step:62, loss:3.720695972442627\n",
            "step:63, loss:4.501411437988281\n",
            "step:64, loss:3.588984489440918\n",
            "step:65, loss:5.941861152648926\n",
            "step:66, loss:8.344205856323242\n",
            "step:67, loss:4.132534503936768\n",
            "step:68, loss:8.885615348815918\n",
            "step:69, loss:5.920732021331787\n",
            "step:70, loss:4.440324306488037\n",
            "step:71, loss:1.9618779420852661\n",
            "step:72, loss:4.234290599822998\n",
            "step:73, loss:7.407681465148926\n",
            "step:74, loss:1.7406151294708252\n",
            "step:75, loss:24.656938552856445\n",
            "step:76, loss:4.1530280113220215\n",
            "33\n",
            "-----\n",
            "step:0, loss:4.742161750793457\n",
            "step:1, loss:3.6029787063598633\n",
            "step:2, loss:4.124283790588379\n",
            "step:3, loss:2.8092949390411377\n",
            "step:4, loss:1.8496477603912354\n",
            "step:5, loss:4.93675422668457\n",
            "step:6, loss:6.5456438064575195\n",
            "step:7, loss:22.860639572143555\n",
            "step:8, loss:4.776914596557617\n",
            "step:9, loss:7.867862224578857\n",
            "step:10, loss:2.4733662605285645\n",
            "step:11, loss:2.5450448989868164\n",
            "step:12, loss:2.022524356842041\n",
            "step:13, loss:2.120572566986084\n",
            "step:14, loss:7.838571071624756\n",
            "step:15, loss:2.996016025543213\n",
            "step:16, loss:2.1668691635131836\n",
            "step:17, loss:2.5182933807373047\n",
            "step:18, loss:1.2464654445648193\n",
            "step:19, loss:4.8610124588012695\n",
            "step:20, loss:8.848404884338379\n",
            "step:21, loss:4.1233720779418945\n",
            "step:22, loss:27.498138427734375\n",
            "step:23, loss:4.468636989593506\n",
            "step:24, loss:3.0459070205688477\n",
            "step:25, loss:4.798327445983887\n",
            "step:26, loss:3.054582118988037\n",
            "step:27, loss:8.165687561035156\n",
            "step:28, loss:1.8566462993621826\n",
            "step:29, loss:3.9091637134552\n",
            "step:30, loss:2.2600576877593994\n",
            "step:31, loss:1.0901916027069092\n",
            "step:32, loss:19.78567886352539\n",
            "step:33, loss:2.3117525577545166\n",
            "step:34, loss:2.8221142292022705\n",
            "step:35, loss:1.5829269886016846\n",
            "step:36, loss:21.80222511291504\n",
            "step:37, loss:3.1462059020996094\n",
            "step:38, loss:4.416138172149658\n",
            "step:39, loss:4.586606025695801\n",
            "step:40, loss:2.226879358291626\n",
            "step:41, loss:2.096592903137207\n",
            "step:42, loss:5.035109996795654\n",
            "step:43, loss:3.4393150806427\n",
            "step:44, loss:1.7949261665344238\n",
            "step:45, loss:1.9694263935089111\n",
            "step:46, loss:1.4996551275253296\n",
            "step:47, loss:10.276498794555664\n",
            "step:48, loss:1.6517006158828735\n",
            "step:49, loss:24.294870376586914\n",
            "step:50, loss:2.566311836242676\n",
            "step:51, loss:4.937130451202393\n",
            "step:52, loss:3.2523465156555176\n",
            "step:53, loss:9.424703598022461\n",
            "step:54, loss:1.4066975116729736\n",
            "step:55, loss:6.694101333618164\n",
            "step:56, loss:3.921515464782715\n",
            "step:57, loss:12.662099838256836\n",
            "step:58, loss:3.4016175270080566\n",
            "step:59, loss:8.771272659301758\n",
            "step:60, loss:4.425623416900635\n",
            "step:61, loss:5.739142417907715\n",
            "step:62, loss:2.845623731613159\n",
            "step:63, loss:10.583858489990234\n",
            "step:64, loss:81.63352966308594\n",
            "step:65, loss:2.4881019592285156\n",
            "step:66, loss:76.08094024658203\n",
            "step:67, loss:6.147558689117432\n",
            "step:68, loss:20.073083877563477\n",
            "step:69, loss:2.535325527191162\n",
            "step:70, loss:2.8286471366882324\n",
            "step:71, loss:8.502157211303711\n",
            "step:72, loss:1.2268503904342651\n",
            "step:73, loss:1.8577079772949219\n",
            "step:74, loss:1.589240550994873\n",
            "step:75, loss:1.8195273876190186\n",
            "step:76, loss:1.8197133541107178\n",
            "34\n",
            "-----\n",
            "step:0, loss:1.5113215446472168\n",
            "step:1, loss:0.9930204153060913\n",
            "step:2, loss:2.6834540367126465\n",
            "step:3, loss:3.197617530822754\n",
            "step:4, loss:8.462713241577148\n",
            "step:5, loss:2.3140370845794678\n",
            "step:6, loss:9.406548500061035\n",
            "step:7, loss:30.938997268676758\n",
            "step:8, loss:8.269007682800293\n",
            "step:9, loss:1.9722297191619873\n",
            "step:10, loss:4.157348155975342\n",
            "step:11, loss:2.4361703395843506\n",
            "step:12, loss:4.07432222366333\n",
            "step:13, loss:2.2649478912353516\n",
            "step:14, loss:2.4243862628936768\n",
            "step:15, loss:3.904014825820923\n",
            "step:16, loss:2.825164794921875\n",
            "step:17, loss:5.364500045776367\n",
            "step:18, loss:16.840084075927734\n",
            "step:19, loss:1.7563629150390625\n",
            "step:20, loss:3.13201642036438\n",
            "step:21, loss:2.662569522857666\n",
            "step:22, loss:3.485062837600708\n",
            "step:23, loss:107.26558685302734\n",
            "step:24, loss:22.052013397216797\n",
            "step:25, loss:4.81580114364624\n",
            "step:26, loss:75.957763671875\n",
            "step:27, loss:3.247009038925171\n",
            "step:28, loss:1.9350862503051758\n",
            "step:29, loss:23.264144897460938\n",
            "step:30, loss:4.227896213531494\n",
            "step:31, loss:1.8419818878173828\n",
            "step:32, loss:2.9832873344421387\n",
            "step:33, loss:1.8716068267822266\n",
            "step:34, loss:3.605253219604492\n",
            "step:35, loss:2.207643985748291\n",
            "step:36, loss:2.7284116744995117\n",
            "step:37, loss:10.779770851135254\n",
            "step:38, loss:4.283735275268555\n",
            "step:39, loss:1.81520414352417\n",
            "step:40, loss:1.0751254558563232\n",
            "step:41, loss:1.8869010210037231\n",
            "step:42, loss:2.582327365875244\n",
            "step:43, loss:2.139244318008423\n",
            "step:44, loss:4.463588714599609\n",
            "step:45, loss:8.285115242004395\n",
            "step:46, loss:4.377145290374756\n",
            "step:47, loss:2.4991531372070312\n",
            "step:48, loss:1.5734736919403076\n",
            "step:49, loss:2.983765125274658\n",
            "step:50, loss:10.45839786529541\n",
            "step:51, loss:3.139103412628174\n",
            "step:52, loss:2.9737539291381836\n",
            "step:53, loss:2.7911696434020996\n",
            "step:54, loss:2.6050992012023926\n",
            "step:55, loss:5.7506103515625\n",
            "step:56, loss:2.720667600631714\n",
            "step:57, loss:3.1929636001586914\n",
            "step:58, loss:4.463070869445801\n",
            "step:59, loss:2.5827319622039795\n",
            "step:60, loss:23.917884826660156\n",
            "step:61, loss:1.321617841720581\n",
            "step:62, loss:7.881213188171387\n",
            "step:63, loss:17.801406860351562\n",
            "step:64, loss:2.1161890029907227\n",
            "step:65, loss:1.7748435735702515\n",
            "step:66, loss:2.663440465927124\n",
            "step:67, loss:2.5322258472442627\n",
            "step:68, loss:9.865324020385742\n",
            "step:69, loss:2.853534460067749\n",
            "step:70, loss:1.7583038806915283\n",
            "step:71, loss:5.5007429122924805\n",
            "step:72, loss:1.7554563283920288\n",
            "step:73, loss:5.414403915405273\n",
            "step:74, loss:1.2858080863952637\n",
            "step:75, loss:1.9207885265350342\n",
            "step:76, loss:4.428058624267578\n",
            "35\n",
            "-----\n",
            "step:0, loss:3.015838623046875\n",
            "step:1, loss:7.826011657714844\n",
            "step:2, loss:1.390714168548584\n",
            "step:3, loss:8.775914192199707\n",
            "step:4, loss:5.95421028137207\n",
            "step:5, loss:0.890451967716217\n",
            "step:6, loss:4.590816974639893\n",
            "step:7, loss:2.505033493041992\n",
            "step:8, loss:3.1392345428466797\n",
            "step:9, loss:4.435524940490723\n",
            "step:10, loss:2.8997576236724854\n",
            "step:11, loss:3.2447383403778076\n",
            "step:12, loss:2.4558889865875244\n",
            "step:13, loss:2.6191294193267822\n",
            "step:14, loss:5.369000434875488\n",
            "step:15, loss:94.12198638916016\n",
            "step:16, loss:2.390138626098633\n",
            "step:17, loss:2.3650929927825928\n",
            "step:18, loss:9.82058334350586\n",
            "step:19, loss:2.4105281829833984\n",
            "step:20, loss:3.751844882965088\n",
            "step:21, loss:4.011876106262207\n",
            "step:22, loss:4.261507034301758\n",
            "step:23, loss:6.6646928787231445\n",
            "step:24, loss:2.6834216117858887\n",
            "step:25, loss:4.04538106918335\n",
            "step:26, loss:2.1534647941589355\n",
            "step:27, loss:5.494253158569336\n",
            "step:28, loss:3.404697895050049\n",
            "step:29, loss:3.729316473007202\n",
            "step:30, loss:12.177322387695312\n",
            "step:31, loss:1.2053608894348145\n",
            "step:32, loss:4.9836745262146\n",
            "step:33, loss:1.6681233644485474\n",
            "step:34, loss:3.288140296936035\n",
            "step:35, loss:1.7091397047042847\n",
            "step:36, loss:23.426361083984375\n",
            "step:37, loss:7.511779308319092\n",
            "step:38, loss:1.5801422595977783\n",
            "step:39, loss:2.148275852203369\n",
            "step:40, loss:2.9664151668548584\n",
            "step:41, loss:2.4948179721832275\n",
            "step:42, loss:1.421168565750122\n",
            "step:43, loss:7.760051727294922\n",
            "step:44, loss:1.9353376626968384\n",
            "step:45, loss:11.229596138000488\n",
            "step:46, loss:1.2713751792907715\n",
            "step:47, loss:19.4393253326416\n",
            "step:48, loss:3.3579092025756836\n",
            "step:49, loss:3.5454258918762207\n",
            "step:50, loss:25.279687881469727\n",
            "step:51, loss:15.406721115112305\n",
            "step:52, loss:4.285921573638916\n",
            "step:53, loss:2.513546943664551\n",
            "step:54, loss:1.6035211086273193\n",
            "step:55, loss:7.144357204437256\n",
            "step:56, loss:2.2471861839294434\n",
            "step:57, loss:20.035646438598633\n",
            "step:58, loss:4.101458549499512\n",
            "step:59, loss:77.02843475341797\n",
            "step:60, loss:1.2024319171905518\n",
            "step:61, loss:2.7699713706970215\n",
            "step:62, loss:4.254276275634766\n",
            "step:63, loss:1.9496815204620361\n",
            "step:64, loss:3.5496788024902344\n",
            "step:65, loss:1.2295796871185303\n",
            "step:66, loss:1.0634263753890991\n",
            "step:67, loss:4.139535903930664\n",
            "step:68, loss:3.1294403076171875\n",
            "step:69, loss:1.3767578601837158\n",
            "step:70, loss:8.853253364562988\n",
            "step:71, loss:2.2305142879486084\n",
            "step:72, loss:2.0818262100219727\n",
            "step:73, loss:3.2343697547912598\n",
            "step:74, loss:2.9759979248046875\n",
            "step:75, loss:1.9815349578857422\n",
            "step:76, loss:2.706568956375122\n",
            "36\n",
            "-----\n",
            "step:0, loss:1.8808833360671997\n",
            "step:1, loss:2.2503390312194824\n",
            "step:2, loss:11.554359436035156\n",
            "step:3, loss:2.056478261947632\n",
            "step:4, loss:3.331552028656006\n",
            "step:5, loss:2.519087314605713\n",
            "step:6, loss:1.9591115713119507\n",
            "step:7, loss:0.9682972431182861\n",
            "step:8, loss:3.24575138092041\n",
            "step:9, loss:1.2040094137191772\n",
            "step:10, loss:1.3795548677444458\n",
            "step:11, loss:21.20990753173828\n",
            "step:12, loss:4.405063629150391\n",
            "step:13, loss:3.770203113555908\n",
            "step:14, loss:1.0851421356201172\n",
            "step:15, loss:8.666918754577637\n",
            "step:16, loss:9.647686004638672\n",
            "step:17, loss:2.6640820503234863\n",
            "step:18, loss:6.3959174156188965\n",
            "step:19, loss:2.5781288146972656\n",
            "step:20, loss:80.12091064453125\n",
            "step:21, loss:3.7255730628967285\n",
            "step:22, loss:2.002408504486084\n",
            "step:23, loss:1.5747125148773193\n",
            "step:24, loss:3.1192855834960938\n",
            "step:25, loss:3.127533435821533\n",
            "step:26, loss:6.576386451721191\n",
            "step:27, loss:4.396365642547607\n",
            "step:28, loss:78.00830841064453\n",
            "step:29, loss:9.755675315856934\n",
            "step:30, loss:3.1705551147460938\n",
            "step:31, loss:1.1297813653945923\n",
            "step:32, loss:2.607776165008545\n",
            "step:33, loss:1.767825961112976\n",
            "step:34, loss:2.5162150859832764\n",
            "step:35, loss:1.9241456985473633\n",
            "step:36, loss:1.3213789463043213\n",
            "step:37, loss:2.491952419281006\n",
            "step:38, loss:1.9124417304992676\n",
            "step:39, loss:5.289243698120117\n",
            "step:40, loss:2.1956727504730225\n",
            "step:41, loss:3.1134161949157715\n",
            "step:42, loss:1.154625415802002\n",
            "step:43, loss:8.15207576751709\n",
            "step:44, loss:1.3899784088134766\n",
            "step:45, loss:5.709051132202148\n",
            "step:46, loss:0.8352476954460144\n",
            "step:47, loss:2.686553478240967\n",
            "step:48, loss:1.9490859508514404\n",
            "step:49, loss:1.199448823928833\n",
            "step:50, loss:3.2988955974578857\n",
            "step:51, loss:7.9266815185546875\n",
            "step:52, loss:3.232788562774658\n",
            "step:53, loss:1.6249734163284302\n",
            "step:54, loss:1.5643523931503296\n",
            "step:55, loss:6.634668350219727\n",
            "step:56, loss:4.871347427368164\n",
            "step:57, loss:1.3737151622772217\n",
            "step:58, loss:26.217266082763672\n",
            "step:59, loss:1.8692550659179688\n",
            "step:60, loss:20.2868595123291\n",
            "step:61, loss:5.909542560577393\n",
            "step:62, loss:8.728460311889648\n",
            "step:63, loss:2.168109655380249\n",
            "step:64, loss:1.5371153354644775\n",
            "step:65, loss:4.871100902557373\n",
            "step:66, loss:3.9239299297332764\n",
            "step:67, loss:12.166891098022461\n",
            "step:68, loss:2.249696731567383\n",
            "step:69, loss:4.198095798492432\n",
            "step:70, loss:13.30675220489502\n",
            "step:71, loss:14.524663925170898\n",
            "step:72, loss:20.678800582885742\n",
            "step:73, loss:3.8789913654327393\n",
            "step:74, loss:1.9959626197814941\n",
            "step:75, loss:5.426887512207031\n",
            "step:76, loss:3.4610464572906494\n",
            "37\n",
            "-----\n",
            "step:0, loss:4.052129745483398\n",
            "step:1, loss:4.607449531555176\n",
            "step:2, loss:1.8403433561325073\n",
            "step:3, loss:24.087661743164062\n",
            "step:4, loss:3.0549042224884033\n",
            "step:5, loss:1.2980661392211914\n",
            "step:6, loss:7.311164855957031\n",
            "step:7, loss:1.9789469242095947\n",
            "step:8, loss:0.6280480623245239\n",
            "step:9, loss:1.1889276504516602\n",
            "step:10, loss:2.981377124786377\n",
            "step:11, loss:1.3402599096298218\n",
            "step:12, loss:2.4390869140625\n",
            "step:13, loss:80.93600463867188\n",
            "step:14, loss:2.098787784576416\n",
            "step:15, loss:2.0106987953186035\n",
            "step:16, loss:2.360985040664673\n",
            "step:17, loss:86.83116912841797\n",
            "step:18, loss:3.1445581912994385\n",
            "step:19, loss:3.1207308769226074\n",
            "step:20, loss:2.265042304992676\n",
            "step:21, loss:1.953199863433838\n",
            "step:22, loss:1.8407083749771118\n",
            "step:23, loss:2.719832181930542\n",
            "step:24, loss:2.7951197624206543\n",
            "step:25, loss:7.545845031738281\n",
            "step:26, loss:3.7165021896362305\n",
            "step:27, loss:1.7871363162994385\n",
            "step:28, loss:4.628704071044922\n",
            "step:29, loss:4.4041924476623535\n",
            "step:30, loss:11.86838436126709\n",
            "step:31, loss:1.3633465766906738\n",
            "step:32, loss:6.192149639129639\n",
            "step:33, loss:3.6771366596221924\n",
            "step:34, loss:13.44443130493164\n",
            "step:35, loss:3.426694393157959\n",
            "step:36, loss:1.0385687351226807\n",
            "step:37, loss:1.6589770317077637\n",
            "step:38, loss:22.134815216064453\n",
            "step:39, loss:1.7056965827941895\n",
            "step:40, loss:3.082533359527588\n",
            "step:41, loss:6.471212387084961\n",
            "step:42, loss:1.7820141315460205\n",
            "step:43, loss:2.361053466796875\n",
            "step:44, loss:2.887503147125244\n",
            "step:45, loss:2.472188711166382\n",
            "step:46, loss:1.7052353620529175\n",
            "step:47, loss:2.333950996398926\n",
            "step:48, loss:0.7691091895103455\n",
            "step:49, loss:1.4127531051635742\n",
            "step:50, loss:8.347976684570312\n",
            "step:51, loss:8.466204643249512\n",
            "step:52, loss:32.01938247680664\n",
            "step:53, loss:7.045285224914551\n",
            "step:54, loss:3.5598039627075195\n",
            "step:55, loss:1.3425800800323486\n",
            "step:56, loss:4.028596878051758\n",
            "step:57, loss:3.144500970840454\n",
            "step:58, loss:4.522701740264893\n",
            "step:59, loss:24.05698013305664\n",
            "step:60, loss:2.125016689300537\n",
            "step:61, loss:2.0701539516448975\n",
            "step:62, loss:1.6275054216384888\n",
            "step:63, loss:4.355825424194336\n",
            "step:64, loss:1.8335812091827393\n",
            "step:65, loss:1.3136858940124512\n",
            "step:66, loss:0.8560962080955505\n",
            "step:67, loss:7.525547027587891\n",
            "step:68, loss:9.721274375915527\n",
            "step:69, loss:1.6475639343261719\n",
            "step:70, loss:2.224257707595825\n",
            "step:71, loss:3.261969804763794\n",
            "step:72, loss:1.858994960784912\n",
            "step:73, loss:1.6299383640289307\n",
            "step:74, loss:2.367061138153076\n",
            "step:75, loss:3.8546431064605713\n",
            "step:76, loss:1.378877878189087\n",
            "38\n",
            "-----\n",
            "step:0, loss:3.235111713409424\n",
            "step:1, loss:6.05897331237793\n",
            "step:2, loss:3.145751476287842\n",
            "step:3, loss:9.572025299072266\n",
            "step:4, loss:2.71453595161438\n",
            "step:5, loss:2.2185845375061035\n",
            "step:6, loss:1.344890832901001\n",
            "step:7, loss:1.2084521055221558\n",
            "step:8, loss:3.4486260414123535\n",
            "step:9, loss:5.626042366027832\n",
            "step:10, loss:6.812733173370361\n",
            "step:11, loss:5.964691162109375\n",
            "step:12, loss:2.172484874725342\n",
            "step:13, loss:25.297082901000977\n",
            "step:14, loss:0.9887222647666931\n",
            "step:15, loss:1.5454208850860596\n",
            "step:16, loss:34.757911682128906\n",
            "step:17, loss:5.34288215637207\n",
            "step:18, loss:2.7348012924194336\n",
            "step:19, loss:0.9146132469177246\n",
            "step:20, loss:1.5554423332214355\n",
            "step:21, loss:2.5398805141448975\n",
            "step:22, loss:1.873840093612671\n",
            "step:23, loss:1.015041470527649\n",
            "step:24, loss:3.4633560180664062\n",
            "step:25, loss:2.8905792236328125\n",
            "step:26, loss:1.2554707527160645\n",
            "step:27, loss:3.59456205368042\n",
            "step:28, loss:92.59495544433594\n",
            "step:29, loss:2.2223474979400635\n",
            "step:30, loss:3.2767298221588135\n",
            "step:31, loss:3.2513175010681152\n",
            "step:32, loss:7.384658336639404\n",
            "step:33, loss:4.89189338684082\n",
            "step:34, loss:4.9387054443359375\n",
            "step:35, loss:39.01405715942383\n",
            "step:36, loss:1.2132751941680908\n",
            "step:37, loss:5.213825702667236\n",
            "step:38, loss:1.8143556118011475\n",
            "step:39, loss:2.055891513824463\n",
            "step:40, loss:5.488920211791992\n",
            "step:41, loss:3.1555559635162354\n",
            "step:42, loss:3.3137097358703613\n",
            "step:43, loss:2.121591806411743\n",
            "step:44, loss:1.325201153755188\n",
            "step:45, loss:2.8735694885253906\n",
            "step:46, loss:2.223421335220337\n",
            "step:47, loss:2.161555767059326\n",
            "step:48, loss:1.355776309967041\n",
            "step:49, loss:3.632920265197754\n",
            "step:50, loss:1.2464951276779175\n",
            "step:51, loss:1.8632521629333496\n",
            "step:52, loss:1.8401432037353516\n",
            "step:53, loss:2.786782741546631\n",
            "step:54, loss:1.7565561532974243\n",
            "step:55, loss:74.4035873413086\n",
            "step:56, loss:5.537564277648926\n",
            "step:57, loss:2.491901397705078\n",
            "step:58, loss:1.2010399103164673\n",
            "step:59, loss:0.8839449882507324\n",
            "step:60, loss:3.881395101547241\n",
            "step:61, loss:11.493063926696777\n",
            "step:62, loss:10.57621955871582\n",
            "step:63, loss:1.632359266281128\n",
            "step:64, loss:3.5196704864501953\n",
            "step:65, loss:1.2426013946533203\n",
            "step:66, loss:1.987640619277954\n",
            "step:67, loss:9.300118446350098\n",
            "step:68, loss:3.316956043243408\n",
            "step:69, loss:5.429169178009033\n",
            "step:70, loss:1.6120542287826538\n",
            "step:71, loss:0.8125341534614563\n",
            "step:72, loss:2.8207430839538574\n",
            "step:73, loss:1.7923388481140137\n",
            "step:74, loss:1.6412967443466187\n",
            "step:75, loss:7.884665489196777\n",
            "step:76, loss:1.046468734741211\n",
            "39\n",
            "-----\n",
            "step:0, loss:1.6442461013793945\n",
            "step:1, loss:3.426363945007324\n",
            "step:2, loss:1.7737047672271729\n",
            "step:3, loss:8.604931831359863\n",
            "step:4, loss:1.0628564357757568\n",
            "step:5, loss:1.8092660903930664\n",
            "step:6, loss:3.275407552719116\n",
            "step:7, loss:2.248353958129883\n",
            "step:8, loss:0.5664300918579102\n",
            "step:9, loss:2.173499345779419\n",
            "step:10, loss:3.721430778503418\n",
            "step:11, loss:2.4590139389038086\n",
            "step:12, loss:0.7761818170547485\n",
            "step:13, loss:1.1103434562683105\n",
            "step:14, loss:1.400659203529358\n",
            "step:15, loss:1.4008339643478394\n",
            "step:16, loss:6.70017671585083\n",
            "step:17, loss:1.2159596681594849\n",
            "step:18, loss:86.01821899414062\n",
            "step:19, loss:2.693755626678467\n",
            "step:20, loss:8.385242462158203\n",
            "step:21, loss:7.481767177581787\n",
            "step:22, loss:24.212465286254883\n",
            "step:23, loss:2.0966265201568604\n",
            "step:24, loss:2.538072347640991\n",
            "step:25, loss:3.0332610607147217\n",
            "step:26, loss:1.4151890277862549\n",
            "step:27, loss:2.8013782501220703\n",
            "step:28, loss:3.8046703338623047\n",
            "step:29, loss:2.2852718830108643\n",
            "step:30, loss:2.6855356693267822\n",
            "step:31, loss:1.4124255180358887\n",
            "step:32, loss:12.220796585083008\n",
            "step:33, loss:1.3355073928833008\n",
            "step:34, loss:22.690753936767578\n",
            "step:35, loss:3.0216732025146484\n",
            "step:36, loss:1.8005435466766357\n",
            "step:37, loss:22.16200065612793\n",
            "step:38, loss:5.663497447967529\n",
            "step:39, loss:1.8548640012741089\n",
            "step:40, loss:5.650206565856934\n",
            "step:41, loss:1.3935292959213257\n",
            "step:42, loss:2.395902633666992\n",
            "step:43, loss:1.780073642730713\n",
            "step:44, loss:4.017629623413086\n",
            "step:45, loss:4.451635837554932\n",
            "step:46, loss:2.1738672256469727\n",
            "step:47, loss:1.1327472925186157\n",
            "step:48, loss:6.483602523803711\n",
            "step:49, loss:7.578741073608398\n",
            "step:50, loss:12.955729484558105\n",
            "step:51, loss:1.915340781211853\n",
            "step:52, loss:5.9995436668396\n",
            "step:53, loss:2.3033764362335205\n",
            "step:54, loss:3.8733348846435547\n",
            "step:55, loss:7.339221477508545\n",
            "step:56, loss:2.1966874599456787\n",
            "step:57, loss:2.016589403152466\n",
            "step:58, loss:3.389234781265259\n",
            "step:59, loss:2.5655314922332764\n",
            "step:60, loss:20.642013549804688\n",
            "step:61, loss:1.4235897064208984\n",
            "step:62, loss:1.0005428791046143\n",
            "step:63, loss:2.489895820617676\n",
            "step:64, loss:3.6506145000457764\n",
            "step:65, loss:3.525202751159668\n",
            "step:66, loss:4.000053882598877\n",
            "step:67, loss:79.45670318603516\n",
            "step:68, loss:4.050956726074219\n",
            "step:69, loss:3.7400078773498535\n",
            "step:70, loss:1.702895998954773\n",
            "step:71, loss:3.309494733810425\n",
            "step:72, loss:1.1654932498931885\n",
            "step:73, loss:1.201062560081482\n",
            "step:74, loss:3.860691547393799\n",
            "step:75, loss:1.3200860023498535\n",
            "step:76, loss:1.392736554145813\n",
            "40\n",
            "-----\n",
            "step:0, loss:9.238765716552734\n",
            "step:1, loss:0.838738739490509\n",
            "step:2, loss:2.140704870223999\n",
            "step:3, loss:6.7413177490234375\n",
            "step:4, loss:77.52947998046875\n",
            "step:5, loss:1.5286855697631836\n",
            "step:6, loss:3.582106590270996\n",
            "step:7, loss:19.819162368774414\n",
            "step:8, loss:1.0777404308319092\n",
            "step:9, loss:1.397768497467041\n",
            "step:10, loss:26.51059913635254\n",
            "step:11, loss:3.1669154167175293\n",
            "step:12, loss:2.185870409011841\n",
            "step:13, loss:2.771519660949707\n",
            "step:14, loss:1.46794593334198\n",
            "step:15, loss:0.8354921340942383\n",
            "step:16, loss:2.793095827102661\n",
            "step:17, loss:1.8948431015014648\n",
            "step:18, loss:2.228309154510498\n",
            "step:19, loss:2.518043279647827\n",
            "step:20, loss:1.7142034769058228\n",
            "step:21, loss:7.041115760803223\n",
            "step:22, loss:1.5381758213043213\n",
            "step:23, loss:3.111166477203369\n",
            "step:24, loss:1.6079652309417725\n",
            "step:25, loss:1.3333467245101929\n",
            "step:26, loss:3.6629347801208496\n",
            "step:27, loss:3.8190860748291016\n",
            "step:28, loss:12.634666442871094\n",
            "step:29, loss:3.96309757232666\n",
            "step:30, loss:4.3817291259765625\n",
            "step:31, loss:2.869312286376953\n",
            "step:32, loss:5.9505462646484375\n",
            "step:33, loss:2.144217014312744\n",
            "step:34, loss:3.306765556335449\n",
            "step:35, loss:1.6029753684997559\n",
            "step:36, loss:1.5100646018981934\n",
            "step:37, loss:3.017841339111328\n",
            "step:38, loss:1.2821260690689087\n",
            "step:39, loss:1.9836246967315674\n",
            "step:40, loss:3.541389226913452\n",
            "step:41, loss:2.1484107971191406\n",
            "step:42, loss:2.156630039215088\n",
            "step:43, loss:1.73796808719635\n",
            "step:44, loss:2.2286088466644287\n",
            "step:45, loss:1.8395510911941528\n",
            "step:46, loss:1.4549578428268433\n",
            "step:47, loss:2.7280654907226562\n",
            "step:48, loss:1.5577232837677002\n",
            "step:49, loss:23.46502685546875\n",
            "step:50, loss:1.1074658632278442\n",
            "step:51, loss:2.1934609413146973\n",
            "step:52, loss:1.911285638809204\n",
            "step:53, loss:8.013952255249023\n",
            "step:54, loss:3.329252243041992\n",
            "step:55, loss:8.42630386352539\n",
            "step:56, loss:1.1132649183273315\n",
            "step:57, loss:2.4787240028381348\n",
            "step:58, loss:2.806140661239624\n",
            "step:59, loss:1.1036922931671143\n",
            "step:60, loss:18.336183547973633\n",
            "step:61, loss:1.4181349277496338\n",
            "step:62, loss:1.3062341213226318\n",
            "step:63, loss:9.338044166564941\n",
            "step:64, loss:4.130937099456787\n",
            "step:65, loss:2.030118465423584\n",
            "step:66, loss:2.589210033416748\n",
            "step:67, loss:1.707432508468628\n",
            "step:68, loss:2.969240427017212\n",
            "step:69, loss:4.272701263427734\n",
            "step:70, loss:3.954632043838501\n",
            "step:71, loss:1.677818775177002\n",
            "step:72, loss:19.05820083618164\n",
            "step:73, loss:68.46743774414062\n",
            "step:74, loss:1.9382622241973877\n",
            "step:75, loss:3.2193050384521484\n",
            "step:76, loss:12.351709365844727\n",
            "41\n",
            "-----\n",
            "step:0, loss:1.5570783615112305\n",
            "step:1, loss:1.7982351779937744\n",
            "step:2, loss:2.8978147506713867\n",
            "step:3, loss:2.7104105949401855\n",
            "step:4, loss:1.9817414283752441\n",
            "step:5, loss:1.8481218814849854\n",
            "step:6, loss:22.355321884155273\n",
            "step:7, loss:1.699314832687378\n",
            "step:8, loss:1.5175464153289795\n",
            "step:9, loss:1.594372272491455\n",
            "step:10, loss:4.02045202255249\n",
            "step:11, loss:1.3296828269958496\n",
            "step:12, loss:9.38397216796875\n",
            "step:13, loss:1.2667384147644043\n",
            "step:14, loss:9.863971710205078\n",
            "step:15, loss:4.049560546875\n",
            "step:16, loss:1.570911169052124\n",
            "step:17, loss:1.3413666486740112\n",
            "step:18, loss:1.1152496337890625\n",
            "step:19, loss:1.8026612997055054\n",
            "step:20, loss:71.02770233154297\n",
            "step:21, loss:3.548748016357422\n",
            "step:22, loss:9.745525360107422\n",
            "step:23, loss:3.2350010871887207\n",
            "step:24, loss:1.6209224462509155\n",
            "step:25, loss:2.3682358264923096\n",
            "step:26, loss:3.329169750213623\n",
            "step:27, loss:2.0719079971313477\n",
            "step:28, loss:2.734109878540039\n",
            "step:29, loss:1.7917803525924683\n",
            "step:30, loss:4.7870588302612305\n",
            "step:31, loss:18.433856964111328\n",
            "step:32, loss:2.7158565521240234\n",
            "step:33, loss:1.8069186210632324\n",
            "step:34, loss:3.7462098598480225\n",
            "step:35, loss:1.3601537942886353\n",
            "step:36, loss:3.4174728393554688\n",
            "step:37, loss:4.269933223724365\n",
            "step:38, loss:20.10604476928711\n",
            "step:39, loss:2.298501968383789\n",
            "step:40, loss:1.651286244392395\n",
            "step:41, loss:3.272599220275879\n",
            "step:42, loss:2.940345287322998\n",
            "step:43, loss:4.892122268676758\n",
            "step:44, loss:1.311508297920227\n",
            "step:45, loss:1.4201102256774902\n",
            "step:46, loss:0.9838265180587769\n",
            "step:47, loss:2.824389696121216\n",
            "step:48, loss:1.3519052267074585\n",
            "step:49, loss:82.16203308105469\n",
            "step:50, loss:11.188628196716309\n",
            "step:51, loss:2.5386319160461426\n",
            "step:52, loss:4.204307556152344\n",
            "step:53, loss:2.6850016117095947\n",
            "step:54, loss:2.421557903289795\n",
            "step:55, loss:3.982346773147583\n",
            "step:56, loss:1.0395044088363647\n",
            "step:57, loss:2.099228858947754\n",
            "step:58, loss:3.228541374206543\n",
            "step:59, loss:2.2457070350646973\n",
            "step:60, loss:3.9087538719177246\n",
            "step:61, loss:1.8815803527832031\n",
            "step:62, loss:1.4827632904052734\n",
            "step:63, loss:9.15385627746582\n",
            "step:64, loss:1.7916152477264404\n",
            "step:65, loss:5.804025173187256\n",
            "step:66, loss:4.2938995361328125\n",
            "step:67, loss:1.5817818641662598\n",
            "step:68, loss:2.321993350982666\n",
            "step:69, loss:8.460131645202637\n",
            "step:70, loss:1.1106936931610107\n",
            "step:71, loss:3.528704881668091\n",
            "step:72, loss:1.1571627855300903\n",
            "step:73, loss:2.672295093536377\n",
            "step:74, loss:35.21185302734375\n",
            "step:75, loss:2.039897918701172\n",
            "step:76, loss:3.7185463905334473\n",
            "42\n",
            "-----\n",
            "step:0, loss:3.0292739868164062\n",
            "step:1, loss:1.2916682958602905\n",
            "step:2, loss:1.938582181930542\n",
            "step:3, loss:1.8778761625289917\n",
            "step:4, loss:0.8636950254440308\n",
            "step:5, loss:1.71668541431427\n",
            "step:6, loss:1.4901869297027588\n",
            "step:7, loss:1.0974297523498535\n",
            "step:8, loss:0.836459219455719\n",
            "step:9, loss:6.033305644989014\n",
            "step:10, loss:38.284873962402344\n",
            "step:11, loss:2.6308395862579346\n",
            "step:12, loss:3.609821319580078\n",
            "step:13, loss:1.7672557830810547\n",
            "step:14, loss:3.8443336486816406\n",
            "step:15, loss:5.278268337249756\n",
            "step:16, loss:1.7169013023376465\n",
            "step:17, loss:1.9859141111373901\n",
            "step:18, loss:1.0096228122711182\n",
            "step:19, loss:2.291281223297119\n",
            "step:20, loss:1.4534602165222168\n",
            "step:21, loss:5.364859580993652\n",
            "step:22, loss:1.8155102729797363\n",
            "step:23, loss:4.665398597717285\n",
            "step:24, loss:2.308823585510254\n",
            "step:25, loss:3.60369873046875\n",
            "step:26, loss:1.1202824115753174\n",
            "step:27, loss:1.0754625797271729\n",
            "step:28, loss:1.2642673254013062\n",
            "step:29, loss:5.801233768463135\n",
            "step:30, loss:2.102903127670288\n",
            "step:31, loss:6.223812580108643\n",
            "step:32, loss:2.20426607131958\n",
            "step:33, loss:0.8794757127761841\n",
            "step:34, loss:1.8026096820831299\n",
            "step:35, loss:8.231995582580566\n",
            "step:36, loss:4.876906394958496\n",
            "step:37, loss:18.38246726989746\n",
            "step:38, loss:2.4696602821350098\n",
            "step:39, loss:5.56911039352417\n",
            "step:40, loss:4.936768531799316\n",
            "step:41, loss:1.4357415437698364\n",
            "step:42, loss:3.0571465492248535\n",
            "step:43, loss:2.1436305046081543\n",
            "step:44, loss:4.509596824645996\n",
            "step:45, loss:6.326930999755859\n",
            "step:46, loss:1.298367977142334\n",
            "step:47, loss:10.797738075256348\n",
            "step:48, loss:2.255678415298462\n",
            "step:49, loss:1.4231067895889282\n",
            "step:50, loss:1.3542815446853638\n",
            "step:51, loss:1.2602566480636597\n",
            "step:52, loss:7.543032646179199\n",
            "step:53, loss:74.45560455322266\n",
            "step:54, loss:13.017014503479004\n",
            "step:55, loss:3.688995599746704\n",
            "step:56, loss:2.5961267948150635\n",
            "step:57, loss:1.3918849229812622\n",
            "step:58, loss:12.234222412109375\n",
            "step:59, loss:1.6927458047866821\n",
            "step:60, loss:76.49647521972656\n",
            "step:61, loss:2.4827680587768555\n",
            "step:62, loss:2.447249412536621\n",
            "step:63, loss:3.955610752105713\n",
            "step:64, loss:1.0490193367004395\n",
            "step:65, loss:21.987627029418945\n",
            "step:66, loss:4.796179294586182\n",
            "step:67, loss:1.3215254545211792\n",
            "step:68, loss:1.123032808303833\n",
            "step:69, loss:0.7283428311347961\n",
            "step:70, loss:3.739034414291382\n",
            "step:71, loss:3.58457088470459\n",
            "step:72, loss:1.4203110933303833\n",
            "step:73, loss:8.803018569946289\n",
            "step:74, loss:2.27773118019104\n",
            "step:75, loss:2.540905714035034\n",
            "step:76, loss:2.7450294494628906\n",
            "43\n",
            "-----\n",
            "step:0, loss:6.526973247528076\n",
            "step:1, loss:2.441753625869751\n",
            "step:2, loss:3.500776529312134\n",
            "step:3, loss:1.115917444229126\n",
            "step:4, loss:4.6485772132873535\n",
            "step:5, loss:2.0118987560272217\n",
            "step:6, loss:1.086684226989746\n",
            "step:7, loss:1.7932803630828857\n",
            "step:8, loss:2.661811351776123\n",
            "step:9, loss:8.453699111938477\n",
            "step:10, loss:1.6360602378845215\n",
            "step:11, loss:2.2666356563568115\n",
            "step:12, loss:92.1725082397461\n",
            "step:13, loss:1.8502098321914673\n",
            "step:14, loss:3.8901865482330322\n",
            "step:15, loss:27.671161651611328\n",
            "step:16, loss:2.296154737472534\n",
            "step:17, loss:1.5181111097335815\n",
            "step:18, loss:75.3768081665039\n",
            "step:19, loss:1.5195271968841553\n",
            "step:20, loss:4.991716384887695\n",
            "step:21, loss:6.9130449295043945\n",
            "step:22, loss:6.667033672332764\n",
            "step:23, loss:2.2090487480163574\n",
            "step:24, loss:2.420313835144043\n",
            "step:25, loss:1.5014886856079102\n",
            "step:26, loss:1.3356761932373047\n",
            "step:27, loss:1.6836220026016235\n",
            "step:28, loss:2.0032734870910645\n",
            "step:29, loss:2.28890323638916\n",
            "step:30, loss:1.9026448726654053\n",
            "step:31, loss:0.9778804779052734\n",
            "step:32, loss:2.8850388526916504\n",
            "step:33, loss:2.714702606201172\n",
            "step:34, loss:2.519968032836914\n",
            "step:35, loss:21.71133804321289\n",
            "step:36, loss:3.03256893157959\n",
            "step:37, loss:3.583824396133423\n",
            "step:38, loss:4.509768486022949\n",
            "step:39, loss:4.461309432983398\n",
            "step:40, loss:1.3451188802719116\n",
            "step:41, loss:4.531703472137451\n",
            "step:42, loss:8.468158721923828\n",
            "step:43, loss:1.5719821453094482\n",
            "step:44, loss:1.388836145401001\n",
            "step:45, loss:2.403432607650757\n",
            "step:46, loss:2.1195974349975586\n",
            "step:47, loss:4.108227252960205\n",
            "step:48, loss:1.1003148555755615\n",
            "step:49, loss:1.1108225584030151\n",
            "step:50, loss:1.1554584503173828\n",
            "step:51, loss:1.3289754390716553\n",
            "step:52, loss:11.507710456848145\n",
            "step:53, loss:1.689622402191162\n",
            "step:54, loss:7.159235000610352\n",
            "step:55, loss:2.0892763137817383\n",
            "step:56, loss:0.6513699293136597\n",
            "step:57, loss:1.2299847602844238\n",
            "step:58, loss:20.399660110473633\n",
            "step:59, loss:3.265089988708496\n",
            "step:60, loss:2.2708678245544434\n",
            "step:61, loss:2.328876495361328\n",
            "step:62, loss:2.796598196029663\n",
            "step:63, loss:2.4912753105163574\n",
            "step:64, loss:0.9479259252548218\n",
            "step:65, loss:2.571834087371826\n",
            "step:66, loss:3.3407511711120605\n",
            "step:67, loss:1.7800724506378174\n",
            "step:68, loss:1.4810479879379272\n",
            "step:69, loss:1.2243590354919434\n",
            "step:70, loss:1.5148231983184814\n",
            "step:71, loss:1.5250272750854492\n",
            "step:72, loss:3.3074018955230713\n",
            "step:73, loss:5.062988758087158\n",
            "step:74, loss:1.5818325281143188\n",
            "step:75, loss:5.801573276519775\n",
            "step:76, loss:3.068878173828125\n",
            "44\n",
            "-----\n",
            "step:0, loss:2.4039478302001953\n",
            "step:1, loss:4.665125846862793\n",
            "step:2, loss:1.8985036611557007\n",
            "step:3, loss:2.426292657852173\n",
            "step:4, loss:3.0197675228118896\n",
            "step:5, loss:2.9588370323181152\n",
            "step:6, loss:2.3081393241882324\n",
            "step:7, loss:0.9562709927558899\n",
            "step:8, loss:2.1211776733398438\n",
            "step:9, loss:2.1286230087280273\n",
            "step:10, loss:0.8856555819511414\n",
            "step:11, loss:6.438389778137207\n",
            "step:12, loss:1.1006436347961426\n",
            "step:13, loss:7.479116439819336\n",
            "step:14, loss:2.546057939529419\n",
            "step:15, loss:3.9035933017730713\n",
            "step:16, loss:0.6169278621673584\n",
            "step:17, loss:4.180685043334961\n",
            "step:18, loss:1.050567626953125\n",
            "step:19, loss:2.2254185676574707\n",
            "step:20, loss:2.704207420349121\n",
            "step:21, loss:3.218524932861328\n",
            "step:22, loss:2.7487778663635254\n",
            "step:23, loss:1.5021167993545532\n",
            "step:24, loss:1.557459831237793\n",
            "step:25, loss:1.0627076625823975\n",
            "step:26, loss:1.8594341278076172\n",
            "step:27, loss:1.7506197690963745\n",
            "step:28, loss:2.857976198196411\n",
            "step:29, loss:2.0081305503845215\n",
            "step:30, loss:1.0814990997314453\n",
            "step:31, loss:22.68938446044922\n",
            "step:32, loss:2.3703269958496094\n",
            "step:33, loss:3.8462605476379395\n",
            "step:34, loss:8.06031608581543\n",
            "step:35, loss:70.84183502197266\n",
            "step:36, loss:18.916000366210938\n",
            "step:37, loss:2.7646684646606445\n",
            "step:38, loss:2.5295536518096924\n",
            "step:39, loss:1.6900522708892822\n",
            "step:40, loss:7.943446636199951\n",
            "step:41, loss:1.213714599609375\n",
            "step:42, loss:2.0229711532592773\n",
            "step:43, loss:1.1315853595733643\n",
            "step:44, loss:10.368169784545898\n",
            "step:45, loss:1.777383804321289\n",
            "step:46, loss:4.437648773193359\n",
            "step:47, loss:3.8221147060394287\n",
            "step:48, loss:3.8115782737731934\n",
            "step:49, loss:70.51351928710938\n",
            "step:50, loss:2.320435047149658\n",
            "step:51, loss:2.5246071815490723\n",
            "step:52, loss:0.9287669658660889\n",
            "step:53, loss:5.738585472106934\n",
            "step:54, loss:19.703550338745117\n",
            "step:55, loss:1.239361047744751\n",
            "step:56, loss:3.281672716140747\n",
            "step:57, loss:1.9743409156799316\n",
            "step:58, loss:2.498955249786377\n",
            "step:59, loss:3.534158706665039\n",
            "step:60, loss:1.0074812173843384\n",
            "step:61, loss:1.6406033039093018\n",
            "step:62, loss:37.59627151489258\n",
            "step:63, loss:1.7529175281524658\n",
            "step:64, loss:2.0902700424194336\n",
            "step:65, loss:1.4847928285598755\n",
            "step:66, loss:5.8121137619018555\n",
            "step:67, loss:1.659705400466919\n",
            "step:68, loss:1.6292579174041748\n",
            "step:69, loss:1.703169584274292\n",
            "step:70, loss:1.9446946382522583\n",
            "step:71, loss:1.2144567966461182\n",
            "step:72, loss:2.0527706146240234\n",
            "step:73, loss:6.807003498077393\n",
            "step:74, loss:5.882852554321289\n",
            "step:75, loss:2.008002758026123\n",
            "step:76, loss:1.6323596239089966\n",
            "45\n",
            "-----\n",
            "step:0, loss:4.288760662078857\n",
            "step:1, loss:2.209557056427002\n",
            "step:2, loss:2.816202163696289\n",
            "step:3, loss:3.44882869720459\n",
            "step:4, loss:4.14844274520874\n",
            "step:5, loss:2.393397569656372\n",
            "step:6, loss:0.8221888542175293\n",
            "step:7, loss:8.778103828430176\n",
            "step:8, loss:4.987110137939453\n",
            "step:9, loss:2.9481968879699707\n",
            "step:10, loss:3.6991186141967773\n",
            "step:11, loss:2.1730740070343018\n",
            "step:12, loss:19.833942413330078\n",
            "step:13, loss:1.511062502861023\n",
            "step:14, loss:6.463052749633789\n",
            "step:15, loss:1.7003666162490845\n",
            "step:16, loss:2.9392523765563965\n",
            "step:17, loss:3.3521125316619873\n",
            "step:18, loss:2.3897433280944824\n",
            "step:19, loss:66.84799194335938\n",
            "step:20, loss:5.831270217895508\n",
            "step:21, loss:15.2798490524292\n",
            "step:22, loss:2.3933463096618652\n",
            "step:23, loss:2.1745529174804688\n",
            "step:24, loss:2.068660020828247\n",
            "step:25, loss:1.2706148624420166\n",
            "step:26, loss:1.5817571878433228\n",
            "step:27, loss:1.387221097946167\n",
            "step:28, loss:70.49998474121094\n",
            "step:29, loss:3.46498441696167\n",
            "step:30, loss:1.484440565109253\n",
            "step:31, loss:6.507040977478027\n",
            "step:32, loss:2.031846046447754\n",
            "step:33, loss:9.345410346984863\n",
            "step:34, loss:1.8651689291000366\n",
            "step:35, loss:1.474043846130371\n",
            "step:36, loss:3.8591527938842773\n",
            "step:37, loss:1.61639404296875\n",
            "step:38, loss:1.5359911918640137\n",
            "step:39, loss:1.5411502122879028\n",
            "step:40, loss:1.333631992340088\n",
            "step:41, loss:2.1641077995300293\n",
            "step:42, loss:3.1634299755096436\n",
            "step:43, loss:19.92319679260254\n",
            "step:44, loss:1.4858489036560059\n",
            "step:45, loss:2.5486855506896973\n",
            "step:46, loss:3.5054025650024414\n",
            "step:47, loss:7.630163192749023\n",
            "step:48, loss:4.069723606109619\n",
            "step:49, loss:3.339231491088867\n",
            "step:50, loss:1.821587085723877\n",
            "step:51, loss:3.2069547176361084\n",
            "step:52, loss:9.983420372009277\n",
            "step:53, loss:5.60257625579834\n",
            "step:54, loss:7.630241394042969\n",
            "step:55, loss:4.7764129638671875\n",
            "step:56, loss:2.8098526000976562\n",
            "step:57, loss:0.9894636869430542\n",
            "step:58, loss:1.1090269088745117\n",
            "step:59, loss:1.3144980669021606\n",
            "step:60, loss:1.9578723907470703\n",
            "step:61, loss:3.2233495712280273\n",
            "step:62, loss:0.9986392259597778\n",
            "step:63, loss:1.4116634130477905\n",
            "step:64, loss:1.31959867477417\n",
            "step:65, loss:1.380711555480957\n",
            "step:66, loss:1.8909261226654053\n",
            "step:67, loss:2.1806633472442627\n",
            "step:68, loss:1.3944530487060547\n",
            "step:69, loss:2.130922317504883\n",
            "step:70, loss:0.8284960985183716\n",
            "step:71, loss:17.839008331298828\n",
            "step:72, loss:7.841350078582764\n",
            "step:73, loss:2.1850414276123047\n",
            "step:74, loss:1.625826120376587\n",
            "step:75, loss:3.215111494064331\n",
            "step:76, loss:6.178865432739258\n",
            "46\n",
            "-----\n",
            "step:0, loss:1.0703821182250977\n",
            "step:1, loss:1.6262617111206055\n",
            "step:2, loss:1.4957702159881592\n",
            "step:3, loss:5.758203029632568\n",
            "step:4, loss:2.1124560832977295\n",
            "step:5, loss:0.8661574125289917\n",
            "step:6, loss:19.233257293701172\n",
            "step:7, loss:2.731217861175537\n",
            "step:8, loss:14.181412696838379\n",
            "step:9, loss:2.1342594623565674\n",
            "step:10, loss:10.340409278869629\n",
            "step:11, loss:1.097887396812439\n",
            "step:12, loss:70.01318359375\n",
            "step:13, loss:1.5874907970428467\n",
            "step:14, loss:7.936095237731934\n",
            "step:15, loss:10.172344207763672\n",
            "step:16, loss:1.6510283946990967\n",
            "step:17, loss:0.9289707541465759\n",
            "step:18, loss:2.820779800415039\n",
            "step:19, loss:6.151998519897461\n",
            "step:20, loss:1.7790310382843018\n",
            "step:21, loss:91.90585327148438\n",
            "step:22, loss:1.567155122756958\n",
            "step:23, loss:1.962679147720337\n",
            "step:24, loss:1.6332817077636719\n",
            "step:25, loss:5.180104732513428\n",
            "step:26, loss:2.1083030700683594\n",
            "step:27, loss:1.8929591178894043\n",
            "step:28, loss:3.387282609939575\n",
            "step:29, loss:1.2216256856918335\n",
            "step:30, loss:1.7128770351409912\n",
            "step:31, loss:1.3480865955352783\n",
            "step:32, loss:3.362579584121704\n",
            "step:33, loss:0.9749537706375122\n",
            "step:34, loss:3.86264705657959\n",
            "step:35, loss:2.0329737663269043\n",
            "step:36, loss:5.1491498947143555\n",
            "step:37, loss:7.857377052307129\n",
            "step:38, loss:1.3215869665145874\n",
            "step:39, loss:5.348968505859375\n",
            "step:40, loss:2.1189112663269043\n",
            "step:41, loss:8.9772310256958\n",
            "step:42, loss:1.209183692932129\n",
            "step:43, loss:1.3075072765350342\n",
            "step:44, loss:1.1494587659835815\n",
            "step:45, loss:1.427973747253418\n",
            "step:46, loss:2.159379243850708\n",
            "step:47, loss:1.627300500869751\n",
            "step:48, loss:5.238645553588867\n",
            "step:49, loss:1.4086666107177734\n",
            "step:50, loss:1.6727218627929688\n",
            "step:51, loss:0.8377458453178406\n",
            "step:52, loss:0.9558274745941162\n",
            "step:53, loss:15.961170196533203\n",
            "step:54, loss:2.0167126655578613\n",
            "step:55, loss:2.6289851665496826\n",
            "step:56, loss:1.141047716140747\n",
            "step:57, loss:1.7899296283721924\n",
            "step:58, loss:1.4699270725250244\n",
            "step:59, loss:2.2241642475128174\n",
            "step:60, loss:1.474497675895691\n",
            "step:61, loss:1.0838184356689453\n",
            "step:62, loss:3.8062057495117188\n",
            "step:63, loss:0.9775402545928955\n",
            "step:64, loss:1.8793835639953613\n",
            "step:65, loss:1.611891746520996\n",
            "step:66, loss:1.5323131084442139\n",
            "step:67, loss:1.9258544445037842\n",
            "step:68, loss:20.836856842041016\n",
            "step:69, loss:0.8657761812210083\n",
            "step:70, loss:3.5606653690338135\n",
            "step:71, loss:3.469344139099121\n",
            "step:72, loss:1.6479206085205078\n",
            "step:73, loss:1.8825275897979736\n",
            "step:74, loss:8.302653312683105\n",
            "step:75, loss:2.5651097297668457\n",
            "step:76, loss:2.2026729583740234\n",
            "47\n",
            "-----\n",
            "step:0, loss:1.3888118267059326\n",
            "step:1, loss:2.492581605911255\n",
            "step:2, loss:2.2297022342681885\n",
            "step:3, loss:2.0667667388916016\n",
            "step:4, loss:1.3809263706207275\n",
            "step:5, loss:2.711737632751465\n",
            "step:6, loss:3.1929705142974854\n",
            "step:7, loss:5.3957977294921875\n",
            "step:8, loss:5.62339973449707\n",
            "step:9, loss:0.9271998405456543\n",
            "step:10, loss:2.5901641845703125\n",
            "step:11, loss:5.910140514373779\n",
            "step:12, loss:1.7409353256225586\n",
            "step:13, loss:1.6635695695877075\n",
            "step:14, loss:2.0291881561279297\n",
            "step:15, loss:6.042701721191406\n",
            "step:16, loss:4.6422529220581055\n",
            "step:17, loss:17.282123565673828\n",
            "step:18, loss:12.337873458862305\n",
            "step:19, loss:3.103644371032715\n",
            "step:20, loss:8.026762008666992\n",
            "step:21, loss:2.11506724357605\n",
            "step:22, loss:0.9540600776672363\n",
            "step:23, loss:0.9152998924255371\n",
            "step:24, loss:0.8256428837776184\n",
            "step:25, loss:1.6717324256896973\n",
            "step:26, loss:2.8802576065063477\n",
            "step:27, loss:1.6862483024597168\n",
            "step:28, loss:2.612349033355713\n",
            "step:29, loss:1.0964422225952148\n",
            "step:30, loss:1.504093885421753\n",
            "step:31, loss:1.6205193996429443\n",
            "step:32, loss:8.179535865783691\n",
            "step:33, loss:1.2681617736816406\n",
            "step:34, loss:1.5422354936599731\n",
            "step:35, loss:3.009611129760742\n",
            "step:36, loss:66.92068481445312\n",
            "step:37, loss:1.134859561920166\n",
            "step:38, loss:1.292839527130127\n",
            "step:39, loss:4.539064407348633\n",
            "step:40, loss:2.7149667739868164\n",
            "step:41, loss:18.295372009277344\n",
            "step:42, loss:0.8666969537734985\n",
            "step:43, loss:2.3271522521972656\n",
            "step:44, loss:4.3378586769104\n",
            "step:45, loss:9.014738082885742\n",
            "step:46, loss:4.760622501373291\n",
            "step:47, loss:21.064647674560547\n",
            "step:48, loss:2.443451404571533\n",
            "step:49, loss:5.279366493225098\n",
            "step:50, loss:2.7335622310638428\n",
            "step:51, loss:17.111801147460938\n",
            "step:52, loss:1.809407114982605\n",
            "step:53, loss:1.8919814825057983\n",
            "step:54, loss:3.898526191711426\n",
            "step:55, loss:8.377090454101562\n",
            "step:56, loss:6.984578609466553\n",
            "step:57, loss:0.9676413536071777\n",
            "step:58, loss:1.4291832447052002\n",
            "step:59, loss:1.5738089084625244\n",
            "step:60, loss:1.1154911518096924\n",
            "step:61, loss:72.01229858398438\n",
            "step:62, loss:1.4032924175262451\n",
            "step:63, loss:1.699178695678711\n",
            "step:64, loss:1.8356748819351196\n",
            "step:65, loss:2.2860159873962402\n",
            "step:66, loss:4.2757720947265625\n",
            "step:67, loss:1.6237796545028687\n",
            "step:68, loss:1.4339690208435059\n",
            "step:69, loss:0.6511176824569702\n",
            "step:70, loss:1.821223497390747\n",
            "step:71, loss:2.28244686126709\n",
            "step:72, loss:3.8812551498413086\n",
            "step:73, loss:0.8770776987075806\n",
            "step:74, loss:1.8269891738891602\n",
            "step:75, loss:1.9605486392974854\n",
            "step:76, loss:3.0021328926086426\n",
            "48\n",
            "-----\n",
            "step:0, loss:4.69625997543335\n",
            "step:1, loss:1.376450538635254\n",
            "step:2, loss:1.9352281093597412\n",
            "step:3, loss:1.3047552108764648\n",
            "step:4, loss:1.7899494171142578\n",
            "step:5, loss:1.3831747770309448\n",
            "step:6, loss:2.2180519104003906\n",
            "step:7, loss:17.01959991455078\n",
            "step:8, loss:3.241528034210205\n",
            "step:9, loss:5.244632244110107\n",
            "step:10, loss:11.598969459533691\n",
            "step:11, loss:3.1812686920166016\n",
            "step:12, loss:0.7468516826629639\n",
            "step:13, loss:18.571950912475586\n",
            "step:14, loss:1.2609169483184814\n",
            "step:15, loss:1.6331496238708496\n",
            "step:16, loss:9.530536651611328\n",
            "step:17, loss:2.737746238708496\n",
            "step:18, loss:1.3657753467559814\n",
            "step:19, loss:4.261309623718262\n",
            "step:20, loss:1.4980918169021606\n",
            "step:21, loss:1.4889194965362549\n",
            "step:22, loss:1.5108299255371094\n",
            "step:23, loss:2.479581594467163\n",
            "step:24, loss:1.5101311206817627\n",
            "step:25, loss:17.13163185119629\n",
            "step:26, loss:2.513012409210205\n",
            "step:27, loss:0.8263227939605713\n",
            "step:28, loss:1.0172507762908936\n",
            "step:29, loss:1.3175344467163086\n",
            "step:30, loss:71.48479461669922\n",
            "step:31, loss:1.456700086593628\n",
            "step:32, loss:10.323969841003418\n",
            "step:33, loss:2.3011839389801025\n",
            "step:34, loss:3.163463830947876\n",
            "step:35, loss:4.054823875427246\n",
            "step:36, loss:2.641829490661621\n",
            "step:37, loss:1.5178141593933105\n",
            "step:38, loss:1.9049327373504639\n",
            "step:39, loss:1.5580129623413086\n",
            "step:40, loss:2.200972080230713\n",
            "step:41, loss:3.2496771812438965\n",
            "step:42, loss:0.9913884997367859\n",
            "step:43, loss:3.457705497741699\n",
            "step:44, loss:1.066998839378357\n",
            "step:45, loss:1.6542115211486816\n",
            "step:46, loss:2.1120505332946777\n",
            "step:47, loss:1.199636697769165\n",
            "step:48, loss:1.3069758415222168\n",
            "step:49, loss:1.2047101259231567\n",
            "step:50, loss:1.9493860006332397\n",
            "step:51, loss:1.6372406482696533\n",
            "step:52, loss:2.423015832901001\n",
            "step:53, loss:2.746311664581299\n",
            "step:54, loss:0.8532516956329346\n",
            "step:55, loss:1.3851099014282227\n",
            "step:56, loss:7.340171813964844\n",
            "step:57, loss:1.0527243614196777\n",
            "step:58, loss:4.236517906188965\n",
            "step:59, loss:3.0288994312286377\n",
            "step:60, loss:80.30158996582031\n",
            "step:61, loss:2.1601498126983643\n",
            "step:62, loss:3.4628491401672363\n",
            "step:63, loss:9.894773483276367\n",
            "step:64, loss:4.95842170715332\n",
            "step:65, loss:1.2156579494476318\n",
            "step:66, loss:9.986565589904785\n",
            "step:67, loss:1.0432467460632324\n",
            "step:68, loss:3.8323569297790527\n",
            "step:69, loss:0.9938513040542603\n",
            "step:70, loss:1.2228776216506958\n",
            "step:71, loss:2.8337082862854004\n",
            "step:72, loss:1.8216211795806885\n",
            "step:73, loss:2.126797676086426\n",
            "step:74, loss:6.85312557220459\n",
            "step:75, loss:3.1275784969329834\n",
            "step:76, loss:1.2818726301193237\n",
            "49\n",
            "-----\n",
            "step:0, loss:5.279253005981445\n",
            "step:1, loss:9.20519733428955\n",
            "step:2, loss:4.551219463348389\n",
            "step:3, loss:4.433087348937988\n",
            "step:4, loss:1.2362431287765503\n",
            "step:5, loss:1.0488075017929077\n",
            "step:6, loss:2.37160587310791\n",
            "step:7, loss:17.65277862548828\n",
            "step:8, loss:5.08344841003418\n",
            "step:9, loss:6.725149154663086\n",
            "step:10, loss:20.56277084350586\n",
            "step:11, loss:1.451507806777954\n",
            "step:12, loss:3.283742904663086\n",
            "step:13, loss:3.074193000793457\n",
            "step:14, loss:1.4356515407562256\n",
            "step:15, loss:1.0799939632415771\n",
            "step:16, loss:0.9494548439979553\n",
            "step:17, loss:5.623978614807129\n",
            "step:18, loss:4.873292446136475\n",
            "step:19, loss:3.5492653846740723\n",
            "step:20, loss:1.9395102262496948\n",
            "step:21, loss:3.1613528728485107\n",
            "step:22, loss:1.4961652755737305\n",
            "step:23, loss:18.368478775024414\n",
            "step:24, loss:1.2418713569641113\n",
            "step:25, loss:2.456049680709839\n",
            "step:26, loss:1.8364756107330322\n",
            "step:27, loss:2.535761833190918\n",
            "step:28, loss:2.818067789077759\n",
            "step:29, loss:1.2982600927352905\n",
            "step:30, loss:5.173452377319336\n",
            "step:31, loss:1.372775673866272\n",
            "step:32, loss:1.602840542793274\n",
            "step:33, loss:1.4916244745254517\n",
            "step:34, loss:4.7853474617004395\n",
            "step:35, loss:2.2628073692321777\n",
            "step:36, loss:1.7314785718917847\n",
            "step:37, loss:7.95944881439209\n",
            "step:38, loss:0.6141595840454102\n",
            "step:39, loss:2.495327949523926\n",
            "step:40, loss:2.9434714317321777\n",
            "step:41, loss:1.5159292221069336\n",
            "step:42, loss:0.7854245901107788\n",
            "step:43, loss:71.92264556884766\n",
            "step:44, loss:3.007366180419922\n",
            "step:45, loss:2.39221453666687\n",
            "step:46, loss:0.6605886220932007\n",
            "step:47, loss:3.6293833255767822\n",
            "step:48, loss:0.8681608438491821\n",
            "step:49, loss:10.139233589172363\n",
            "step:50, loss:67.52364349365234\n",
            "step:51, loss:1.15031099319458\n",
            "step:52, loss:1.3942382335662842\n",
            "step:53, loss:2.170198917388916\n",
            "step:54, loss:0.6229629516601562\n",
            "step:55, loss:1.7208189964294434\n",
            "step:56, loss:2.021589517593384\n",
            "step:57, loss:2.24863338470459\n",
            "step:58, loss:1.1397905349731445\n",
            "step:59, loss:2.834388256072998\n",
            "step:60, loss:0.9952837824821472\n",
            "step:61, loss:1.876350998878479\n",
            "step:62, loss:2.1415610313415527\n",
            "step:63, loss:4.073918342590332\n",
            "step:64, loss:10.020793914794922\n",
            "step:65, loss:1.2077691555023193\n",
            "step:66, loss:1.2836196422576904\n",
            "step:67, loss:0.9635261297225952\n",
            "step:68, loss:4.161272048950195\n",
            "step:69, loss:1.901613473892212\n",
            "step:70, loss:0.9442398548126221\n",
            "step:71, loss:19.479684829711914\n",
            "step:72, loss:1.59507417678833\n",
            "step:73, loss:1.283434271812439\n",
            "step:74, loss:2.9670796394348145\n",
            "step:75, loss:2.3501782417297363\n",
            "step:76, loss:1.8116633892059326\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.utils.clip_grad import clip_grad_norm\n",
        "def train(model, optimizer, scheduler, loss_function, epochs, \n",
        "          train_dataloader, device, clip_value=2):\n",
        "    for epoch in range(epochs):\n",
        "        # list1 = []\n",
        "        print(epoch)\n",
        "        print(\"-----\")\n",
        "        best_loss = 1e10\n",
        "        model.train()\n",
        "        for step, batch in enumerate(train_dataloader):  \n",
        "            batch_inputs, batch_masks, batch_labels = \\\n",
        "                               tuple(b.to(device) for b in batch)\n",
        "            model.zero_grad()\n",
        "            outputs = model(batch_inputs, batch_masks)           \n",
        "            loss = loss_function(outputs.squeeze().float(), \n",
        "                             batch_labels.squeeze().float())\n",
        "            print(f'step:{step}, loss:{loss}') \n",
        "            # list1.append(loss)\n",
        "            loss.backward()\n",
        "            clip_grad_norm(model.parameters(), clip_value)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "        # lost_list.append(list1)\n",
        "\n",
        "    return model\n",
        "lost_list = [[]]\n",
        "model = train(model, optimizer, scheduler, loss_function, epochs, train_dataloader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w1sXTNkL_Wx"
      },
      "source": [
        "#Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwuwB9jjMAbu"
      },
      "outputs": [],
      "source": [
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    for batch in dataloader:\n",
        "        batch_inputs, batch_masks, _ = \\\n",
        "                                  tuple(b.to(device) for b in batch)\n",
        "        with torch.no_grad():\n",
        "            output += model(batch_inputs, \n",
        "                            batch_masks).view(1,-1).tolist()[0]\n",
        "    return output\n",
        "\n",
        "def r2_score(outputs, labels):\n",
        "    labels_mean = torch.mean(labels)\n",
        "    ss_tot = torch.sum((labels - labels_mean) ** 2)\n",
        "    ss_res = torch.sum((labels - outputs) ** 2)\n",
        "    r2 = 1 - ss_res / ss_tot\n",
        "    return r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U_lTZJgMDkC"
      },
      "outputs": [],
      "source": [
        "val_set = val_data_fr[['description', 'demand']]\n",
        "val_set['cleaned_description'] = \\\n",
        "                val_set.description.apply(clean_text)\n",
        "encoded_val_corpus = \\\n",
        "                tokenizer(text=val_set.cleaned_description.tolist(),\n",
        "                          add_special_tokens=True,\n",
        "                          padding='max_length',\n",
        "                          truncation='longest_first',\n",
        "                          max_length=300,\n",
        "                          return_attention_mask=True)\n",
        "val_input_ids = np.array(encoded_val_corpus['input_ids'])\n",
        "val_attention_mask = np.array(encoded_val_corpus['attention_mask'])\n",
        "val_labels = val_set.demand.to_numpy()\n",
        "val_dataloader = create_dataloaders(val_input_ids, \n",
        "                         val_attention_mask, val_labels, batch_size)\n",
        "y_pred = predict(model, val_dataloader, device)\n",
        "y_test = val_set.demand.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "yQ1Qq0aKadqj",
        "outputId": "bb03eabe-fd9d-41f0-ed15-c55edf2d844d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# n = data[\"comment_clean\"].str.len()\n",
        "nn, bins, patches = plt.hist(y_test,100, facecolor ='g', alpha=0.75)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "h6AMphxMpEQQ",
        "outputId": "4d43e446-4ee6-43c3-fba2-7134ab9a2d2f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# n = data[\"comment_clean\"].str.len()\n",
        "nn, bins, patches = plt.hist(y_pred,100, facecolor ='g', alpha=0.75)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7LTLBfFtHYq"
      },
      "source": [
        "###r_squared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2FXjWc-MIZO",
        "outputId": "d6e8c9a2-3b24-48a0-846d-33bdf6edb848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_absolute_error:5.95840333116493\n",
            "median_absolute_error:3.697394371032715\n",
            "mean_squared_error:84.55478486192455\n",
            "mean_absolute_percentage_error:5411598324276544.0\n",
            "mdape:1.1502163310845692\n",
            "r_squared:-0.901494799127142\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import median_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.metrics import r2_score\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mdae = median_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "mdape = ((pd.Series(y_test) - pd.Series(y_pred))\\\n",
        "         / pd.Series(y_test)).abs().median()\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "print(f'mean_absolute_error:{mae}')\n",
        "print(f'median_absolute_error:{mdae}')\n",
        "print(f'mean_squared_error:{mse}')\n",
        "print(f'mean_absolute_percentage_error:{mape}')\n",
        "print(f'mdape:{mdape}')\n",
        "print(f'r_squared:{r_squared}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXscCeeWM1H6",
        "outputId": "30b4d064-84d9-4ac8-a159-f639c89f368e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "r_squared:-0.2702942822298109\n"
          ]
        }
      ],
      "source": [
        "def r2_score2(outputs, labels):\n",
        "    labels_mean = labels.mean()\n",
        "    ss_tot = ((labels - labels_mean) ** 2).sum()\n",
        "    ss_res = ((labels - outputs) ** 2).sum()\n",
        "    r2 = 1 - ss_res / ss_tot\n",
        "    return r2\n",
        "r_squared = r2_score2(y_pred, y_test)\n",
        "print(f'r_squared:{r_squared}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE8OPLTvvKxh"
      },
      "outputs": [],
      "source": [
        "y_pred = np.array(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znOwshrvvYdM",
        "outputId": "4b053882-816e-47aa-d634-67db42e25200"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4.09746276894325"
            ]
          },
          "execution_count": 220,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VarK-3f2tM6I"
      },
      "source": [
        "###mean pisson deviance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85D9ev_q2QGN"
      },
      "outputs": [],
      "source": [
        "def custom_poisson_deviance(y_true,y_pred):\n",
        "  prediction=y_pred.copy()\n",
        "  prediction = np.array(prediction)\n",
        "  prediction[np.where(prediction<=0)]=1e-10\n",
        "  return mean_poisson_deviance(y_true, prediction)\n",
        "from sklearn.metrics import mean_poisson_deviance\n",
        "custom_poisson_deviance(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKvmWUnsshIc"
      },
      "source": [
        "#test listing prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fsS2v8ityy9"
      },
      "outputs": [],
      "source": [
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    for batch in dataloader:\n",
        "        batch_inputs, batch_masks = \\\n",
        "                                  tuple(b.to(device) for b in batch)\n",
        "        with torch.no_grad():\n",
        "            output += model(batch_inputs, \n",
        "                            batch_masks).view(1,-1).tolist()[0]\n",
        "    return output\n",
        "\n",
        "def r2_score(outputs, labels):\n",
        "    labels_mean = torch.mean(labels)\n",
        "    ss_tot = torch.sum((labels - labels_mean) ** 2)\n",
        "    ss_res = torch.sum((labels - outputs) ** 2)\n",
        "    r2 = 1 - ss_res / ss_tot\n",
        "    return r2\n",
        "y_pred = predict(model, train_dataloader, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_fr[\"Prediction\"] = y_pred\n",
        "# df_fr[\"Prediction\"].loc[df_fr[\"Prediction\"]<0]=0"
      ],
      "metadata": {
        "id": "86unYZhd-C2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "I5ZZRIijYU6v",
        "outputId": "8230f98e-08bf-412a-d346-bcaf217e0c05"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "nn, bins, patches = plt.hist(df_fr.Prediction, 50, facecolor ='g', alpha=0.75)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3u7y1LguiM2"
      },
      "source": [
        "#fill data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aamqX5zMuE0S"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(PATH_GDRIVE_TMP + \"/test_overall_data_with_prediction_WN.csv\")\n",
        "# data = pd.read_csv(PATH_GDRIVE_TMP + \"/test_overall_data_with_prediction_NN.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data = pickle.load(open(PATH_GDRIVE_TMP+\"/test_listings_overall.pkl\", \"rb\"))\n",
        "data.loc[df_fr.index, \"Prediction\"] = df_fr[\"Prediction\"] "
      ],
      "metadata": {
        "id": "NYmnU5CqFVlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTlLTw-eupsc"
      },
      "outputs": [],
      "source": [
        "pickle.dump(data, open(PATH_GDRIVE_TMP + \"/test_overall_data_with_prediction_NN.pkl\", \"wb\"))\n",
        "data.to_csv(PATH_GDRIVE_TMP+\"/test_overall_data_with_prediction_NN.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(data, open(PATH_GDRIVE_TMP + \"/test_data_with_prediction_WN.pkl\", \"wb\"))\n",
        "data.to_csv(PATH_GDRIVE_TMP+\"/test_overall_data_with_prediction_WN.csv\", index=False)"
      ],
      "metadata": {
        "id": "2eGBco7AGXbi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "I7px9rtuZYKB",
        "NHsLMEXQKaVD",
        "HEe7RJQcKknj",
        "tTsl8fMh-fjF",
        "0W6EIQPmW_LQ",
        "x_yWkIzLa9cO",
        "tfdmKTBdXXhE",
        "UNK0725S5h7l",
        "I7LTLBfFtHYq"
      ],
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "93393b5ebcca437e98e8768dc1672e48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d46bc0aaa634a20a5de046c8b06a62e",
              "IPY_MODEL_9391594ee7e4473bb171f43f1cb14bc6",
              "IPY_MODEL_ba6bd4f8692341cda4d7cd9bbd367526"
            ],
            "layout": "IPY_MODEL_05230496cade47d3ac62d679f7fbe958"
          }
        },
        "1d46bc0aaa634a20a5de046c8b06a62e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f15789be7bfb43f2a1b2d01ed04c196f",
            "placeholder": "​",
            "style": "IPY_MODEL_b1daf5f445a84090b9f7c5a6d45aa17f",
            "value": "Downloading: 100%"
          }
        },
        "9391594ee7e4473bb171f43f1cb14bc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58379656e53d4c80bdda1185cb2998ae",
            "max": 810912,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2768e3bc9e8848f19d60a4be2b58f051",
            "value": 810912
          }
        },
        "ba6bd4f8692341cda4d7cd9bbd367526": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76a033af8f44460684b1b79c399c092d",
            "placeholder": "​",
            "style": "IPY_MODEL_bc14021acf2c46868f2f2ddeb9a5866d",
            "value": " 811k/811k [00:00&lt;00:00, 3.50MB/s]"
          }
        },
        "05230496cade47d3ac62d679f7fbe958": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f15789be7bfb43f2a1b2d01ed04c196f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1daf5f445a84090b9f7c5a6d45aa17f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58379656e53d4c80bdda1185cb2998ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2768e3bc9e8848f19d60a4be2b58f051": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76a033af8f44460684b1b79c399c092d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc14021acf2c46868f2f2ddeb9a5866d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a61ac57486f4fef853e054f964a5b8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3903740f4b2a41a0a5a88c6adf1656bd",
              "IPY_MODEL_4320a2f4d9fa4f77a9839738c3b55933",
              "IPY_MODEL_59856030b3bf4279bb66cb2ab50472c4"
            ],
            "layout": "IPY_MODEL_b431a6fb46d246599cbc14ef9dd293b9"
          }
        },
        "3903740f4b2a41a0a5a88c6adf1656bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dda0da8d32b24a4c9a7b6af7e7699d58",
            "placeholder": "​",
            "style": "IPY_MODEL_8147d1c90cce41fabfa00c691836b2fb",
            "value": "Downloading: 100%"
          }
        },
        "4320a2f4d9fa4f77a9839738c3b55933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af54e8971a9f46c4b1a6fc95aecffa10",
            "max": 508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e6221838e764147a0d756c79df54a35",
            "value": 508
          }
        },
        "59856030b3bf4279bb66cb2ab50472c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff051fe525e24ba59bf6cce708cfb48a",
            "placeholder": "​",
            "style": "IPY_MODEL_e5fec491d4ea432c9471c440d974beb0",
            "value": " 508/508 [00:00&lt;00:00, 25.5kB/s]"
          }
        },
        "b431a6fb46d246599cbc14ef9dd293b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dda0da8d32b24a4c9a7b6af7e7699d58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8147d1c90cce41fabfa00c691836b2fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af54e8971a9f46c4b1a6fc95aecffa10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e6221838e764147a0d756c79df54a35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff051fe525e24ba59bf6cce708cfb48a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5fec491d4ea432c9471c440d974beb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}