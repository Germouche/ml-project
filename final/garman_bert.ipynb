{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfWiL6vlIX7u"
      },
      "source": [
        "#import every needed Library \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM2KmqWmMbwy",
        "outputId": "fd718f8a-08f5-45a3-ea79-225dfdee2410"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from langdetect) (1.15.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=cdf5a8d07959cc2c021b5df915f401d97f28eb770d3d57ceee4158292830f31a\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgY4DzJwINO9",
        "outputId": "de039dc4-febf-445a-bd80-33f731f57493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import warnings\n",
        "from langdetect import detect\n",
        "\n",
        "pd.options.display.max_colwidth = 6000\n",
        "pd.options.display.max_rows = 400\n",
        "np.set_printoptions(suppress=True)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAmIPsfNIjJ1"
      },
      "source": [
        "#loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQaR5-z6JeN0",
        "outputId": "58d28930-14e4-48b0-856d-a8819435a15a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh9hLfbIIdP9"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/ML/project2/test_listings_text.pkl', 'rb') as f:\n",
        "    data = pickle.load(f) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19HmYPMHhT1n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fxak0dQUuC-"
      },
      "outputs": [],
      "source": [
        "CURR_PATH = !pwd\n",
        "# PARAMETERS\n",
        "PATH_DATA = CURR_PATH[0]\n",
        "PATH_GDRIVE_TMP = \"/content/drive/MyDrive/ML/project2\"  # Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVtoxMWbhj1R"
      },
      "outputs": [],
      "source": [
        "data['Listing Title'].head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n8fJcLUiBBL"
      },
      "outputs": [],
      "source": [
        "data['Listing Description'].head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjiqEwqziyVV"
      },
      "outputs": [],
      "source": [
        "data['Listing Description'] .head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyKD1z4KI2sU"
      },
      "source": [
        "##detect language "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2ReFTiYI1nF",
        "outputId": "0074dec8-0020-4966-8f53-1424e7976781"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "de    8212\n",
              "fr    1554\n",
              "it     234\n",
              "Name: Description Langage, dtype: int64"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def detect2(x):\n",
        "  try:\n",
        "    return detect(x)\n",
        "  except:\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ephjYyHw8cWm"
      },
      "source": [
        "##loading train listing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV_au_NiJBZd"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "with open('/content/drive/MyDrive/ML/project2/listings.pkl', 'rb') as f:\n",
        "    data = pickle.load(f) \n",
        "#combine description\n",
        "data['Listing Description'] = data['Listing Title'] + '.' + data['Listing Description']\n",
        "i = data['Listing Description'].isna()\n",
        "data.loc[i,'Listing Description']= data.loc[i,'Listing Title']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#language classify\n",
        "data['Description Langage']=data['Listing Description'].apply(lambda x: detect2(x))\n",
        "print(len(data))\n",
        "#save data with language tag\n",
        "pickle.dump(data, open(PATH_GDRIVE_TMP + \"/data_with_language.pkl\", \"wb\"))\n",
        "data['Description Langage'].value_counts ()"
      ],
      "metadata": {
        "id": "lunN4rR9v_30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQmANUpeZSJ-"
      },
      "source": [
        "###for building model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9J9Pt-CpyPH",
        "outputId": "11a7ee3c-084f-4cca-9304-86c5a46bf4bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "de1: 16361\n",
            "16323\n",
            "de2: 16323\n",
            "425\n",
            "19954\n"
          ]
        }
      ],
      "source": [
        "# data = pickle.load(open(PATH_GDRIVE_TMP+\"/data_with_language.pkl\", \"rb\"))\n",
        "data_de = data.loc[data['Description Langage'] == 'de', (\"Listing Description\",\"Listing Title\",\"Demand\")]\n",
        "data_fr = data.loc[data['Description Langage'] == 'fr', (\"Listing Description\",\"Listing Title\",\"Demand\")]\n",
        "data_it = data.loc[data['Description Langage'] == 'it', (\"Listing Description\",\"Listing Title\",\"Demand\")]\n",
        "print(\"de1:\",len(data_de))\n",
        "data_de = data_de[data_de[\"Listing Description\"].apply(len)>100]\n",
        "print(len(data_de))\n",
        "print(\"de2:\",len(data_de))\n",
        "print(len(data_it))\n",
        "print(len(data_de)+len(data_fr)+len(data_it))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEV8LtrKCLu3"
      },
      "outputs": [],
      "source": [
        "data_de.columns = ['description', 'title', 'demand']\n",
        "data_fr.columns = ['description', 'title', 'demand']\n",
        "data_it.columns = ['description', 'title', 'demand']\n",
        "df_de, val_data_de = train_test_split(data_de, test_size=0.2, random_state=30)\n",
        "df_fr, val_data_fr = train_test_split(data_fr, test_size=0.2, random_state=30)\n",
        "df_it, val_data_it = train_test_split(data_it, test_size=0.2, random_state=30)\n",
        "print(df_de.head(2))\n",
        "print(df_fr.head(2))\n",
        "print(df_it.head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCtCyOk38kYK"
      },
      "source": [
        "##loading test listing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52_NfIVL8Wxu"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "with open('/content/drive/MyDrive/ML/project2/test_listings_overall.pkl', 'rb') as f:\n",
        "    data = pickle.load(f) \n",
        "data = pickle.load(open(PATH_GDRIVE_TMP+\"/test_overall_data_with_language.pkl\", \"rb\"))\n",
        "data['Listing Description'] = data['Listing Title'] + '.' + data['Listing Description']\n",
        "i = data['Listing Description'].isna()\n",
        "data.loc[i,'Listing Description']= data.loc[i,'Listing Title']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS0aB7ultmC5"
      },
      "outputs": [],
      "source": [
        "data['Description Langage']=data['Listing Description'].apply(lambda x: detect2(x))\n",
        "print(len(data))\n",
        "pickle.dump(data, open(PATH_GDRIVE_TMP + \"/test_data_with_language.pkl\", \"wb\"))\n",
        "data['Description Langage'].value_counts ()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7px9rtuZYKB"
      },
      "source": [
        "###for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pCfPVRaJF-g"
      },
      "outputs": [],
      "source": [
        "# data = pickle.load(open(PATH_GDRIVE_TMP+\"/test_overall_data_with_language.pkl\", \"rb\"))\n",
        "data_de = data.loc[data['Description Langage'] == 'de', (\"Listing Description\",\"Listing Title\",\"Prediction\")]\n",
        "data_fr = data.loc[data['Description Langage'] == 'fr', (\"Listing Description\",\"Listing Title\",\"Prediction\")]\n",
        "data_it = data.loc[data['Description Langage'] == 'it', (\"Listing Description\",\"Listing Title\",\"Prediction\")]\n",
        "print(\"before filter:\",len(data_de))\n",
        "data_de = data_de[data_de[\"Listing Description\"].apply(len)>100]\n",
        "print(len(data_fr))\n",
        "print(\"after filter:\",len(data_de))\n",
        "print(len(data_it))\n",
        "print(len(data_de)+len(data_fr)+len(data_de))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEsN6jYdZaEu"
      },
      "outputs": [],
      "source": [
        "data_de.columns = ['description', 'title', 'Prediction']\n",
        "data_fr.columns = ['description', 'title', 'Prediction']\n",
        "data_it.columns = ['description', 'title', 'Prediction']\n",
        "df_de = data_de\n",
        "df_fr = data_fr\n",
        "df_it = data_it\n",
        "print(df_de.head(2))\n",
        "print(df_fr.head(2))\n",
        "print(df_it.head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXHe-rkCJveI"
      },
      "source": [
        "#data processing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHsLMEXQKaVD"
      },
      "source": [
        "##clean1\n",
        "*   Converting text to lower-case\n",
        "*   Standardising representations of a same entity such as “€”, “euro” and “euros” or “m2” and “m²”\n",
        "*   Cleaning out certain patterns that are unlikely to be meaningful such as URLs, phone numbers, emails and bank account references.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7v6YY56J2Yh"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def treat_euro(text):\n",
        "    text = re.sub(r'(euro[^s])|(euros)|(€)', ' euros', text)\n",
        "    return text\n",
        "def treat_m2(text):\n",
        "    text = re.sub(r'(m2)|(m²)', ' m²', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7DDEjetKXPD"
      },
      "outputs": [],
      "source": [
        "def filter_ibans(text):\n",
        "    pattern = r'fr\\d{2}[ ]\\d{4}[ ]\\d{4}[ ]\\d{4}[ ]\\d{4}[ ]\\d{2}|fr\\d{20}|fr[ ]\\d{2}[ ]\\d{3}[ ]\\d{3}[ ]\\d{3}[ ]\\d{5}'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "def remove_space_between_numbers(text):\n",
        "    text = re.sub(r'(\\d)\\s+(\\d)', r'\\1\\2', text)\n",
        "    return text\n",
        "def filter_emails(text):\n",
        "    pattern = r'(?:(?!.*?[.]{2})[a-zA-Z0-9](?:[a-zA-Z0-9.+!%-]{1,64}|)|\\\"[a-zA-Z0-9.+!% -]{1,64}\\\")@[a-zA-Z0-9][a-zA-Z0-9.-]+(.[a-z]{2,}|.[0-9]{1,})'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "def filter_ref(text):\n",
        "    pattern = r'(\\(*)(ref|réf)(\\.|[ ])\\d+(\\)*)'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "def filter_websites(text):\n",
        "    pattern = r'(http\\:\\/\\/|https\\:\\/\\/)?([a-z0-9][a-z0-9\\-]*\\.)+[a-z][a-z\\-]*'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "def filter_phone_numbers(text):\n",
        "    pattern = r'(?:(?:\\+|00)33[\\s.-]{0,3}(?:\\(0\\)[\\s.-]{0,3})?|0)[1-9](?:(?:[\\s.-]?\\d{2}){4}|\\d{2}(?:[\\s.-]?\\d{3}){2})|(\\d{2}[ ]\\d{2}[ ]\\d{3}[ ]\\d{3})'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "def filter_tag(text):\n",
        "    pattern = r'<[^>]+>'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBvJf5xpKiZZ"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace(u'\\xa0', u' ')\n",
        "    text = treat_m2(text)\n",
        "    text = treat_euro(text)\n",
        "    text = filter_phone_numbers(text)\n",
        "    text = filter_emails(text)\n",
        "    text = filter_ibans(text)\n",
        "    text = filter_ref(text)\n",
        "    text = filter_websites(text)\n",
        "    text = remove_space_between_numbers(text)\n",
        "    text = filter_tag(text)\n",
        "    return text\n",
        "df_de['cleaned_description'] = df_de.description.apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiucDYWAwihP"
      },
      "outputs": [],
      "source": [
        "df_de"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hocMNlkgYcTj"
      },
      "source": [
        "##clean2\n",
        "only in case of dealing stop word (which is not necessary in this task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEe7RJQcKknj"
      },
      "source": [
        "###for french"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euUn8_QvYM6x",
        "outputId": "6ead212f-ae14-40bd-9c50-859a496b9325"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "stemmer = SnowballStemmer(\"french\")\n",
        "stop_words = set(stopwords.words(\"french\"))\n",
        "\n",
        "\n",
        "def clean_text_fr(text, for_embedding=False):\n",
        "    \"\"\"\n",
        "        - remove any html tags (< /br> often found)\n",
        "        - Keep only ASCII + European Chars and whitespace, no digits\n",
        "        - remove single letter chars\n",
        "        - convert all whitespaces (tabs etc.) to single wspace\n",
        "        if not for embedding (but e.g. tdf-idf):\n",
        "        - all lowercase\n",
        "        - remove stopwords, punctuation and stemm\n",
        "    \"\"\"\n",
        "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
        "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
        "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
        "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
        "    if for_embedding:\n",
        "        # Keep punctuation\n",
        "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
        "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
        "\n",
        "    text = re.sub(RE_TAGS, \" \", text)\n",
        "    text = re.sub(RE_ASCII, \" \", text)\n",
        "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
        "    text = re.sub(RE_WSPACE, \" \", text)\n",
        "\n",
        "    word_tokens = word_tokenize(text)\n",
        "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
        "\n",
        "    if for_embedding:\n",
        "        # no stemming, lowering and punctuation / stop words removal\n",
        "        words_filtered = word_tokens\n",
        "    else:\n",
        "        words_filtered = [\n",
        "            stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n",
        "        ]\n",
        "\n",
        "    text_clean = \" \".join(words_filtered)\n",
        "    return text_clean\n",
        "\n",
        "df_fr['cleaned_description'] = df_fr.description.apply(clean_text_fr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTsl8fMh-fjF"
      },
      "source": [
        "###for Italian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVhW1PrS4-nG",
        "outputId": "6745b398-7d66-4c47-ccc2-33c25a820358"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "stemmer = SnowballStemmer(\"italian\")\n",
        "stop_words = set(stopwords.words(\"italian\"))\n",
        "\n",
        "\n",
        "def clean_text_it(text, for_embedding=False):\n",
        "    \"\"\"\n",
        "        - remove any html tags (< /br> often found)\n",
        "        - Keep only ASCII + European Chars and whitespace, no digits\n",
        "        - remove single letter chars\n",
        "        - convert all whitespaces (tabs etc.) to single wspace\n",
        "        if not for embedding (but e.g. tdf-idf):\n",
        "        - all lowercase\n",
        "        - remove stopwords, punctuation and stemm\n",
        "    \"\"\"\n",
        "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
        "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
        "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
        "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
        "    if for_embedding:\n",
        "        # Keep punctuation\n",
        "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
        "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
        "\n",
        "    text = re.sub(RE_TAGS, \" \", text)\n",
        "    text = re.sub(RE_ASCII, \" \", text)\n",
        "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
        "    text = re.sub(RE_WSPACE, \" \", text)\n",
        "\n",
        "    word_tokens = word_tokenize(text)\n",
        "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
        "\n",
        "    if for_embedding:\n",
        "        # no stemming, lowering and punctuation / stop words removal\n",
        "        words_filtered = word_tokens\n",
        "    else:\n",
        "        words_filtered = [\n",
        "            stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n",
        "        ]\n",
        "\n",
        "    text_clean = \" \".join(words_filtered)\n",
        "    return text_clean\n",
        "\n",
        "df_it['cleaned_description'] = df_it.description.apply(clean_text_it)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W6EIQPmW_LQ"
      },
      "source": [
        "###for german"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePf0FD0iXCLF",
        "outputId": "8919e1ee-ad5a-427d-c837-f1924a3135ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "stemmer = SnowballStemmer(\"german\")\n",
        "stop_words = set(stopwords.words(\"german\"))\n",
        "\n",
        "def clean_text_de(text, for_embedding=False):\n",
        "    \"\"\"\n",
        "        - remove any html tags (< /br> often found)\n",
        "        - Keep only ASCII + European Chars and whitespace, no digits\n",
        "        - remove single letter chars\n",
        "        - convert all whitespaces (tabs etc.) to single wspace\n",
        "        if not for embedding (but e.g. tdf-idf):\n",
        "        - all lowercase\n",
        "        - remove stopwords, punctuation and stemm\n",
        "    \"\"\"\n",
        "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
        "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
        "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
        "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
        "    if for_embedding:\n",
        "        # Keep punctuation\n",
        "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
        "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
        "\n",
        "    text = re.sub(RE_TAGS, \" \", text)\n",
        "    text = re.sub(RE_ASCII, \" \", text)\n",
        "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
        "    text = re.sub(RE_WSPACE, \" \", text)\n",
        "\n",
        "    word_tokens = word_tokenize(text)\n",
        "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
        "\n",
        "    if for_embedding:\n",
        "        # no stemming, lowering and punctuation / stop words removal\n",
        "        words_filtered = word_tokens\n",
        "    else:\n",
        "        words_filtered = [\n",
        "            stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n",
        "        ]\n",
        "\n",
        "    text_clean = \" \".join(words_filtered)\n",
        "    return text_clean\n",
        "\n",
        "df_de['cleaned_description'] = df_de.description.apply(clean_text_de)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7nHk4-iEeTE",
        "outputId": "533028d4-b855-4f42-aa5d-aba593a78699"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting translate\n",
            "  Downloading translate-3.6.1-py2.py3-none-any.whl (12 kB)\n",
            "Collecting libretranslatepy==2.1.1\n",
            "  Downloading libretranslatepy-2.1.1-py3-none-any.whl (3.2 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from translate) (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from translate) (4.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from translate) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->translate) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->translate) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->translate) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->translate) (1.24.3)\n",
            "Installing collected packages: libretranslatepy, translate\n",
            "Successfully installed libretranslatepy-2.1.1 translate-3.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0trsl63Ir94"
      },
      "outputs": [],
      "source": [
        "! pip install -U deep-translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSuUbePYEh6C"
      },
      "outputs": [],
      "source": [
        "from translate import Translator\n",
        "to_lang=\"fr\"\n",
        "translator = Translator(to_lang=to_lang)\n",
        "df_it['description'] = translator.translate(df_it['description'].to_string())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMcXR5UpdcxO"
      },
      "outputs": [],
      "source": [
        "it = df_it['cleaned_description']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JAgZuV3HKnW"
      },
      "outputs": [],
      "source": [
        "df_fr = df_fr.append(df_it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "okJCveBBX8Vl",
        "outputId": "e9471131-ac8d-4cff-a7d7-5af69c91df7b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1f5fe586-1e4c-43f2-93be-61938f9cfd4e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>description</th>\n",
              "      <th>title</th>\n",
              "      <th>prix</th>\n",
              "      <th>cleaned_description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 41 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 35 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 35 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 33 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 31 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 30 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 28 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 25 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 16 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 14 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 03 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 00 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 10 MINUTES 55 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE ...</td>\n",
              "      <td>Appartamento di stabile di alto standing</td>\n",
              "      <td>0.0</td>\n",
              "      <td>lumin appart pian vist lag press centr citt vicin mezz pubblic grand sal ampi terrazz splendid vist golf lug modern spazios cucin abit camer bagn ensu second bagn cabin docc wc ospit ulterior cam parquet grand armad mur corridoi tripl vetr contator individual riscald serpentin npost aut autorimess chf ndipson sub naspett piac chiam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 41 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 35 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 35 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 33 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 31 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 30 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 28 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 25 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 16 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 14 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 03 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 00 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 10 MINUTES 55 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE ...</td>\n",
              "      <td>APPARTAMENTO IN STABILE D'EPOCA NEL CENTRO CITTÀ</td>\n",
              "      <td>1.0</td>\n",
              "      <td>bellissim stabil epoc lumin appart ampi local vicin negoz attrazion mezz pubblic nlo stabil stat estern risan nl appart dispon sal cucin vist pav parquet due cam parquet bagn vasc colonn lav asciug balcon vist via nass ampi corridoi pav mosaic nnon perd occasion poter viv bellezz stabil stabil cuor via nass approffitt tutt comod centr citt offrir naspett telefon</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f5fe586-1e4c-43f2-93be-61938f9cfd4e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1f5fe586-1e4c-43f2-93be-61938f9cfd4e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1f5fe586-1e4c-43f2-93be-61938f9cfd4e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       description  \\\n",
              "0  MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 41 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 35 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 35 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 33 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 31 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 30 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 28 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 25 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 16 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 14 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 03 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 00 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 10 MINUTES 55 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE ...   \n",
              "1  MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 41 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 35 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 35 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 33 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 31 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 30 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 28 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 25 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 16 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 14 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 03 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 11 MINUTES 00 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  17 HOURS 10 MINUTES 55 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE ...   \n",
              "\n",
              "                                              title  prix  \\\n",
              "0          Appartamento di stabile di alto standing   0.0   \n",
              "1  APPARTAMENTO IN STABILE D'EPOCA NEL CENTRO CITTÀ   1.0   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                            cleaned_description  \n",
              "0                                 lumin appart pian vist lag press centr citt vicin mezz pubblic grand sal ampi terrazz splendid vist golf lug modern spazios cucin abit camer bagn ensu second bagn cabin docc wc ospit ulterior cam parquet grand armad mur corridoi tripl vetr contator individual riscald serpentin npost aut autorimess chf ndipson sub naspett piac chiam  \n",
              "1  bellissim stabil epoc lumin appart ampi local vicin negoz attrazion mezz pubblic nlo stabil stat estern risan nl appart dispon sal cucin vist pav parquet due cam parquet bagn vasc colonn lav asciug balcon vist via nass ampi corridoi pav mosaic nnon perd occasion poter viv bellezz stabil stabil cuor via nass approffitt tutt comod centr citt offrir naspett telefon  "
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_it.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mby9rdOGcJxM"
      },
      "source": [
        "#German model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7z8BAyRKvU8"
      },
      "source": [
        "##Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoET8vZjOSZm",
        "outputId": "67ca5b5a-c816-4437-8f6f-19ec794ea95b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 85.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 65.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 4.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-OGslPmO4ds"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, TFBertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "fa752e777d2840ee97698082e4140bcd",
            "0d263f897b644e2d916fe26204fff383",
            "dccece74a3284801b04a9b6f83c397e6",
            "f449d83865c04d0480ed380dcbad2dd1",
            "ace743ffbbc24c49a3eb61707618323d",
            "6c78274637394660b14d5b053dfc7b40",
            "c305748f69bd45afa1e2e3366dc3a1b5",
            "09b5e0b7c4134951b7a85404430b3c64",
            "49307881a6684b1dbfa571e91b132f51",
            "719fd2dafd674483b7aeca0baf6321bf",
            "9a581650cb0a4260ab44c83f73f56c93",
            "c31031556bce4986a8787fe0de219ac3",
            "2885954765ed496c96768495fce93070",
            "efc178e026454948b3e6b7bbbf932da9",
            "2184e3de3e6f4b60b139c440661a0248",
            "f79fa6ff30db44c28e8981af4643b98d",
            "a336d0811f2f4ac19affc51779d0ba18",
            "297d7ae09c484fe792d69dccf9852672",
            "caff6a3db8cf4240aa5fc33542796eba",
            "ec8d3e54100c4e28bb0bbb2c327d607f",
            "3b14b2efb86343068f76d7fcdaa3570d",
            "752c05b154a44bf29543a2e042be0ded",
            "66cd42e29b2c4d7a9749d747ca437215",
            "d99ae2955daf49c0a1963489c7d733b0",
            "3b634c4cc0e64ea7968f4bd2f8add018",
            "13446137d8a549d2bda6eb547e3f4b08",
            "59eb0e5681d344908a0204d2d3ee1ca2",
            "b9666abebae44bb4964565ae1475ddc6",
            "c1ba18bc392d44259e72a739328c39c1",
            "4e7ddf163b2a4042aa227f5a4cf1d42e",
            "3b578bf0c3064d9188eafb2ffcc30b27",
            "0b5ab496d104418e9b8169aba60560a5",
            "8d833ac3873e4575a2f3a56f3a39252d"
          ]
        },
        "id": "wbpSLQ8IPWl8",
        "outputId": "99f06668-afc6-4f73-990b-8ae7017cf22f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/255k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa752e777d2840ee97698082e4140bcd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c31031556bce4986a8787fe0de219ac3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66cd42e29b2c4d7a9749d747ca437215"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-german-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZ-SKp2mKupY"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
        "encoded_corpus = tokenizer(text=df_de.cleaned_description.tolist(),\n",
        "                            add_special_tokens=True,\n",
        "                            padding='max_length',\n",
        "                            truncation='longest_first',\n",
        "                            max_length=300,\n",
        "                            return_attention_mask=True)\n",
        "input_ids = encoded_corpus['input_ids']\n",
        "attention_mask = encoded_corpus['attention_mask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5D0WQtYLfiH",
        "outputId": "0964f039-645b-4f79-b447-1b7fe79b2bf8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13058"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4s9jUomCFCa"
      },
      "source": [
        "###for building model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDyIimwLK5Sc",
        "outputId": "0bfa4e69-a341-47e6-b9b8-47c7a76acf87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (569 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "def filter_long_descriptions(tokenizer, descriptions, max_len):\n",
        "    indices = []\n",
        "    lengths = tokenizer(descriptions, padding=False, \n",
        "                     truncation=False, return_length=True)['length']\n",
        "    for i in range(len(descriptions)):\n",
        "        if lengths[i] <= max_len-2:\n",
        "            indices.append(i)\n",
        "    return indices\n",
        "short_descriptions = filter_long_descriptions(tokenizer, \n",
        "                               df_de.cleaned_description.tolist(), 300)\n",
        "input_ids = np.array(input_ids)[short_descriptions]\n",
        "attention_mask = np.array(attention_mask)[short_descriptions]\n",
        "labels = df_de.demand.to_numpy()[short_descriptions]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ2DEXgBCBWr"
      },
      "source": [
        "###for testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsQ-zmjpB9r_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def filter_long_descriptions(tokenizer, descriptions, max_len):\n",
        "    indices = []\n",
        "    lengths = tokenizer(descriptions, padding=False, \n",
        "                     truncation=False, return_length=True)['length']\n",
        "    for i in range(len(descriptions)):\n",
        "        if lengths[i] <= max_len-2:\n",
        "            indices.append(i)\n",
        "    return indices\n",
        "short_descriptions = filter_long_descriptions(tokenizer, \n",
        "                               df_de.cleaned_description.tolist(), 300)\n",
        "input_ids = np.array(input_ids)[short_descriptions]\n",
        "attention_mask = np.array(attention_mask)[short_descriptions]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH9xCgHELBCS"
      },
      "source": [
        "##Input formatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZxB-jj7DRBy"
      },
      "source": [
        "###for model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yed6A-WmDLKp"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "test_size = 0.0001\n",
        "seed = 42\n",
        "train_inputs, test_inputs, train_labels, test_labels = \\\n",
        "            train_test_split(input_ids, labels, test_size=test_size, \n",
        "                             random_state=seed)\n",
        "train_masks, test_masks, _, _ = train_test_split(attention_mask, \n",
        "                                        labels, test_size=test_size, \n",
        "                                        random_state=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmApqgPMDOjV"
      },
      "source": [
        "###for test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQaAbR7V6Bbn"
      },
      "outputs": [],
      "source": [
        "train_inputs = input_ids\n",
        "train_masks = attention_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMlDBFRF5pWw"
      },
      "source": [
        "##create_dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLRCxvJeBugI"
      },
      "source": [
        "###dataloader for training model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjbjBT6Q65yI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "batch_size = 32\n",
        "def create_dataloaders(inputs, masks, labels, batch_size):\n",
        "    input_tensor = torch.tensor(inputs)\n",
        "    mask_tensor = torch.tensor(masks)\n",
        "    labels_tensor = torch.tensor(labels)\n",
        "    dataset = TensorDataset(input_tensor, mask_tensor, \n",
        "                            labels_tensor)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, \n",
        "                            shuffle=True)\n",
        "    return dataloader\n",
        "train_dataloader = create_dataloaders(train_inputs, train_masks, \n",
        "                                      train_labels, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhIIDTTqBnOu"
      },
      "source": [
        "###dataloader for testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfbNG8nXBtaS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "batch_size = 32\n",
        "def create_dataloaders(inputs, masks, batch_size):\n",
        "    input_tensor = torch.tensor(inputs)\n",
        "    mask_tensor = torch.tensor(masks)\n",
        "    dataset = TensorDataset(input_tensor, mask_tensor)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, \n",
        "                            shuffle=True)\n",
        "    return dataloader\n",
        "train_dataloader = create_dataloaders(train_inputs, train_masks, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdpU0WF7683_"
      },
      "source": [
        "###with test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXeaOiBcLMA6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "batch_size = 32\n",
        "def create_dataloaders(inputs, masks, labels, batch_size):\n",
        "    input_tensor = torch.tensor(inputs)\n",
        "    mask_tensor = torch.tensor(masks)\n",
        "    labels_tensor = torch.tensor(labels)\n",
        "    dataset = TensorDataset(input_tensor, mask_tensor, \n",
        "                            labels_tensor)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, \n",
        "                            shuffle=True)\n",
        "    return dataloader\n",
        "train_dataloader = create_dataloaders(train_inputs, train_masks, \n",
        "                                      train_labels, batch_size)\n",
        "test_dataloader = create_dataloaders(test_inputs, test_masks, \n",
        "                                     test_labels, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTLKK7qYLQSu"
      },
      "source": [
        "##Implementing the model in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "a00d5a4e623e4e639c0187250c741266",
            "d60c01cb114b4c33a15c23ae7c6dcb3a",
            "b6f46243d9d742e6a6b2da24791dd6a2",
            "ffb72a539e7b4c0e86a3487734359e2b",
            "2c4b1a72d94f49aea8ac50978fb5bde7",
            "62e42dc51f8e4c7e9d949af503e94550",
            "f71d0bb6e73a4d309605d29b42dea15a",
            "c6a8ed8cf5274cb4b61de39a09f3df31",
            "931f40a152344f46ac54acd7709a7b9c",
            "49c0f9ffe3c24a0bb4d3d5889c31d888",
            "57a7e117ead843b4978df4c1ff180cc6"
          ]
        },
        "id": "j2vWVmIyLXEM",
        "outputId": "9834691c-60b2-4033-8ea0-be6392b203e0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/439M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a00d5a4e623e4e639c0187250c741266"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "class BertRegressor(nn.Module):\n",
        "    \n",
        "    def __init__(self, drop_rate=0.2):\n",
        "        \n",
        "        super(BertRegressor, self).__init__()\n",
        "        D_in, D_out = 768, 1\n",
        "        self.Bert = \\\n",
        "                   BertModel.from_pretrained('bert-base-german-cased')\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Dropout(drop_rate),\n",
        "            nn.Linear(D_in, D_out))\n",
        "    def forward(self, input_ids, attention_masks):\n",
        "        \n",
        "        outputs = self.Bert(input_ids, attention_masks)\n",
        "        class_label_output = outputs[1]\n",
        "        outputs = self.regressor(class_label_output)\n",
        "        return outputs\n",
        "model = BertRegressor(drop_rate=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maXTtsQJLb1N"
      },
      "source": [
        "##Setting up the training environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYHqHNYrLaCW",
        "outputId": "b3cca4b0-1e3c-4fdf-9a64-d5b95f378381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertRegressor(\n",
              "  (Bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (regressor): Sequential(\n",
              "    (0): Dropout(p=0.2, inplace=False)\n",
              "    (1): Linear(in_features=768, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU.\")\n",
        "else:\n",
        "    print(\"No GPU available, using the CPU instead.\")\n",
        "    device = torch.device(\"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3x9KepPLjDv"
      },
      "source": [
        "##Optimizer, scheduler and loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k31eQSR5Lk8f"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=5e-5,\n",
        "                  eps=1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLdQdVENLoVP"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "epochs = 30\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,       \n",
        "                 num_warmup_steps=0, num_training_steps=total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWNH6_3mLq7T"
      },
      "outputs": [],
      "source": [
        "loss_function = nn.MSELoss()\n",
        "# loss_function = nn.PoissonNLLLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWtGr2SLLtLb"
      },
      "source": [
        "##Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xDHKBjKLuWJ",
        "outputId": "96f220ac-a283-4738-aa52-667505471635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "step:123, loss:182.42529296875\n",
            "step:124, loss:216.08663940429688\n",
            "step:125, loss:249.96258544921875\n",
            "step:126, loss:29.198301315307617\n",
            "step:127, loss:55.558990478515625\n",
            "step:128, loss:240.7830810546875\n",
            "step:129, loss:133.007080078125\n",
            "step:130, loss:553.751708984375\n",
            "step:131, loss:50.759456634521484\n",
            "step:132, loss:34.23454666137695\n",
            "step:133, loss:67.90618896484375\n",
            "step:134, loss:100.80609893798828\n",
            "step:135, loss:142.29544067382812\n",
            "step:136, loss:53.247676849365234\n",
            "step:137, loss:73.38996887207031\n",
            "step:138, loss:19.991769790649414\n",
            "step:139, loss:71.67697143554688\n",
            "step:140, loss:26.482393264770508\n",
            "step:141, loss:52.480979919433594\n",
            "step:142, loss:142.80465698242188\n",
            "step:143, loss:84.02981567382812\n",
            "step:144, loss:323.5472412109375\n",
            "step:145, loss:246.22850036621094\n",
            "step:146, loss:63.99395751953125\n",
            "step:147, loss:8136.80859375\n",
            "step:148, loss:60.8900146484375\n",
            "step:149, loss:83.83916473388672\n",
            "step:150, loss:551.5324096679688\n",
            "step:151, loss:43.98417663574219\n",
            "step:152, loss:42.31993865966797\n",
            "step:153, loss:40.87052536010742\n",
            "step:154, loss:28.097938537597656\n",
            "step:155, loss:29.979469299316406\n",
            "step:156, loss:150.56112670898438\n",
            "step:157, loss:6.961120128631592\n",
            "step:158, loss:52.83811950683594\n",
            "step:159, loss:39.99250793457031\n",
            "step:160, loss:215.82586669921875\n",
            "step:161, loss:42.863624572753906\n",
            "step:162, loss:130.95957946777344\n",
            "step:163, loss:511.8238220214844\n",
            "step:164, loss:152.7767333984375\n",
            "step:165, loss:87.7284927368164\n",
            "step:166, loss:43.49051284790039\n",
            "step:167, loss:8.984334945678711\n",
            "step:168, loss:45.062767028808594\n",
            "step:169, loss:279.987060546875\n",
            "step:170, loss:105.1126708984375\n",
            "step:171, loss:58.85108947753906\n",
            "step:172, loss:243.5793914794922\n",
            "step:173, loss:91.10958099365234\n",
            "step:174, loss:18.14618682861328\n",
            "step:175, loss:37.89497375488281\n",
            "step:176, loss:52.1689567565918\n",
            "step:177, loss:840.8696899414062\n",
            "step:178, loss:65.6087875366211\n",
            "step:179, loss:75.06352996826172\n",
            "step:180, loss:27.312862396240234\n",
            "step:181, loss:143.3386688232422\n",
            "step:182, loss:34.233436584472656\n",
            "step:183, loss:337.653564453125\n",
            "step:184, loss:94.84474182128906\n",
            "step:185, loss:107.98942565917969\n",
            "step:186, loss:53.89543151855469\n",
            "step:187, loss:91.4302978515625\n",
            "step:188, loss:20.965160369873047\n",
            "step:189, loss:28.13958740234375\n",
            "step:190, loss:43.88142776489258\n",
            "step:191, loss:298.3175354003906\n",
            "step:192, loss:19.985326766967773\n",
            "step:193, loss:243.61105346679688\n",
            "step:194, loss:225.86917114257812\n",
            "step:195, loss:148.0177764892578\n",
            "step:196, loss:29.17018699645996\n",
            "step:197, loss:45.04393005371094\n",
            "step:198, loss:111.57489013671875\n",
            "step:199, loss:25.535247802734375\n",
            "step:200, loss:60.065799713134766\n",
            "step:201, loss:411.4248962402344\n",
            "step:202, loss:44.136192321777344\n",
            "6\n",
            "-----\n",
            "step:0, loss:211.48558044433594\n",
            "step:1, loss:123.96305084228516\n",
            "step:2, loss:119.58193969726562\n",
            "step:3, loss:31.217079162597656\n",
            "step:4, loss:183.0951690673828\n",
            "step:5, loss:46.02820587158203\n",
            "step:6, loss:21.104351043701172\n",
            "step:7, loss:247.6198272705078\n",
            "step:8, loss:63.2404670715332\n",
            "step:9, loss:59.55928039550781\n",
            "step:10, loss:381.70611572265625\n",
            "step:11, loss:765.4153442382812\n",
            "step:12, loss:125.01080322265625\n",
            "step:13, loss:42.980682373046875\n",
            "step:14, loss:913.197265625\n",
            "step:15, loss:66.73454284667969\n",
            "step:16, loss:29.20781135559082\n",
            "step:17, loss:124.24275207519531\n",
            "step:18, loss:98.42362976074219\n",
            "step:19, loss:17.099044799804688\n",
            "step:20, loss:254.38441467285156\n",
            "step:21, loss:24.539159774780273\n",
            "step:22, loss:60.947540283203125\n",
            "step:23, loss:381.98236083984375\n",
            "step:24, loss:10.113009452819824\n",
            "step:25, loss:59.85991668701172\n",
            "step:26, loss:298.81036376953125\n",
            "step:27, loss:109.04362487792969\n",
            "step:28, loss:560.4298095703125\n",
            "step:29, loss:65.89940643310547\n",
            "step:30, loss:185.1684112548828\n",
            "step:31, loss:89.03889465332031\n",
            "step:32, loss:141.60276794433594\n",
            "step:33, loss:57.71614074707031\n",
            "step:34, loss:238.9995574951172\n",
            "step:35, loss:85.93872833251953\n",
            "step:36, loss:128.4910430908203\n",
            "step:37, loss:69.39617919921875\n",
            "step:38, loss:13146.724609375\n",
            "step:39, loss:56.44026184082031\n",
            "step:40, loss:104.89257049560547\n",
            "step:41, loss:38.82521438598633\n",
            "step:42, loss:50.71687698364258\n",
            "step:43, loss:173.08172607421875\n",
            "step:44, loss:126.34515380859375\n",
            "step:45, loss:33.67598342895508\n",
            "step:46, loss:258.45220947265625\n",
            "step:47, loss:25.39314079284668\n",
            "step:48, loss:21.403154373168945\n",
            "step:49, loss:4901.669921875\n",
            "step:50, loss:168.36036682128906\n",
            "step:51, loss:421.7411193847656\n",
            "step:52, loss:64.04557800292969\n",
            "step:53, loss:184.20700073242188\n",
            "step:54, loss:152.84506225585938\n",
            "step:55, loss:66.502685546875\n",
            "step:56, loss:29.530330657958984\n",
            "step:57, loss:136.65916442871094\n",
            "step:58, loss:82.26181030273438\n",
            "step:59, loss:1311.552490234375\n",
            "step:60, loss:44.184242248535156\n",
            "step:61, loss:68.91990661621094\n",
            "step:62, loss:251.77639770507812\n",
            "step:63, loss:574.0867919921875\n",
            "step:64, loss:17.048301696777344\n",
            "step:65, loss:105.82926940917969\n",
            "step:66, loss:23.781587600708008\n",
            "step:67, loss:660.700927734375\n",
            "step:68, loss:546.13037109375\n",
            "step:69, loss:13.605119705200195\n",
            "step:70, loss:612.6529541015625\n",
            "step:71, loss:59.79082107543945\n",
            "step:72, loss:10.233131408691406\n",
            "step:73, loss:38.501243591308594\n",
            "step:74, loss:159.82347106933594\n",
            "step:75, loss:79.47789764404297\n",
            "step:76, loss:360.4187927246094\n",
            "step:77, loss:8006.7021484375\n",
            "step:78, loss:465.7305908203125\n",
            "step:79, loss:41.203392028808594\n",
            "step:80, loss:204.77264404296875\n",
            "step:81, loss:103.83526611328125\n",
            "step:82, loss:24.886775970458984\n",
            "step:83, loss:68.58883666992188\n",
            "step:84, loss:471.6228332519531\n",
            "step:85, loss:102.04960632324219\n",
            "step:86, loss:90.38339233398438\n",
            "step:87, loss:41.300567626953125\n",
            "step:88, loss:143.92587280273438\n",
            "step:89, loss:64.72940826416016\n",
            "step:90, loss:60.64546203613281\n",
            "step:91, loss:85.18679809570312\n",
            "step:92, loss:104.51274871826172\n",
            "step:93, loss:38.338626861572266\n",
            "step:94, loss:50.232696533203125\n",
            "step:95, loss:20.416961669921875\n",
            "step:96, loss:954.64208984375\n",
            "step:97, loss:103.59663391113281\n",
            "step:98, loss:24.009265899658203\n",
            "step:99, loss:13.063379287719727\n",
            "step:100, loss:97.18323516845703\n",
            "step:101, loss:7922.0869140625\n",
            "step:102, loss:57.90127182006836\n",
            "step:103, loss:65.73171997070312\n",
            "step:104, loss:99.3788833618164\n",
            "step:105, loss:255.90335083007812\n",
            "step:106, loss:36.06450271606445\n",
            "step:107, loss:62.262535095214844\n",
            "step:108, loss:23.580455780029297\n",
            "step:109, loss:273.15716552734375\n",
            "step:110, loss:228.58389282226562\n",
            "step:111, loss:622.8929443359375\n",
            "step:112, loss:74.54207611083984\n",
            "step:113, loss:231.29620361328125\n",
            "step:114, loss:218.09054565429688\n",
            "step:115, loss:32.0908088684082\n",
            "step:116, loss:76.9092025756836\n",
            "step:117, loss:2995.092529296875\n",
            "step:118, loss:66.20997619628906\n",
            "step:119, loss:132.0021209716797\n",
            "step:120, loss:12.889098167419434\n",
            "step:121, loss:75.02029418945312\n",
            "step:122, loss:48.385772705078125\n",
            "step:123, loss:305.0344543457031\n",
            "step:124, loss:44.288700103759766\n",
            "step:125, loss:206.2048797607422\n",
            "step:126, loss:95.85533142089844\n",
            "step:127, loss:18.039306640625\n",
            "step:128, loss:114.41765594482422\n",
            "step:129, loss:18.606698989868164\n",
            "step:130, loss:106.56038665771484\n",
            "step:131, loss:182.60287475585938\n",
            "step:132, loss:34.368404388427734\n",
            "step:133, loss:14.552221298217773\n",
            "step:134, loss:73.67780303955078\n",
            "step:135, loss:356.5464172363281\n",
            "step:136, loss:340.5338439941406\n",
            "step:137, loss:1389.501953125\n",
            "step:138, loss:114.59536743164062\n",
            "step:139, loss:47.40217590332031\n",
            "step:140, loss:37.609153747558594\n",
            "step:141, loss:41.626346588134766\n",
            "step:142, loss:218.36557006835938\n",
            "step:143, loss:47.071693420410156\n",
            "step:144, loss:89.90617370605469\n",
            "step:145, loss:32.01010513305664\n",
            "step:146, loss:65.9757080078125\n",
            "step:147, loss:215.16183471679688\n",
            "step:148, loss:39.01719284057617\n",
            "step:149, loss:171.78802490234375\n",
            "step:150, loss:225.07473754882812\n",
            "step:151, loss:65.12284851074219\n",
            "step:152, loss:44.14562225341797\n",
            "step:153, loss:22.74610137939453\n",
            "step:154, loss:1827.319580078125\n",
            "step:155, loss:552.556396484375\n",
            "step:156, loss:32.51416778564453\n",
            "step:157, loss:50.7859001159668\n",
            "step:158, loss:35.97483825683594\n",
            "step:159, loss:11.062044143676758\n",
            "step:160, loss:13.580024719238281\n",
            "step:161, loss:21.441360473632812\n",
            "step:162, loss:26.413619995117188\n",
            "step:163, loss:26.338178634643555\n",
            "step:164, loss:46.22602844238281\n",
            "step:165, loss:121.0110855102539\n",
            "step:166, loss:35.627967834472656\n",
            "step:167, loss:88.34095764160156\n",
            "step:168, loss:240.35157775878906\n",
            "step:169, loss:20.287181854248047\n",
            "step:170, loss:66.7613754272461\n",
            "step:171, loss:89.29197692871094\n",
            "step:172, loss:22.849163055419922\n",
            "step:173, loss:29.490768432617188\n",
            "step:174, loss:1057.0716552734375\n",
            "step:175, loss:144.85484313964844\n",
            "step:176, loss:158.9232940673828\n",
            "step:177, loss:22.72191619873047\n",
            "step:178, loss:38.93086242675781\n",
            "step:179, loss:109.66746520996094\n",
            "step:180, loss:530.7321166992188\n",
            "step:181, loss:956.3428955078125\n",
            "step:182, loss:227.9346923828125\n",
            "step:183, loss:275.09649658203125\n",
            "step:184, loss:214.07452392578125\n",
            "step:185, loss:122.13646697998047\n",
            "step:186, loss:17.25444984436035\n",
            "step:187, loss:88.19268035888672\n",
            "step:188, loss:96.93132019042969\n",
            "step:189, loss:148.42431640625\n",
            "step:190, loss:242.93397521972656\n",
            "step:191, loss:200.42686462402344\n",
            "step:192, loss:39.3438720703125\n",
            "step:193, loss:316.3615417480469\n",
            "step:194, loss:1974.02490234375\n",
            "step:195, loss:69.3373794555664\n",
            "step:196, loss:17.01811981201172\n",
            "step:197, loss:184.13031005859375\n",
            "step:198, loss:98.93962097167969\n",
            "step:199, loss:346.29595947265625\n",
            "step:200, loss:505.88006591796875\n",
            "step:201, loss:513.92236328125\n",
            "step:202, loss:370.99505615234375\n",
            "7\n",
            "-----\n",
            "step:0, loss:48.75098419189453\n",
            "step:1, loss:84.76533508300781\n",
            "step:2, loss:11.002264022827148\n",
            "step:3, loss:278.050537109375\n",
            "step:4, loss:31.4068603515625\n",
            "step:5, loss:121.70316314697266\n",
            "step:6, loss:130.98129272460938\n",
            "step:7, loss:62.852989196777344\n",
            "step:8, loss:95.1078109741211\n",
            "step:9, loss:2993.76416015625\n",
            "step:10, loss:20.80579948425293\n",
            "step:11, loss:32.9190673828125\n",
            "step:12, loss:32.86598587036133\n",
            "step:13, loss:169.8597412109375\n",
            "step:14, loss:54.8006591796875\n",
            "step:15, loss:49.975563049316406\n",
            "step:16, loss:1866.714599609375\n",
            "step:17, loss:25.812328338623047\n",
            "step:18, loss:42.47074890136719\n",
            "step:19, loss:39.38666534423828\n",
            "step:20, loss:212.972412109375\n",
            "step:21, loss:96.81765747070312\n",
            "step:22, loss:26.235580444335938\n",
            "step:23, loss:960.6016845703125\n",
            "step:24, loss:16.441143035888672\n",
            "step:25, loss:867.990478515625\n",
            "step:26, loss:24.06082534790039\n",
            "step:27, loss:26.462932586669922\n",
            "step:28, loss:59.81938171386719\n",
            "step:29, loss:8.379061698913574\n",
            "step:30, loss:22.356388092041016\n",
            "step:31, loss:144.4915771484375\n",
            "step:32, loss:64.38220977783203\n",
            "step:33, loss:61.78399658203125\n",
            "step:34, loss:41.24329376220703\n",
            "step:35, loss:266.9624938964844\n",
            "step:36, loss:25.96169090270996\n",
            "step:37, loss:36.52144241333008\n",
            "step:38, loss:60.07971954345703\n",
            "step:39, loss:305.67852783203125\n",
            "step:40, loss:205.99853515625\n",
            "step:41, loss:36.518558502197266\n",
            "step:42, loss:897.3590698242188\n",
            "step:43, loss:9.218005180358887\n",
            "step:44, loss:235.3443145751953\n",
            "step:45, loss:126.26019287109375\n",
            "step:46, loss:114.39254760742188\n",
            "step:47, loss:84.74986267089844\n",
            "step:48, loss:15.571832656860352\n",
            "step:49, loss:42.079261779785156\n",
            "step:50, loss:29.0506649017334\n",
            "step:51, loss:224.3062286376953\n",
            "step:52, loss:213.1168212890625\n",
            "step:53, loss:51.721160888671875\n",
            "step:54, loss:130.5679931640625\n",
            "step:55, loss:260.0693359375\n",
            "step:56, loss:393.8602294921875\n",
            "step:57, loss:11.754737854003906\n",
            "step:58, loss:121.78357696533203\n",
            "step:59, loss:76.44908142089844\n",
            "step:60, loss:17.066858291625977\n",
            "step:61, loss:304.64251708984375\n",
            "step:62, loss:94.5332260131836\n",
            "step:63, loss:1951.0509033203125\n",
            "step:64, loss:57.49486541748047\n",
            "step:65, loss:79.3887939453125\n",
            "step:66, loss:111.12319946289062\n",
            "step:67, loss:166.123046875\n",
            "step:68, loss:441.4890441894531\n",
            "step:69, loss:38.731658935546875\n",
            "step:70, loss:144.00180053710938\n",
            "step:71, loss:46.068363189697266\n",
            "step:72, loss:46.19541931152344\n",
            "step:73, loss:56.54942321777344\n",
            "step:74, loss:76.65864562988281\n",
            "step:75, loss:1024.1470947265625\n",
            "step:76, loss:65.93359375\n",
            "step:77, loss:114.58071899414062\n",
            "step:78, loss:172.95530700683594\n",
            "step:79, loss:21.795066833496094\n",
            "step:80, loss:34.43693542480469\n",
            "step:81, loss:158.3828887939453\n",
            "step:82, loss:45.497467041015625\n",
            "step:83, loss:454.063720703125\n",
            "step:84, loss:60.04945755004883\n",
            "step:85, loss:685.2827758789062\n",
            "step:86, loss:91.66510772705078\n",
            "step:87, loss:79.06295013427734\n",
            "step:88, loss:13.426085472106934\n",
            "step:89, loss:197.23388671875\n",
            "step:90, loss:82.02265167236328\n",
            "step:91, loss:531.9610595703125\n",
            "step:92, loss:10.138773918151855\n",
            "step:93, loss:32.266212463378906\n",
            "step:94, loss:314.7206726074219\n",
            "step:95, loss:54.92240905761719\n",
            "step:96, loss:63.867431640625\n",
            "step:97, loss:120.66255187988281\n",
            "step:98, loss:1155.3046875\n",
            "step:99, loss:99.78441619873047\n",
            "step:100, loss:67.193603515625\n",
            "step:101, loss:46.86857986450195\n",
            "step:102, loss:94.7122802734375\n",
            "step:103, loss:102.71388244628906\n",
            "step:104, loss:31.516376495361328\n",
            "step:105, loss:147.91481018066406\n",
            "step:106, loss:208.91555786132812\n",
            "step:107, loss:897.578125\n",
            "step:108, loss:29.920047760009766\n",
            "step:109, loss:594.951416015625\n",
            "step:110, loss:133.1647186279297\n",
            "step:111, loss:38.72933578491211\n",
            "step:112, loss:49.60436248779297\n",
            "step:113, loss:44.25434494018555\n",
            "step:114, loss:15.102508544921875\n",
            "step:115, loss:44.279380798339844\n",
            "step:116, loss:36.726654052734375\n",
            "step:117, loss:424.7030944824219\n",
            "step:118, loss:55.739044189453125\n",
            "step:119, loss:19.553617477416992\n",
            "step:120, loss:63.40009689331055\n",
            "step:121, loss:7939.0869140625\n",
            "step:122, loss:79.66572570800781\n",
            "step:123, loss:56.61328125\n",
            "step:124, loss:165.7528839111328\n",
            "step:125, loss:127.50837707519531\n",
            "step:126, loss:93.57211303710938\n",
            "step:127, loss:52.902122497558594\n",
            "step:128, loss:219.788818359375\n",
            "step:129, loss:233.40963745117188\n",
            "step:130, loss:9.25533676147461\n",
            "step:131, loss:241.15866088867188\n",
            "step:132, loss:52.814208984375\n",
            "step:133, loss:72.3551254272461\n",
            "step:134, loss:244.08555603027344\n",
            "step:135, loss:85.61637878417969\n",
            "step:136, loss:14.542550086975098\n",
            "step:137, loss:46.73991012573242\n",
            "step:138, loss:43.67725372314453\n",
            "step:139, loss:53.62196731567383\n",
            "step:140, loss:907.0858154296875\n",
            "step:141, loss:87.90615844726562\n",
            "step:142, loss:329.49102783203125\n",
            "step:143, loss:28.84958267211914\n",
            "step:144, loss:50.079097747802734\n",
            "step:145, loss:51.286685943603516\n",
            "step:146, loss:1372.21142578125\n",
            "step:147, loss:74.04217529296875\n",
            "step:148, loss:128.7649383544922\n",
            "step:149, loss:56.11130142211914\n",
            "step:150, loss:4983.587890625\n",
            "step:151, loss:75.95790100097656\n",
            "step:152, loss:272.68402099609375\n",
            "step:153, loss:238.82826232910156\n",
            "step:154, loss:69.88496398925781\n",
            "step:155, loss:158.740478515625\n",
            "step:156, loss:261.794921875\n",
            "step:157, loss:70.82109832763672\n",
            "step:158, loss:48.56958770751953\n",
            "step:159, loss:84.00640869140625\n",
            "step:160, loss:11.819941520690918\n",
            "step:161, loss:78.2408447265625\n",
            "step:162, loss:325.79840087890625\n",
            "step:163, loss:7779.97607421875\n",
            "step:164, loss:110.15526580810547\n",
            "step:165, loss:8.854904174804688\n",
            "step:166, loss:83.68829345703125\n",
            "step:167, loss:329.0513610839844\n",
            "step:168, loss:60.672889709472656\n",
            "step:169, loss:43.1335563659668\n",
            "step:170, loss:22.49640655517578\n",
            "step:171, loss:58.10784912109375\n",
            "step:172, loss:58.67927169799805\n",
            "step:173, loss:68.2177734375\n",
            "step:174, loss:9.337593078613281\n",
            "step:175, loss:164.3411865234375\n",
            "step:176, loss:60.45294189453125\n",
            "step:177, loss:34.50740432739258\n",
            "step:178, loss:268.9297180175781\n",
            "step:179, loss:464.03753662109375\n",
            "step:180, loss:45.444942474365234\n",
            "step:181, loss:51.46589279174805\n",
            "step:182, loss:82.87652587890625\n",
            "step:183, loss:45.01559829711914\n",
            "step:184, loss:316.2567138671875\n",
            "step:185, loss:69.5050048828125\n",
            "step:186, loss:462.829833984375\n",
            "step:187, loss:274.76336669921875\n",
            "step:188, loss:40.521671295166016\n",
            "step:189, loss:174.0857391357422\n",
            "step:190, loss:160.8434295654297\n",
            "step:191, loss:271.84503173828125\n",
            "step:192, loss:13.835206031799316\n",
            "step:193, loss:28.271387100219727\n",
            "step:194, loss:12851.2021484375\n",
            "step:195, loss:67.0423583984375\n",
            "step:196, loss:366.2315979003906\n",
            "step:197, loss:129.62582397460938\n",
            "step:198, loss:396.278076171875\n",
            "step:199, loss:821.8148193359375\n",
            "step:200, loss:101.90140533447266\n",
            "step:201, loss:36.20671463012695\n",
            "step:202, loss:287.730712890625\n",
            "8\n",
            "-----\n",
            "step:0, loss:61.062843322753906\n",
            "step:1, loss:111.10943603515625\n",
            "step:2, loss:130.1768035888672\n",
            "step:3, loss:65.70890045166016\n",
            "step:4, loss:81.01039123535156\n",
            "step:5, loss:35.470916748046875\n",
            "step:6, loss:849.5380859375\n",
            "step:7, loss:36.976993560791016\n",
            "step:8, loss:94.43488311767578\n",
            "step:9, loss:34.07984924316406\n",
            "step:10, loss:109.92713928222656\n",
            "step:11, loss:22.4566707611084\n",
            "step:12, loss:201.48477172851562\n",
            "step:13, loss:77.30033874511719\n",
            "step:14, loss:17.29730987548828\n",
            "step:15, loss:43.178653717041016\n",
            "step:16, loss:13.05783462524414\n",
            "step:17, loss:182.384033203125\n",
            "step:18, loss:124.06964874267578\n",
            "step:19, loss:44.74229431152344\n",
            "step:20, loss:27.02035903930664\n",
            "step:21, loss:164.59608459472656\n",
            "step:22, loss:6.957718849182129\n",
            "step:23, loss:12.902713775634766\n",
            "step:24, loss:74.71517944335938\n",
            "step:25, loss:102.6162109375\n",
            "step:26, loss:60.63972473144531\n",
            "step:27, loss:602.547607421875\n",
            "step:28, loss:971.2791748046875\n",
            "step:29, loss:63.786136627197266\n",
            "step:30, loss:87.59295654296875\n",
            "step:31, loss:36.71682357788086\n",
            "step:32, loss:28.938159942626953\n",
            "step:33, loss:60.70905685424805\n",
            "step:34, loss:731.15185546875\n",
            "step:35, loss:260.9585266113281\n",
            "step:36, loss:54.694557189941406\n",
            "step:37, loss:18.4123592376709\n",
            "step:38, loss:324.6260681152344\n",
            "step:39, loss:62.89722442626953\n",
            "step:40, loss:13182.6337890625\n",
            "step:41, loss:23.32474136352539\n",
            "step:42, loss:78.34857177734375\n",
            "step:43, loss:25.368017196655273\n",
            "step:44, loss:34.20043182373047\n",
            "step:45, loss:75.18863677978516\n",
            "step:46, loss:246.13003540039062\n",
            "step:47, loss:18.96603012084961\n",
            "step:48, loss:77.88172912597656\n",
            "step:49, loss:232.74697875976562\n",
            "step:50, loss:14.064294815063477\n",
            "step:51, loss:535.2947387695312\n",
            "step:52, loss:89.81036376953125\n",
            "step:53, loss:74.06610870361328\n",
            "step:54, loss:4891.4658203125\n",
            "step:55, loss:152.80264282226562\n",
            "step:56, loss:121.41128540039062\n",
            "step:57, loss:7.271688461303711\n",
            "step:58, loss:25.190677642822266\n",
            "step:59, loss:18.054336547851562\n",
            "step:60, loss:48.937782287597656\n",
            "step:61, loss:507.91241455078125\n",
            "step:62, loss:418.8266296386719\n",
            "step:63, loss:113.56198120117188\n",
            "step:64, loss:73.4683837890625\n",
            "step:65, loss:47.391578674316406\n",
            "step:66, loss:866.3627319335938\n",
            "step:67, loss:13.578583717346191\n",
            "step:68, loss:19.50666046142578\n",
            "step:69, loss:38.83595657348633\n",
            "step:70, loss:1057.511474609375\n",
            "step:71, loss:32.60990524291992\n",
            "step:72, loss:7.893381118774414\n",
            "step:73, loss:113.84725189208984\n",
            "step:74, loss:64.98143005371094\n",
            "step:75, loss:81.56889343261719\n",
            "step:76, loss:29.873069763183594\n",
            "step:77, loss:1786.558349609375\n",
            "step:78, loss:21.516231536865234\n",
            "step:79, loss:91.79853057861328\n",
            "step:80, loss:348.1741943359375\n",
            "step:81, loss:38.808860778808594\n",
            "step:82, loss:228.59030151367188\n",
            "step:83, loss:275.8445129394531\n",
            "step:84, loss:127.13751220703125\n",
            "step:85, loss:104.45262145996094\n",
            "step:86, loss:1991.632080078125\n",
            "step:87, loss:48.732967376708984\n",
            "step:88, loss:17.088401794433594\n",
            "step:89, loss:3033.849609375\n",
            "step:90, loss:188.77334594726562\n",
            "step:91, loss:836.4671630859375\n",
            "step:92, loss:12.871862411499023\n",
            "step:93, loss:276.453125\n",
            "step:94, loss:25.145511627197266\n",
            "step:95, loss:184.24594116210938\n",
            "step:96, loss:234.95013427734375\n",
            "step:97, loss:177.7546844482422\n",
            "step:98, loss:65.78868103027344\n",
            "step:99, loss:29.721057891845703\n",
            "step:100, loss:66.0932388305664\n",
            "step:101, loss:228.19696044921875\n",
            "step:102, loss:24.000530242919922\n",
            "step:103, loss:1313.0897216796875\n",
            "step:104, loss:120.31507110595703\n",
            "step:105, loss:17.623611450195312\n",
            "step:106, loss:190.95095825195312\n",
            "step:107, loss:667.3627319335938\n",
            "step:108, loss:27.597095489501953\n",
            "step:109, loss:283.539306640625\n",
            "step:110, loss:878.8574829101562\n",
            "step:111, loss:74.41401672363281\n",
            "step:112, loss:393.6790466308594\n",
            "step:113, loss:31.199838638305664\n",
            "step:114, loss:60.197410583496094\n",
            "step:115, loss:562.755615234375\n",
            "step:116, loss:205.3060302734375\n",
            "step:117, loss:21.535846710205078\n",
            "step:118, loss:58.59071350097656\n",
            "step:119, loss:59.565391540527344\n",
            "step:120, loss:22.95914077758789\n",
            "step:121, loss:101.79549407958984\n",
            "step:122, loss:213.9104461669922\n",
            "step:123, loss:69.9852294921875\n",
            "step:124, loss:8.319743156433105\n",
            "step:125, loss:15.601858139038086\n",
            "step:126, loss:51.80375289916992\n",
            "step:127, loss:58.878944396972656\n",
            "step:128, loss:42.89399719238281\n",
            "step:129, loss:38.07075500488281\n",
            "step:130, loss:56.35710144042969\n",
            "step:131, loss:59.62892532348633\n",
            "step:132, loss:449.16497802734375\n",
            "step:133, loss:188.9468231201172\n",
            "step:134, loss:131.51351928710938\n",
            "step:135, loss:230.12753295898438\n",
            "step:136, loss:187.33819580078125\n",
            "step:137, loss:119.86090087890625\n",
            "step:138, loss:40.53765869140625\n",
            "step:139, loss:33.72732925415039\n",
            "step:140, loss:35.409645080566406\n",
            "step:141, loss:95.17147827148438\n",
            "step:142, loss:24.28413200378418\n",
            "step:143, loss:370.593505859375\n",
            "step:144, loss:20.115781784057617\n",
            "step:145, loss:362.314453125\n",
            "step:146, loss:15.599328994750977\n",
            "step:147, loss:73.8870620727539\n",
            "step:148, loss:33.84660720825195\n",
            "step:149, loss:407.7508239746094\n",
            "step:150, loss:28.876768112182617\n",
            "step:151, loss:43.08505630493164\n",
            "step:152, loss:34.17196273803711\n",
            "step:153, loss:294.19854736328125\n",
            "step:154, loss:28.825637817382812\n",
            "step:155, loss:16.91303825378418\n",
            "step:156, loss:119.02288818359375\n",
            "step:157, loss:8.424501419067383\n",
            "step:158, loss:95.52863311767578\n",
            "step:159, loss:484.0625\n",
            "step:160, loss:15.128438949584961\n",
            "step:161, loss:53.168006896972656\n",
            "step:162, loss:84.91466522216797\n",
            "step:163, loss:402.4718933105469\n",
            "step:164, loss:43.49396514892578\n",
            "step:165, loss:8184.42724609375\n",
            "step:166, loss:177.5714569091797\n",
            "step:167, loss:202.9520721435547\n",
            "step:168, loss:20.625965118408203\n",
            "step:169, loss:574.6588134765625\n",
            "step:170, loss:42.408504486083984\n",
            "step:171, loss:16.65055274963379\n",
            "step:172, loss:278.5717468261719\n",
            "step:173, loss:48.61227798461914\n",
            "step:174, loss:56.73128128051758\n",
            "step:175, loss:110.1999282836914\n",
            "step:176, loss:57.176509857177734\n",
            "step:177, loss:7854.87451171875\n",
            "step:178, loss:327.8676452636719\n",
            "step:179, loss:65.44039916992188\n",
            "step:180, loss:110.81010437011719\n",
            "step:181, loss:72.55296325683594\n",
            "step:182, loss:223.90553283691406\n",
            "step:183, loss:32.69821548461914\n",
            "step:184, loss:23.60919189453125\n",
            "step:185, loss:98.28228759765625\n",
            "step:186, loss:25.771778106689453\n",
            "step:187, loss:25.159198760986328\n",
            "step:188, loss:44.22767639160156\n",
            "step:189, loss:16.757219314575195\n",
            "step:190, loss:36.02439880371094\n",
            "step:191, loss:46.01475524902344\n",
            "step:192, loss:23.726713180541992\n",
            "step:193, loss:32.415321350097656\n",
            "step:194, loss:46.13780212402344\n",
            "step:195, loss:92.95768737792969\n",
            "step:196, loss:113.99887084960938\n",
            "step:197, loss:268.7272644042969\n",
            "step:198, loss:59.50486373901367\n",
            "step:199, loss:54.675621032714844\n",
            "step:200, loss:23.704748153686523\n",
            "step:201, loss:23.519609451293945\n",
            "step:202, loss:26.33074188232422\n",
            "9\n",
            "-----\n",
            "step:0, loss:295.20147705078125\n",
            "step:1, loss:1791.572509765625\n",
            "step:2, loss:59.00868606567383\n",
            "step:3, loss:448.6408386230469\n",
            "step:4, loss:28.60994529724121\n",
            "step:5, loss:27.321474075317383\n",
            "step:6, loss:11.679306030273438\n",
            "step:7, loss:6.891395568847656\n",
            "step:8, loss:83.49126434326172\n",
            "step:9, loss:103.97941589355469\n",
            "step:10, loss:23.680822372436523\n",
            "step:11, loss:101.17894744873047\n",
            "step:12, loss:56.71013641357422\n",
            "step:13, loss:224.9365234375\n",
            "step:14, loss:76.97681427001953\n",
            "step:15, loss:9.317658424377441\n",
            "step:16, loss:83.7875747680664\n",
            "step:17, loss:13.462348937988281\n",
            "step:18, loss:4738.373046875\n",
            "step:19, loss:49.353111267089844\n",
            "step:20, loss:117.7452621459961\n",
            "step:21, loss:16.190940856933594\n",
            "step:22, loss:66.94376373291016\n",
            "step:23, loss:23.899564743041992\n",
            "step:24, loss:73.2501220703125\n",
            "step:25, loss:54.581764221191406\n",
            "step:26, loss:28.101276397705078\n",
            "step:27, loss:11.77737808227539\n",
            "step:28, loss:274.36297607421875\n",
            "step:29, loss:92.87044525146484\n",
            "step:30, loss:33.792781829833984\n",
            "step:31, loss:25.20050048828125\n",
            "step:32, loss:102.4743881225586\n",
            "step:33, loss:11.045039176940918\n",
            "step:34, loss:87.4610366821289\n",
            "step:35, loss:555.69482421875\n",
            "step:36, loss:74.53236389160156\n",
            "step:37, loss:217.49575805664062\n",
            "step:38, loss:993.2318115234375\n",
            "step:39, loss:21.41153335571289\n",
            "step:40, loss:181.52981567382812\n",
            "step:41, loss:173.36790466308594\n",
            "step:42, loss:199.01023864746094\n",
            "step:43, loss:46.22953414916992\n",
            "step:44, loss:192.90631103515625\n",
            "step:45, loss:68.26346588134766\n",
            "step:46, loss:204.6158447265625\n",
            "step:47, loss:85.72217559814453\n",
            "step:48, loss:207.8502197265625\n",
            "step:49, loss:50.0010986328125\n",
            "step:50, loss:64.13318634033203\n",
            "step:51, loss:681.0899047851562\n",
            "step:52, loss:313.89434814453125\n",
            "step:53, loss:21.97342872619629\n",
            "step:54, loss:7963.7314453125\n",
            "step:55, loss:67.21282958984375\n",
            "step:56, loss:10.445131301879883\n",
            "step:57, loss:112.94117736816406\n",
            "step:58, loss:105.0357666015625\n",
            "step:59, loss:12.849287986755371\n",
            "step:60, loss:13.679100036621094\n",
            "step:61, loss:132.05581665039062\n",
            "step:62, loss:49.81725311279297\n",
            "step:63, loss:1338.5386962890625\n",
            "step:64, loss:67.6697998046875\n",
            "step:65, loss:27.757102966308594\n",
            "step:66, loss:23.39590072631836\n",
            "step:67, loss:994.5377197265625\n",
            "step:68, loss:38.14961624145508\n",
            "step:69, loss:26.213573455810547\n",
            "step:70, loss:40.993412017822266\n",
            "step:71, loss:20.747421264648438\n",
            "step:72, loss:18.539939880371094\n",
            "step:73, loss:39.0587158203125\n",
            "step:74, loss:20.849658966064453\n",
            "step:75, loss:44.184993743896484\n",
            "step:76, loss:79.53323364257812\n",
            "step:77, loss:974.536865234375\n",
            "step:78, loss:411.08880615234375\n",
            "step:79, loss:11.182847023010254\n",
            "step:80, loss:333.9308166503906\n",
            "step:81, loss:81.55436706542969\n",
            "step:82, loss:147.96484375\n",
            "step:83, loss:2915.741943359375\n",
            "step:84, loss:48.64085388183594\n",
            "step:85, loss:25.67158317565918\n",
            "step:86, loss:144.8481903076172\n",
            "step:87, loss:18.97258949279785\n",
            "step:88, loss:118.43045043945312\n",
            "step:89, loss:28.272621154785156\n",
            "step:90, loss:84.17389678955078\n",
            "step:91, loss:18.392337799072266\n",
            "step:92, loss:43.253787994384766\n",
            "step:93, loss:244.0896453857422\n",
            "step:94, loss:3.479191780090332\n",
            "step:95, loss:105.01133728027344\n",
            "step:96, loss:267.3944396972656\n",
            "step:97, loss:181.4989776611328\n",
            "step:98, loss:71.1200942993164\n",
            "step:99, loss:200.48324584960938\n",
            "step:100, loss:21.871309280395508\n",
            "step:101, loss:69.52888488769531\n",
            "step:102, loss:288.02056884765625\n",
            "step:103, loss:84.92108154296875\n",
            "step:104, loss:64.28973388671875\n",
            "step:105, loss:131.36981201171875\n",
            "step:106, loss:101.17711639404297\n",
            "step:107, loss:150.61512756347656\n",
            "step:108, loss:51.29472351074219\n",
            "step:109, loss:19.69521713256836\n",
            "step:110, loss:65.21931457519531\n",
            "step:111, loss:39.507240295410156\n",
            "step:112, loss:47.253990173339844\n",
            "step:113, loss:43.229766845703125\n",
            "step:114, loss:196.8975067138672\n",
            "step:115, loss:450.6347351074219\n",
            "step:116, loss:15.713728904724121\n",
            "step:117, loss:30.047706604003906\n",
            "step:118, loss:22.25745964050293\n",
            "step:119, loss:33.2391471862793\n",
            "step:120, loss:64.94430541992188\n",
            "step:121, loss:17.04299545288086\n",
            "step:122, loss:16.194427490234375\n",
            "step:123, loss:54.6949462890625\n",
            "step:124, loss:837.635498046875\n",
            "step:125, loss:459.03009033203125\n",
            "step:126, loss:42.29469299316406\n",
            "step:127, loss:28.506526947021484\n",
            "step:128, loss:72.2645263671875\n",
            "step:129, loss:592.032958984375\n",
            "step:130, loss:49.612266540527344\n",
            "step:131, loss:186.28392028808594\n",
            "step:132, loss:5.598592758178711\n",
            "step:133, loss:217.83441162109375\n",
            "step:134, loss:189.71649169921875\n",
            "step:135, loss:90.23320770263672\n",
            "step:136, loss:62.65364456176758\n",
            "step:137, loss:11.502769470214844\n",
            "step:138, loss:19.00889778137207\n",
            "step:139, loss:14.170248031616211\n",
            "step:140, loss:31.03458595275879\n",
            "step:141, loss:97.28395080566406\n",
            "step:142, loss:296.83306884765625\n",
            "step:143, loss:127.56859588623047\n",
            "step:144, loss:60.59612274169922\n",
            "step:145, loss:94.76348876953125\n",
            "step:146, loss:7673.22265625\n",
            "step:147, loss:23.982379913330078\n",
            "step:148, loss:45.34221267700195\n",
            "step:149, loss:66.41220092773438\n",
            "step:150, loss:31.554140090942383\n",
            "step:151, loss:86.71435546875\n",
            "step:152, loss:11.003000259399414\n",
            "step:153, loss:68.80084991455078\n",
            "step:154, loss:217.3203125\n",
            "step:155, loss:456.6849670410156\n",
            "step:156, loss:506.0050964355469\n",
            "step:157, loss:14.349187850952148\n",
            "step:158, loss:39.02169418334961\n",
            "step:159, loss:170.05001831054688\n",
            "step:160, loss:12.203121185302734\n",
            "step:161, loss:240.5899658203125\n",
            "step:162, loss:180.72378540039062\n",
            "step:163, loss:202.66470336914062\n",
            "step:164, loss:333.2583923339844\n",
            "step:165, loss:26.433855056762695\n",
            "step:166, loss:294.8414611816406\n",
            "step:167, loss:98.4007568359375\n",
            "step:168, loss:866.4877319335938\n",
            "step:169, loss:50.64756774902344\n",
            "step:170, loss:335.06280517578125\n",
            "step:171, loss:56.33469772338867\n",
            "step:172, loss:23.04589080810547\n",
            "step:173, loss:12.303630828857422\n",
            "step:174, loss:20.014949798583984\n",
            "step:175, loss:110.37144470214844\n",
            "step:176, loss:1876.7724609375\n",
            "step:177, loss:24.370803833007812\n",
            "step:178, loss:76.03184509277344\n",
            "step:179, loss:17.495738983154297\n",
            "step:180, loss:116.20747375488281\n",
            "step:181, loss:239.65977478027344\n",
            "step:182, loss:27.760046005249023\n",
            "step:183, loss:301.3514099121094\n",
            "step:184, loss:95.61896514892578\n",
            "step:185, loss:98.7436294555664\n",
            "step:186, loss:14.556280136108398\n",
            "step:187, loss:21.062156677246094\n",
            "step:188, loss:173.79669189453125\n",
            "step:189, loss:12658.4228515625\n",
            "step:190, loss:46.746978759765625\n",
            "step:191, loss:274.0787658691406\n",
            "step:192, loss:33.90093231201172\n",
            "step:193, loss:60.39347839355469\n",
            "step:194, loss:96.71273040771484\n",
            "step:195, loss:19.179885864257812\n",
            "step:196, loss:50.79906463623047\n",
            "step:197, loss:176.11529541015625\n",
            "step:198, loss:14.237439155578613\n",
            "step:199, loss:915.092529296875\n",
            "step:200, loss:396.1549987792969\n",
            "step:201, loss:141.05429077148438\n",
            "step:202, loss:36.91815948486328\n",
            "10\n",
            "-----\n",
            "step:0, loss:23.59197998046875\n",
            "step:1, loss:34.9153938293457\n",
            "step:2, loss:24.53871726989746\n",
            "step:3, loss:40.405242919921875\n",
            "step:4, loss:102.42459869384766\n",
            "step:5, loss:9.082948684692383\n",
            "step:6, loss:156.61911010742188\n",
            "step:7, loss:129.91807556152344\n",
            "step:8, loss:22.698156356811523\n",
            "step:9, loss:60.00196838378906\n",
            "step:10, loss:171.96389770507812\n",
            "step:11, loss:402.04986572265625\n",
            "step:12, loss:31.95345687866211\n",
            "step:13, loss:594.0069580078125\n",
            "step:14, loss:109.59738159179688\n",
            "step:15, loss:420.40850830078125\n",
            "step:16, loss:114.54240417480469\n",
            "step:17, loss:374.153076171875\n",
            "step:18, loss:474.1797180175781\n",
            "step:19, loss:2255.065185546875\n",
            "step:20, loss:809.8286743164062\n",
            "step:21, loss:9.973529815673828\n",
            "step:22, loss:358.7339172363281\n",
            "step:23, loss:140.70755004882812\n",
            "step:24, loss:20.67223358154297\n",
            "step:25, loss:144.35923767089844\n",
            "step:26, loss:168.99205017089844\n",
            "step:27, loss:60.0718879699707\n",
            "step:28, loss:55.595558166503906\n",
            "step:29, loss:155.55133056640625\n",
            "step:30, loss:63.97160720825195\n",
            "step:31, loss:8.167156219482422\n",
            "step:32, loss:16.766443252563477\n",
            "step:33, loss:14.179677963256836\n",
            "step:34, loss:389.2692565917969\n",
            "step:35, loss:4910.3671875\n",
            "step:36, loss:41.37787628173828\n",
            "step:37, loss:31.64200210571289\n",
            "step:38, loss:15.212109565734863\n",
            "step:39, loss:9.757782936096191\n",
            "step:40, loss:288.67144775390625\n",
            "step:41, loss:148.84121704101562\n",
            "step:42, loss:114.52647399902344\n",
            "step:43, loss:94.42060852050781\n",
            "step:44, loss:16.9910831451416\n",
            "step:45, loss:89.39940643310547\n",
            "step:46, loss:33.90492630004883\n",
            "step:47, loss:1007.4520874023438\n",
            "step:48, loss:8.767290115356445\n",
            "step:49, loss:69.06611633300781\n",
            "step:50, loss:259.2292175292969\n",
            "step:51, loss:60.76021194458008\n",
            "step:52, loss:47.21369171142578\n",
            "step:53, loss:68.09504699707031\n",
            "step:54, loss:283.5480041503906\n",
            "step:55, loss:59.7943115234375\n",
            "step:56, loss:371.35308837890625\n",
            "step:57, loss:27.25590705871582\n",
            "step:58, loss:913.1737670898438\n",
            "step:59, loss:15.141644477844238\n",
            "step:60, loss:10.700302124023438\n",
            "step:61, loss:16.56998062133789\n",
            "step:62, loss:8.228605270385742\n",
            "step:63, loss:113.20069122314453\n",
            "step:64, loss:28.0192813873291\n",
            "step:65, loss:46.379364013671875\n",
            "step:66, loss:34.829410552978516\n",
            "step:67, loss:35.838844299316406\n",
            "step:68, loss:68.3821792602539\n",
            "step:69, loss:244.32981872558594\n",
            "step:70, loss:254.2876739501953\n",
            "step:71, loss:69.39228820800781\n",
            "step:72, loss:101.1067886352539\n",
            "step:73, loss:35.991424560546875\n",
            "step:74, loss:15.245023727416992\n",
            "step:75, loss:11.973543167114258\n",
            "step:76, loss:651.9503784179688\n",
            "step:77, loss:37.88942337036133\n",
            "step:78, loss:51.353660583496094\n",
            "step:79, loss:73.54722595214844\n",
            "step:80, loss:76.30081939697266\n",
            "step:81, loss:256.9361572265625\n",
            "step:82, loss:106.70246887207031\n",
            "step:83, loss:49.561859130859375\n",
            "step:84, loss:7617.705078125\n",
            "step:85, loss:21.424850463867188\n",
            "step:86, loss:17.93462562561035\n",
            "step:87, loss:11.697460174560547\n",
            "step:88, loss:24.329408645629883\n",
            "step:89, loss:101.93953704833984\n",
            "step:90, loss:87.34795379638672\n",
            "step:91, loss:40.81439971923828\n",
            "step:92, loss:97.3919677734375\n",
            "step:93, loss:73.71924591064453\n",
            "step:94, loss:34.44108581542969\n",
            "step:95, loss:27.38788604736328\n",
            "step:96, loss:100.23001861572266\n",
            "step:97, loss:25.463741302490234\n",
            "step:98, loss:10.406198501586914\n",
            "step:99, loss:198.2156982421875\n",
            "step:100, loss:286.1518859863281\n",
            "step:101, loss:114.09934997558594\n",
            "step:102, loss:534.9498291015625\n",
            "step:103, loss:22.673439025878906\n",
            "step:104, loss:20.613208770751953\n",
            "step:105, loss:24.716583251953125\n",
            "step:106, loss:82.69468688964844\n",
            "step:107, loss:165.70425415039062\n",
            "step:108, loss:22.19809341430664\n",
            "step:109, loss:121.01319122314453\n",
            "step:110, loss:9.860983848571777\n",
            "step:111, loss:367.5483703613281\n",
            "step:112, loss:49.531497955322266\n",
            "step:113, loss:31.045875549316406\n",
            "step:114, loss:259.5094299316406\n",
            "step:115, loss:48.1486701965332\n",
            "step:116, loss:16.19192123413086\n",
            "step:117, loss:58.57158660888672\n",
            "step:118, loss:16.658355712890625\n",
            "step:119, loss:10.306955337524414\n",
            "step:120, loss:69.0062255859375\n",
            "step:121, loss:57.84623336791992\n",
            "step:122, loss:61.520172119140625\n",
            "step:123, loss:48.769744873046875\n",
            "step:124, loss:19.825061798095703\n",
            "step:125, loss:20.40442657470703\n",
            "step:126, loss:15.035137176513672\n",
            "step:127, loss:45.97052764892578\n",
            "step:128, loss:31.70570945739746\n",
            "step:129, loss:23.188770294189453\n",
            "step:130, loss:31.04875373840332\n",
            "step:131, loss:9.190109252929688\n",
            "step:132, loss:81.85002899169922\n",
            "step:133, loss:45.970211029052734\n",
            "step:134, loss:397.25421142578125\n",
            "step:135, loss:17.241498947143555\n",
            "step:136, loss:23.919116973876953\n",
            "step:137, loss:24.654172897338867\n",
            "step:138, loss:11.215570449829102\n",
            "step:139, loss:324.5262451171875\n",
            "step:140, loss:9257.0048828125\n",
            "step:141, loss:73.12715148925781\n",
            "step:142, loss:29.63428497314453\n",
            "step:143, loss:13.830150604248047\n",
            "step:144, loss:55.47440719604492\n",
            "step:145, loss:188.94561767578125\n",
            "step:146, loss:87.0901107788086\n",
            "step:147, loss:113.51693725585938\n",
            "step:148, loss:68.1214828491211\n",
            "step:149, loss:1720.2481689453125\n",
            "step:150, loss:11.905967712402344\n",
            "step:151, loss:272.943603515625\n",
            "step:152, loss:26.08291244506836\n",
            "step:153, loss:49.27665328979492\n",
            "step:154, loss:52.82802963256836\n",
            "step:155, loss:11.530709266662598\n",
            "step:156, loss:382.91094970703125\n",
            "step:157, loss:40.977500915527344\n",
            "step:158, loss:33.30653381347656\n",
            "step:159, loss:9.50204849243164\n",
            "step:160, loss:161.01199340820312\n",
            "step:161, loss:17.84619903564453\n",
            "step:162, loss:3284.32421875\n",
            "step:163, loss:330.1531982421875\n",
            "step:164, loss:177.06146240234375\n",
            "step:165, loss:43.946380615234375\n",
            "step:166, loss:86.82879638671875\n",
            "step:167, loss:849.4404296875\n",
            "step:168, loss:129.0290069580078\n",
            "step:169, loss:83.73257446289062\n",
            "step:170, loss:19.900527954101562\n",
            "step:171, loss:71.18157196044922\n",
            "step:172, loss:27.983640670776367\n",
            "step:173, loss:76.99738311767578\n",
            "step:174, loss:68.7139663696289\n",
            "step:175, loss:36.11602020263672\n",
            "step:176, loss:52.18655014038086\n",
            "step:177, loss:220.23463439941406\n",
            "step:178, loss:48.96665954589844\n",
            "step:179, loss:54.30356216430664\n",
            "step:180, loss:12573.1953125\n",
            "step:181, loss:67.43782806396484\n",
            "step:182, loss:122.79911041259766\n",
            "step:183, loss:150.62957763671875\n",
            "step:184, loss:31.366554260253906\n",
            "step:185, loss:23.741233825683594\n",
            "step:186, loss:287.4335632324219\n",
            "step:187, loss:24.807659149169922\n",
            "step:188, loss:242.17922973632812\n",
            "step:189, loss:70.38783264160156\n",
            "step:190, loss:488.004150390625\n",
            "step:191, loss:17.694087982177734\n",
            "step:192, loss:13.691953659057617\n",
            "step:193, loss:888.5202026367188\n",
            "step:194, loss:64.84358215332031\n",
            "step:195, loss:19.96141815185547\n",
            "step:196, loss:73.40446472167969\n",
            "step:197, loss:221.37811279296875\n",
            "step:198, loss:19.449119567871094\n",
            "step:199, loss:22.44829559326172\n",
            "step:200, loss:74.80712890625\n",
            "step:201, loss:188.46112060546875\n",
            "step:202, loss:293.88238525390625\n",
            "11\n",
            "-----\n",
            "step:0, loss:26.52419662475586\n",
            "step:1, loss:70.55905151367188\n",
            "step:2, loss:80.6788101196289\n",
            "step:3, loss:21.893478393554688\n",
            "step:4, loss:23.23095703125\n",
            "step:5, loss:19.860393524169922\n",
            "step:6, loss:129.22482299804688\n",
            "step:7, loss:56.12217712402344\n",
            "step:8, loss:54.49172592163086\n",
            "step:9, loss:239.12554931640625\n",
            "step:10, loss:52.68040466308594\n",
            "step:11, loss:20.853492736816406\n",
            "step:12, loss:45.46980285644531\n",
            "step:13, loss:12.608165740966797\n",
            "step:14, loss:144.3096923828125\n",
            "step:15, loss:78.27841186523438\n",
            "step:16, loss:58.50015640258789\n",
            "step:17, loss:49.10295104980469\n",
            "step:18, loss:20.27033042907715\n",
            "step:19, loss:53.80656051635742\n",
            "step:20, loss:83.81974029541016\n",
            "step:21, loss:27.119091033935547\n",
            "step:22, loss:47.10930633544922\n",
            "step:23, loss:4.276486396789551\n",
            "step:24, loss:109.73887634277344\n",
            "step:25, loss:125.77625274658203\n",
            "step:26, loss:9.392789840698242\n",
            "step:27, loss:45.93661880493164\n",
            "step:28, loss:69.51371765136719\n",
            "step:29, loss:50.76656723022461\n",
            "step:30, loss:42.693843841552734\n",
            "step:31, loss:22.797962188720703\n",
            "step:32, loss:24.96023941040039\n",
            "step:33, loss:54.15745162963867\n",
            "step:34, loss:17.74327850341797\n",
            "step:35, loss:28.088119506835938\n",
            "step:36, loss:49.91350173950195\n",
            "step:37, loss:22.590473175048828\n",
            "step:38, loss:1061.913330078125\n",
            "step:39, loss:49.31668472290039\n",
            "step:40, loss:26.93929672241211\n",
            "step:41, loss:43.29182434082031\n",
            "step:42, loss:182.6416015625\n",
            "step:43, loss:22.756376266479492\n",
            "step:44, loss:38.038108825683594\n",
            "step:45, loss:156.54476928710938\n",
            "step:46, loss:32.965087890625\n",
            "step:47, loss:7.352468490600586\n",
            "step:48, loss:23.092187881469727\n",
            "step:49, loss:39.85333251953125\n",
            "step:50, loss:369.8620910644531\n",
            "step:51, loss:1152.1279296875\n",
            "step:52, loss:76.2817611694336\n",
            "step:53, loss:403.07330322265625\n",
            "step:54, loss:975.5558471679688\n",
            "step:55, loss:95.31978607177734\n",
            "step:56, loss:16.396625518798828\n",
            "step:57, loss:33.205745697021484\n",
            "step:58, loss:156.8717041015625\n",
            "step:59, loss:16.19644546508789\n",
            "step:60, loss:236.16708374023438\n",
            "step:61, loss:109.83387756347656\n",
            "step:62, loss:29.246929168701172\n",
            "step:63, loss:19.213747024536133\n",
            "step:64, loss:18.35824203491211\n",
            "step:65, loss:15.758218765258789\n",
            "step:66, loss:94.65043640136719\n",
            "step:67, loss:2057.427001953125\n",
            "step:68, loss:22.54347038269043\n",
            "step:69, loss:1732.189208984375\n",
            "step:70, loss:61.80167770385742\n",
            "step:71, loss:13.999408721923828\n",
            "step:72, loss:7551.03125\n",
            "step:73, loss:78.44672393798828\n",
            "step:74, loss:152.17251586914062\n",
            "step:75, loss:173.6707305908203\n",
            "step:76, loss:62.0787353515625\n",
            "step:77, loss:44.007938385009766\n",
            "step:78, loss:54.785194396972656\n",
            "step:79, loss:451.8096923828125\n",
            "step:80, loss:21.301454544067383\n",
            "step:81, loss:1371.8868408203125\n",
            "step:82, loss:68.58551025390625\n",
            "step:83, loss:47.35932159423828\n",
            "step:84, loss:22.756092071533203\n",
            "step:85, loss:18.393611907958984\n",
            "step:86, loss:19.775714874267578\n",
            "step:87, loss:124.7249755859375\n",
            "step:88, loss:37.919837951660156\n",
            "step:89, loss:7.446969032287598\n",
            "step:90, loss:33.826026916503906\n",
            "step:91, loss:69.1275634765625\n",
            "step:92, loss:32.30696105957031\n",
            "step:93, loss:28.490615844726562\n",
            "step:94, loss:45.2123908996582\n",
            "step:95, loss:254.6394805908203\n",
            "step:96, loss:202.02291870117188\n",
            "step:97, loss:27.600540161132812\n",
            "step:98, loss:387.2408142089844\n",
            "step:99, loss:17.572288513183594\n",
            "step:100, loss:11.069469451904297\n",
            "step:101, loss:793.9113159179688\n",
            "step:102, loss:851.8907470703125\n",
            "step:103, loss:12491.0107421875\n",
            "step:104, loss:161.3846893310547\n",
            "step:105, loss:12.870756149291992\n",
            "step:106, loss:80.03536987304688\n",
            "step:107, loss:44.3121452331543\n",
            "step:108, loss:98.63164520263672\n",
            "step:109, loss:53.288658142089844\n",
            "step:110, loss:39.63612365722656\n",
            "step:111, loss:8.623574256896973\n",
            "step:112, loss:59.51527404785156\n",
            "step:113, loss:44.364864349365234\n",
            "step:114, loss:10.080001831054688\n",
            "step:115, loss:21.220829010009766\n",
            "step:116, loss:32.79938888549805\n",
            "step:117, loss:50.48242950439453\n",
            "step:118, loss:360.0140380859375\n",
            "step:119, loss:406.7576904296875\n",
            "step:120, loss:11.722224235534668\n",
            "step:121, loss:117.33326721191406\n",
            "step:122, loss:36.279441833496094\n",
            "step:123, loss:43.167293548583984\n",
            "step:124, loss:5.640117645263672\n",
            "step:125, loss:24.422496795654297\n",
            "step:126, loss:20.441465377807617\n",
            "step:127, loss:9.969308853149414\n",
            "step:128, loss:125.5921630859375\n",
            "step:129, loss:159.61187744140625\n",
            "step:130, loss:382.225830078125\n",
            "step:131, loss:540.3182983398438\n",
            "step:132, loss:12.28182601928711\n",
            "step:133, loss:82.68842315673828\n",
            "step:134, loss:41.879669189453125\n",
            "step:135, loss:753.3215942382812\n",
            "step:136, loss:149.90892028808594\n",
            "step:137, loss:36.93426513671875\n",
            "step:138, loss:117.12826538085938\n",
            "step:139, loss:335.7266540527344\n",
            "step:140, loss:16.73934555053711\n",
            "step:141, loss:28.049537658691406\n",
            "step:142, loss:120.33771514892578\n",
            "step:143, loss:63.228519439697266\n",
            "step:144, loss:418.7350158691406\n",
            "step:145, loss:7.390198707580566\n",
            "step:146, loss:913.876953125\n",
            "step:147, loss:82.31966400146484\n",
            "step:148, loss:4726.35888671875\n",
            "step:149, loss:7658.1279296875\n",
            "step:150, loss:73.61013793945312\n",
            "step:151, loss:53.998863220214844\n",
            "step:152, loss:159.08929443359375\n",
            "step:153, loss:43.203636169433594\n",
            "step:154, loss:47.635162353515625\n",
            "step:155, loss:16.404388427734375\n",
            "step:156, loss:10.41568374633789\n",
            "step:157, loss:26.989532470703125\n",
            "step:158, loss:83.4609375\n",
            "step:159, loss:19.17801856994629\n",
            "step:160, loss:58.064918518066406\n",
            "step:161, loss:51.68705749511719\n",
            "step:162, loss:84.71693420410156\n",
            "step:163, loss:190.89666748046875\n",
            "step:164, loss:14.314773559570312\n",
            "step:165, loss:148.6409912109375\n",
            "step:166, loss:59.13677215576172\n",
            "step:167, loss:122.22792053222656\n",
            "step:168, loss:57.95610809326172\n",
            "step:169, loss:65.0372314453125\n",
            "step:170, loss:166.11016845703125\n",
            "step:171, loss:353.18951416015625\n",
            "step:172, loss:52.486061096191406\n",
            "step:173, loss:39.93804931640625\n",
            "step:174, loss:34.275367736816406\n",
            "step:175, loss:357.2176513671875\n",
            "step:176, loss:23.932361602783203\n",
            "step:177, loss:91.18677520751953\n",
            "step:178, loss:12.664172172546387\n",
            "step:179, loss:18.99321937561035\n",
            "step:180, loss:37.310752868652344\n",
            "step:181, loss:274.1871337890625\n",
            "step:182, loss:180.47030639648438\n",
            "step:183, loss:127.08744049072266\n",
            "step:184, loss:517.5909423828125\n",
            "step:185, loss:319.0583801269531\n",
            "step:186, loss:109.88374328613281\n",
            "step:187, loss:2976.441650390625\n",
            "step:188, loss:148.8944091796875\n",
            "step:189, loss:61.12653732299805\n",
            "step:190, loss:70.92416381835938\n",
            "step:191, loss:37.864479064941406\n",
            "step:192, loss:32.74222183227539\n",
            "step:193, loss:8.19943904876709\n",
            "step:194, loss:143.45777893066406\n",
            "step:195, loss:11.664848327636719\n",
            "step:196, loss:29.303165435791016\n",
            "step:197, loss:251.87742614746094\n",
            "step:198, loss:98.16866302490234\n",
            "step:199, loss:37.28840637207031\n",
            "step:200, loss:36.19662857055664\n",
            "step:201, loss:20.536483764648438\n",
            "step:202, loss:15.361071586608887\n",
            "12\n",
            "-----\n",
            "step:0, loss:64.74242401123047\n",
            "step:1, loss:64.71636962890625\n",
            "step:2, loss:48.9615592956543\n",
            "step:3, loss:39.69098663330078\n",
            "step:4, loss:342.0511779785156\n",
            "step:5, loss:232.4227294921875\n",
            "step:6, loss:11.400064468383789\n",
            "step:7, loss:160.88043212890625\n",
            "step:8, loss:11.038468360900879\n",
            "step:9, loss:59.347938537597656\n",
            "step:10, loss:169.79214477539062\n",
            "step:11, loss:171.20938110351562\n",
            "step:12, loss:16.406984329223633\n",
            "step:13, loss:72.9482650756836\n",
            "step:14, loss:37.64433670043945\n",
            "step:15, loss:39.75065231323242\n",
            "step:16, loss:12528.0947265625\n",
            "step:17, loss:6.830530643463135\n",
            "step:18, loss:30.76405906677246\n",
            "step:19, loss:496.13616943359375\n",
            "step:20, loss:41.774627685546875\n",
            "step:21, loss:23.83341407775879\n",
            "step:22, loss:52.527549743652344\n",
            "step:23, loss:5.644633769989014\n",
            "step:24, loss:14.575910568237305\n",
            "step:25, loss:114.78679656982422\n",
            "step:26, loss:75.58921813964844\n",
            "step:27, loss:63.808807373046875\n",
            "step:28, loss:965.1734619140625\n",
            "step:29, loss:90.79022216796875\n",
            "step:30, loss:29.347917556762695\n",
            "step:31, loss:814.337158203125\n",
            "step:32, loss:32.390926361083984\n",
            "step:33, loss:7.418452262878418\n",
            "step:34, loss:36.14848327636719\n",
            "step:35, loss:27.0665283203125\n",
            "step:36, loss:22.187992095947266\n",
            "step:37, loss:16.256591796875\n",
            "step:38, loss:25.50035858154297\n",
            "step:39, loss:26.684267044067383\n",
            "step:40, loss:33.39421081542969\n",
            "step:41, loss:452.587158203125\n",
            "step:42, loss:54.064735412597656\n",
            "step:43, loss:507.358154296875\n",
            "step:44, loss:55.17064666748047\n",
            "step:45, loss:19.45572280883789\n",
            "step:46, loss:34.14092254638672\n",
            "step:47, loss:130.42715454101562\n",
            "step:48, loss:206.58827209472656\n",
            "step:49, loss:31.665401458740234\n",
            "step:50, loss:19.813209533691406\n",
            "step:51, loss:67.32516479492188\n",
            "step:52, loss:21.971343994140625\n",
            "step:53, loss:6.751629829406738\n",
            "step:54, loss:483.3680419921875\n",
            "step:55, loss:56.67533493041992\n",
            "step:56, loss:26.8638973236084\n",
            "step:57, loss:275.70050048828125\n",
            "step:58, loss:12.927257537841797\n",
            "step:59, loss:62.17729949951172\n",
            "step:60, loss:8.5120849609375\n",
            "step:61, loss:13.530163764953613\n",
            "step:62, loss:31.830209732055664\n",
            "step:63, loss:75.55097961425781\n",
            "step:64, loss:32.494606018066406\n",
            "step:65, loss:12.725624084472656\n",
            "step:66, loss:153.51210021972656\n",
            "step:67, loss:22.25139617919922\n",
            "step:68, loss:26.15054702758789\n",
            "step:69, loss:109.50262451171875\n",
            "step:70, loss:133.00588989257812\n",
            "step:71, loss:552.3859252929688\n",
            "step:72, loss:12.79550552368164\n",
            "step:73, loss:274.4034423828125\n",
            "step:74, loss:499.6932373046875\n",
            "step:75, loss:59.22083282470703\n",
            "step:76, loss:7568.84765625\n",
            "step:77, loss:60.131080627441406\n",
            "step:78, loss:119.26205444335938\n",
            "step:79, loss:20.871475219726562\n",
            "step:80, loss:55.4913330078125\n",
            "step:81, loss:2004.630859375\n",
            "step:82, loss:122.29540252685547\n",
            "step:83, loss:15.817914009094238\n",
            "step:84, loss:45.770687103271484\n",
            "step:85, loss:22.782997131347656\n",
            "step:86, loss:81.2082290649414\n",
            "step:87, loss:38.62370681762695\n",
            "step:88, loss:39.438987731933594\n",
            "step:89, loss:8.795066833496094\n",
            "step:90, loss:28.555410385131836\n",
            "step:91, loss:55.675376892089844\n",
            "step:92, loss:103.0997314453125\n",
            "step:93, loss:53.1391716003418\n",
            "step:94, loss:608.4212036132812\n",
            "step:95, loss:35.95132827758789\n",
            "step:96, loss:14.537673950195312\n",
            "step:97, loss:11.068214416503906\n",
            "step:98, loss:157.3551788330078\n",
            "step:99, loss:45.474884033203125\n",
            "step:100, loss:40.76990509033203\n",
            "step:101, loss:6.985527038574219\n",
            "step:102, loss:50.74745178222656\n",
            "step:103, loss:49.315673828125\n",
            "step:104, loss:86.36067199707031\n",
            "step:105, loss:1307.3760986328125\n",
            "step:106, loss:9.22085189819336\n",
            "step:107, loss:18.576887130737305\n",
            "step:108, loss:239.17398071289062\n",
            "step:109, loss:404.44677734375\n",
            "step:110, loss:12.723922729492188\n",
            "step:111, loss:18.329225540161133\n",
            "step:112, loss:150.0128631591797\n",
            "step:113, loss:365.2298278808594\n",
            "step:114, loss:14.542509078979492\n",
            "step:115, loss:6.5297746658325195\n",
            "step:116, loss:403.207275390625\n",
            "step:117, loss:112.21247863769531\n",
            "step:118, loss:34.934974670410156\n",
            "step:119, loss:59.18242645263672\n",
            "step:120, loss:75.51339721679688\n",
            "step:121, loss:106.86756896972656\n",
            "step:122, loss:54.587528228759766\n",
            "step:123, loss:7.617854118347168\n",
            "step:124, loss:18.80263900756836\n",
            "step:125, loss:820.6986083984375\n",
            "step:126, loss:73.11878967285156\n",
            "step:127, loss:1731.8232421875\n",
            "step:128, loss:19.849544525146484\n",
            "step:129, loss:68.42240905761719\n",
            "step:130, loss:81.80332946777344\n",
            "step:131, loss:10.800409317016602\n",
            "step:132, loss:15.13065242767334\n",
            "step:133, loss:81.37987518310547\n",
            "step:134, loss:401.0260009765625\n",
            "step:135, loss:61.119972229003906\n",
            "step:136, loss:57.7144889831543\n",
            "step:137, loss:100.1937255859375\n",
            "step:138, loss:20.733627319335938\n",
            "step:139, loss:8.215909957885742\n",
            "step:140, loss:255.41677856445312\n",
            "step:141, loss:24.829822540283203\n",
            "step:142, loss:34.03388214111328\n",
            "step:143, loss:19.64842987060547\n",
            "step:144, loss:12.811970710754395\n",
            "step:145, loss:10.654548645019531\n",
            "step:146, loss:182.93426513671875\n",
            "step:147, loss:5.404212951660156\n",
            "step:148, loss:89.97927856445312\n",
            "step:149, loss:774.3740844726562\n",
            "step:150, loss:7668.98779296875\n",
            "step:151, loss:31.470226287841797\n",
            "step:152, loss:347.5065002441406\n",
            "step:153, loss:67.77214050292969\n",
            "step:154, loss:9.092912673950195\n",
            "step:155, loss:95.49918365478516\n",
            "step:156, loss:4634.81103515625\n",
            "step:157, loss:35.519630432128906\n",
            "step:158, loss:17.546390533447266\n",
            "step:159, loss:217.34695434570312\n",
            "step:160, loss:68.0188980102539\n",
            "step:161, loss:74.60688781738281\n",
            "step:162, loss:9.845674514770508\n",
            "step:163, loss:17.230655670166016\n",
            "step:164, loss:133.45693969726562\n",
            "step:165, loss:27.162986755371094\n",
            "step:166, loss:25.062721252441406\n",
            "step:167, loss:381.27392578125\n",
            "step:168, loss:8.471248626708984\n",
            "step:169, loss:358.6871643066406\n",
            "step:170, loss:262.7884216308594\n",
            "step:171, loss:16.75396156311035\n",
            "step:172, loss:28.71885871887207\n",
            "step:173, loss:244.2489013671875\n",
            "step:174, loss:31.734073638916016\n",
            "step:175, loss:1039.290283203125\n",
            "step:176, loss:8.754035949707031\n",
            "step:177, loss:78.167724609375\n",
            "step:178, loss:42.868309020996094\n",
            "step:179, loss:164.93455505371094\n",
            "step:180, loss:215.49966430664062\n",
            "step:181, loss:36.45842742919922\n",
            "step:182, loss:9.03924560546875\n",
            "step:183, loss:26.305631637573242\n",
            "step:184, loss:177.67724609375\n",
            "step:185, loss:13.78329086303711\n",
            "step:186, loss:56.796993255615234\n",
            "step:187, loss:15.640213012695312\n",
            "step:188, loss:84.66314697265625\n",
            "step:189, loss:18.029932022094727\n",
            "step:190, loss:141.62008666992188\n",
            "step:191, loss:16.579696655273438\n",
            "step:192, loss:163.93878173828125\n",
            "step:193, loss:199.4569854736328\n",
            "step:194, loss:33.70652389526367\n",
            "step:195, loss:599.427978515625\n",
            "step:196, loss:28.095783233642578\n",
            "step:197, loss:25.72405433654785\n",
            "step:198, loss:12.966251373291016\n",
            "step:199, loss:75.20329284667969\n",
            "step:200, loss:32.241600036621094\n",
            "step:201, loss:2809.272705078125\n",
            "step:202, loss:242.96751403808594\n",
            "13\n",
            "-----\n",
            "step:0, loss:9.898323059082031\n",
            "step:1, loss:23.903667449951172\n",
            "step:2, loss:8.666487693786621\n",
            "step:3, loss:7540.02783203125\n",
            "step:4, loss:22.586854934692383\n",
            "step:5, loss:130.06553649902344\n",
            "step:6, loss:259.74371337890625\n",
            "step:7, loss:13.6724853515625\n",
            "step:8, loss:11.323413848876953\n",
            "step:9, loss:17.253803253173828\n",
            "step:10, loss:101.31040954589844\n",
            "step:11, loss:9.7787504196167\n",
            "step:12, loss:11.477041244506836\n",
            "step:13, loss:12.650741577148438\n",
            "step:14, loss:29.072195053100586\n",
            "step:15, loss:16.583782196044922\n",
            "step:16, loss:33.37577819824219\n",
            "step:17, loss:263.4452209472656\n",
            "step:18, loss:50.66979217529297\n",
            "step:19, loss:300.2743835449219\n",
            "step:20, loss:271.8176574707031\n",
            "step:21, loss:53.08262634277344\n",
            "step:22, loss:7.3701171875\n",
            "step:23, loss:29.77042007446289\n",
            "step:24, loss:30.68218231201172\n",
            "step:25, loss:18.910490036010742\n",
            "step:26, loss:14.757484436035156\n",
            "step:27, loss:57.6917839050293\n",
            "step:28, loss:8.444568634033203\n",
            "step:29, loss:36.58810806274414\n",
            "step:30, loss:24.972827911376953\n",
            "step:31, loss:1668.3104248046875\n",
            "step:32, loss:35.245941162109375\n",
            "step:33, loss:23.584491729736328\n",
            "step:34, loss:144.54385375976562\n",
            "step:35, loss:54.84014892578125\n",
            "step:36, loss:11.144516944885254\n",
            "step:37, loss:37.95359420776367\n",
            "step:38, loss:85.413330078125\n",
            "step:39, loss:27.23100471496582\n",
            "step:40, loss:7745.31689453125\n",
            "step:41, loss:123.56237030029297\n",
            "step:42, loss:24.59384536743164\n",
            "step:43, loss:9.809436798095703\n",
            "step:44, loss:8.299722671508789\n",
            "step:45, loss:100.32650756835938\n",
            "step:46, loss:23.2822265625\n",
            "step:47, loss:148.4184112548828\n",
            "step:48, loss:12.1425142288208\n",
            "step:49, loss:35.74542236328125\n",
            "step:50, loss:926.6538696289062\n",
            "step:51, loss:261.3022766113281\n",
            "step:52, loss:30.996931076049805\n",
            "step:53, loss:126.4012222290039\n",
            "step:54, loss:66.39053344726562\n",
            "step:55, loss:46.89263153076172\n",
            "step:56, loss:49.12835693359375\n",
            "step:57, loss:12.23333740234375\n",
            "step:58, loss:9.314484596252441\n",
            "step:59, loss:22.957820892333984\n",
            "step:60, loss:8.79953670501709\n",
            "step:61, loss:18.69339942932129\n",
            "step:62, loss:18.663745880126953\n",
            "step:63, loss:14.094871520996094\n",
            "step:64, loss:36.45631408691406\n",
            "step:65, loss:13.45030403137207\n",
            "step:66, loss:54.529273986816406\n",
            "step:67, loss:584.4945068359375\n",
            "step:68, loss:100.24179077148438\n",
            "step:69, loss:139.5284423828125\n",
            "step:70, loss:6.992799758911133\n",
            "step:71, loss:12373.12109375\n",
            "step:72, loss:277.606689453125\n",
            "step:73, loss:38.914093017578125\n",
            "step:74, loss:174.15017700195312\n",
            "step:75, loss:39.120277404785156\n",
            "step:76, loss:335.5808410644531\n",
            "step:77, loss:61.792022705078125\n",
            "step:78, loss:69.43347930908203\n",
            "step:79, loss:26.573043823242188\n",
            "step:80, loss:18.173839569091797\n",
            "step:81, loss:168.83421325683594\n",
            "step:82, loss:10.330726623535156\n",
            "step:83, loss:343.8653869628906\n",
            "step:84, loss:28.954299926757812\n",
            "step:85, loss:16.88858413696289\n",
            "step:86, loss:13.244391441345215\n",
            "step:87, loss:23.421524047851562\n",
            "step:88, loss:122.09989929199219\n",
            "step:89, loss:36.964576721191406\n",
            "step:90, loss:24.34097671508789\n",
            "step:91, loss:41.54359436035156\n",
            "step:92, loss:11.84545612335205\n",
            "step:93, loss:14.904762268066406\n",
            "step:94, loss:607.7515258789062\n",
            "step:95, loss:4.839547634124756\n",
            "step:96, loss:222.35780334472656\n",
            "step:97, loss:169.91146850585938\n",
            "step:98, loss:30.46608543395996\n",
            "step:99, loss:9.286870956420898\n",
            "step:100, loss:33.90473556518555\n",
            "step:101, loss:246.7964630126953\n",
            "step:102, loss:42.256553649902344\n",
            "step:103, loss:89.49784088134766\n",
            "step:104, loss:22.038997650146484\n",
            "step:105, loss:10.891674041748047\n",
            "step:106, loss:54.98599624633789\n",
            "step:107, loss:213.5836944580078\n",
            "step:108, loss:112.05955505371094\n",
            "step:109, loss:158.1959686279297\n",
            "step:110, loss:10.564720153808594\n",
            "step:111, loss:15.65719223022461\n",
            "step:112, loss:37.9051513671875\n",
            "step:113, loss:180.70664978027344\n",
            "step:114, loss:24.583356857299805\n",
            "step:115, loss:208.86534118652344\n",
            "step:116, loss:59.233970642089844\n",
            "step:117, loss:135.35061645507812\n",
            "step:118, loss:67.67032623291016\n",
            "step:119, loss:157.8202667236328\n",
            "step:120, loss:132.13763427734375\n",
            "step:121, loss:789.1134033203125\n",
            "step:122, loss:148.25404357910156\n",
            "step:123, loss:51.82938003540039\n",
            "step:124, loss:9.591513633728027\n",
            "step:125, loss:61.270809173583984\n",
            "step:126, loss:110.33847045898438\n",
            "step:127, loss:278.8138122558594\n",
            "step:128, loss:9.25278091430664\n",
            "step:129, loss:9.860087394714355\n",
            "step:130, loss:397.95947265625\n",
            "step:131, loss:265.50732421875\n",
            "step:132, loss:67.6908187866211\n",
            "step:133, loss:39.536685943603516\n",
            "step:134, loss:28.08855628967285\n",
            "step:135, loss:130.03546142578125\n",
            "step:136, loss:18.300525665283203\n",
            "step:137, loss:252.66201782226562\n",
            "step:138, loss:46.135154724121094\n",
            "step:139, loss:4522.35595703125\n",
            "step:140, loss:24.393478393554688\n",
            "step:141, loss:397.69500732421875\n",
            "step:142, loss:56.463783264160156\n",
            "step:143, loss:51.85951614379883\n",
            "step:144, loss:16.667085647583008\n",
            "step:145, loss:38.30640411376953\n",
            "step:146, loss:41.64889144897461\n",
            "step:147, loss:93.30852508544922\n",
            "step:148, loss:395.02655029296875\n",
            "step:149, loss:1749.0185546875\n",
            "step:150, loss:9.034124374389648\n",
            "step:151, loss:31.79729461669922\n",
            "step:152, loss:157.81378173828125\n",
            "step:153, loss:141.30307006835938\n",
            "step:154, loss:72.12110900878906\n",
            "step:155, loss:148.61700439453125\n",
            "step:156, loss:9.725672721862793\n",
            "step:157, loss:221.98135375976562\n",
            "step:158, loss:88.81806945800781\n",
            "step:159, loss:309.99462890625\n",
            "step:160, loss:42.84431076049805\n",
            "step:161, loss:323.8253173828125\n",
            "step:162, loss:11.392546653747559\n",
            "step:163, loss:9.849954605102539\n",
            "step:164, loss:12.08182144165039\n",
            "step:165, loss:80.21892547607422\n",
            "step:166, loss:19.433879852294922\n",
            "step:167, loss:32.221839904785156\n",
            "step:168, loss:11.344143867492676\n",
            "step:169, loss:163.41783142089844\n",
            "step:170, loss:28.401485443115234\n",
            "step:171, loss:169.64651489257812\n",
            "step:172, loss:770.2294921875\n",
            "step:173, loss:18.653202056884766\n",
            "step:174, loss:298.0909118652344\n",
            "step:175, loss:209.76766967773438\n",
            "step:176, loss:187.63009643554688\n",
            "step:177, loss:24.168989181518555\n",
            "step:178, loss:19.185195922851562\n",
            "step:179, loss:129.79161071777344\n",
            "step:180, loss:15.114540100097656\n",
            "step:181, loss:51.3165283203125\n",
            "step:182, loss:20.588809967041016\n",
            "step:183, loss:38.30316925048828\n",
            "step:184, loss:2779.97265625\n",
            "step:185, loss:96.55905151367188\n",
            "step:186, loss:75.53142547607422\n",
            "step:187, loss:41.032875061035156\n",
            "step:188, loss:1226.018798828125\n",
            "step:189, loss:91.59768676757812\n",
            "step:190, loss:302.22314453125\n",
            "step:191, loss:78.4321517944336\n",
            "step:192, loss:7.6150946617126465\n",
            "step:193, loss:22.20900535583496\n",
            "step:194, loss:21.991302490234375\n",
            "step:195, loss:482.5809631347656\n",
            "step:196, loss:171.81918334960938\n",
            "step:197, loss:73.70529174804688\n",
            "step:198, loss:98.01683044433594\n",
            "step:199, loss:890.7214965820312\n",
            "step:200, loss:62.206581115722656\n",
            "step:201, loss:14.915555953979492\n",
            "step:202, loss:2640.28564453125\n",
            "14\n",
            "-----\n",
            "step:0, loss:27.215152740478516\n",
            "step:1, loss:153.24168395996094\n",
            "step:2, loss:399.92083740234375\n",
            "step:3, loss:36.08624267578125\n",
            "step:4, loss:18.304264068603516\n",
            "step:5, loss:134.72640991210938\n",
            "step:6, loss:325.3144226074219\n",
            "step:7, loss:278.57208251953125\n",
            "step:8, loss:366.6248779296875\n",
            "step:9, loss:9.128974914550781\n",
            "step:10, loss:51.0716667175293\n",
            "step:11, loss:27.672027587890625\n",
            "step:12, loss:35.00925827026367\n",
            "step:13, loss:125.86527252197266\n",
            "step:14, loss:7577.287109375\n",
            "step:15, loss:73.81517028808594\n",
            "step:16, loss:22.269201278686523\n",
            "step:17, loss:10.788642883300781\n",
            "step:18, loss:10.25622844696045\n",
            "step:19, loss:128.21119689941406\n",
            "step:20, loss:54.70039367675781\n",
            "step:21, loss:71.37736511230469\n",
            "step:22, loss:11.040833473205566\n",
            "step:23, loss:44.87492752075195\n",
            "step:24, loss:13.316994667053223\n",
            "step:25, loss:9.722128868103027\n",
            "step:26, loss:13.162487983703613\n",
            "step:27, loss:15.882696151733398\n",
            "step:28, loss:9.846175193786621\n",
            "step:29, loss:42.47901916503906\n",
            "step:30, loss:41.35700225830078\n",
            "step:31, loss:16.161602020263672\n",
            "step:32, loss:92.68084716796875\n",
            "step:33, loss:8.521141052246094\n",
            "step:34, loss:201.56509399414062\n",
            "step:35, loss:21.16217041015625\n",
            "step:36, loss:67.47465515136719\n",
            "step:37, loss:127.06415557861328\n",
            "step:38, loss:42.363800048828125\n",
            "step:39, loss:20.03634262084961\n",
            "step:40, loss:10.427020072937012\n",
            "step:41, loss:12485.689453125\n",
            "step:42, loss:51.3946533203125\n",
            "step:43, loss:565.0807495117188\n",
            "step:44, loss:34.22039031982422\n",
            "step:45, loss:8.217832565307617\n",
            "step:46, loss:69.64447021484375\n",
            "step:47, loss:9.37043571472168\n",
            "step:48, loss:1858.21533203125\n",
            "step:49, loss:10.809158325195312\n",
            "step:50, loss:178.74636840820312\n",
            "step:51, loss:13.249820709228516\n",
            "step:52, loss:351.40496826171875\n",
            "step:53, loss:40.244285583496094\n",
            "step:54, loss:61.673065185546875\n",
            "step:55, loss:18.519346237182617\n",
            "step:56, loss:6.646675109863281\n",
            "step:57, loss:295.4364013671875\n",
            "step:58, loss:12.543534278869629\n",
            "step:59, loss:12.013437271118164\n",
            "step:60, loss:165.45498657226562\n",
            "step:61, loss:274.9921875\n",
            "step:62, loss:12.834490776062012\n",
            "step:63, loss:35.53009033203125\n",
            "step:64, loss:77.63749694824219\n",
            "step:65, loss:39.80698776245117\n",
            "step:66, loss:11.695377349853516\n",
            "step:67, loss:7560.8251953125\n",
            "step:68, loss:50.804805755615234\n",
            "step:69, loss:45.70500564575195\n",
            "step:70, loss:141.58949279785156\n",
            "step:71, loss:31.163860321044922\n",
            "step:72, loss:20.069984436035156\n",
            "step:73, loss:106.2743911743164\n",
            "step:74, loss:6.6995530128479\n",
            "step:75, loss:60.87800216674805\n",
            "step:76, loss:8.25659465789795\n",
            "step:77, loss:13.458024978637695\n",
            "step:78, loss:46.1243896484375\n",
            "step:79, loss:107.88091278076172\n",
            "step:80, loss:15.935771942138672\n",
            "step:81, loss:53.064090728759766\n",
            "step:82, loss:5.884971618652344\n",
            "step:83, loss:47.42188262939453\n",
            "step:84, loss:270.4835205078125\n",
            "step:85, loss:126.49783325195312\n",
            "step:86, loss:873.7507934570312\n",
            "step:87, loss:70.61255645751953\n",
            "step:88, loss:371.41680908203125\n",
            "step:89, loss:12.979059219360352\n",
            "step:90, loss:2764.8359375\n",
            "step:91, loss:287.65350341796875\n",
            "step:92, loss:18.18796730041504\n",
            "step:93, loss:21.866750717163086\n",
            "step:94, loss:15.095449447631836\n",
            "step:95, loss:391.5372619628906\n",
            "step:96, loss:24.34918975830078\n",
            "step:97, loss:5.472130298614502\n",
            "step:98, loss:13.529321670532227\n",
            "step:99, loss:40.909847259521484\n",
            "step:100, loss:42.219505310058594\n",
            "step:101, loss:48.533817291259766\n",
            "step:102, loss:8.702701568603516\n",
            "step:103, loss:1030.8770751953125\n",
            "step:104, loss:17.98924446105957\n",
            "step:105, loss:19.5228214263916\n",
            "step:106, loss:51.90647506713867\n",
            "step:107, loss:28.863853454589844\n",
            "step:108, loss:20.496170043945312\n",
            "step:109, loss:14.336503982543945\n",
            "step:110, loss:70.40887451171875\n",
            "step:111, loss:68.00714111328125\n",
            "step:112, loss:80.92560577392578\n",
            "step:113, loss:12.235758781433105\n",
            "step:114, loss:5.435754299163818\n",
            "step:115, loss:57.834373474121094\n",
            "step:116, loss:37.050167083740234\n",
            "step:117, loss:9.434345245361328\n",
            "step:118, loss:19.1097412109375\n",
            "step:119, loss:61.62254333496094\n",
            "step:120, loss:727.7984008789062\n",
            "step:121, loss:83.17108917236328\n",
            "step:122, loss:279.84930419921875\n",
            "step:123, loss:202.50540161132812\n",
            "step:124, loss:212.9247589111328\n",
            "step:125, loss:862.867431640625\n",
            "step:126, loss:102.2314224243164\n",
            "step:127, loss:3.8388900756835938\n",
            "step:128, loss:17.893402099609375\n",
            "step:129, loss:9.233238220214844\n",
            "step:130, loss:16.50825309753418\n",
            "step:131, loss:13.118904113769531\n",
            "step:132, loss:120.14283752441406\n",
            "step:133, loss:31.258567810058594\n",
            "step:134, loss:29.449796676635742\n",
            "step:135, loss:6.286684989929199\n",
            "step:136, loss:14.184104919433594\n",
            "step:137, loss:1665.383056640625\n",
            "step:138, loss:59.54198455810547\n",
            "step:139, loss:60.6475944519043\n",
            "step:140, loss:157.2340087890625\n",
            "step:141, loss:78.76814270019531\n",
            "step:142, loss:49.06391906738281\n",
            "step:143, loss:22.658166885375977\n",
            "step:144, loss:12.13039779663086\n",
            "step:145, loss:31.214813232421875\n",
            "step:146, loss:644.89599609375\n",
            "step:147, loss:50.02153778076172\n",
            "step:148, loss:14.334259033203125\n",
            "step:149, loss:15.38877010345459\n",
            "step:150, loss:100.7920150756836\n",
            "step:151, loss:26.477521896362305\n",
            "step:152, loss:172.59640502929688\n",
            "step:153, loss:87.33334350585938\n",
            "step:154, loss:6.530886650085449\n",
            "step:155, loss:44.61003875732422\n",
            "step:156, loss:16.644119262695312\n",
            "step:157, loss:85.79652404785156\n",
            "step:158, loss:27.608705520629883\n",
            "step:159, loss:14.995586395263672\n",
            "step:160, loss:35.291168212890625\n",
            "step:161, loss:27.12088394165039\n",
            "step:162, loss:15.601770401000977\n",
            "step:163, loss:373.8880615234375\n",
            "step:164, loss:10.83630657196045\n",
            "step:165, loss:49.61297607421875\n",
            "step:166, loss:12.501611709594727\n",
            "step:167, loss:16.530576705932617\n",
            "step:168, loss:259.55267333984375\n",
            "step:169, loss:85.90802001953125\n",
            "step:170, loss:299.5887756347656\n",
            "step:171, loss:5.494349479675293\n",
            "step:172, loss:182.30841064453125\n",
            "step:173, loss:14.20408821105957\n",
            "step:174, loss:12.417436599731445\n",
            "step:175, loss:43.28306579589844\n",
            "step:176, loss:28.732372283935547\n",
            "step:177, loss:153.68856811523438\n",
            "step:178, loss:5118.85595703125\n",
            "step:179, loss:111.21619415283203\n",
            "step:180, loss:6.760735988616943\n",
            "step:181, loss:85.67660522460938\n",
            "step:182, loss:1187.31689453125\n",
            "step:183, loss:9.540443420410156\n",
            "step:184, loss:18.680551528930664\n",
            "step:185, loss:765.1489868164062\n",
            "step:186, loss:327.3338623046875\n",
            "step:187, loss:81.0464859008789\n",
            "step:188, loss:12.165092468261719\n",
            "step:189, loss:356.97357177734375\n",
            "step:190, loss:21.513643264770508\n",
            "step:191, loss:237.1322784423828\n",
            "step:192, loss:120.30142974853516\n",
            "step:193, loss:20.845605850219727\n",
            "step:194, loss:8.806963920593262\n",
            "step:195, loss:67.7401123046875\n",
            "step:196, loss:9.884269714355469\n",
            "step:197, loss:136.83001708984375\n",
            "step:198, loss:113.87699127197266\n",
            "step:199, loss:403.0479736328125\n",
            "step:200, loss:22.4090518951416\n",
            "step:201, loss:10.021376609802246\n",
            "step:202, loss:9.269545555114746\n",
            "15\n",
            "-----\n",
            "step:0, loss:838.7374267578125\n",
            "step:1, loss:8.024239540100098\n",
            "step:2, loss:25.1095027923584\n",
            "step:3, loss:9.623022079467773\n",
            "step:4, loss:111.46154022216797\n",
            "step:5, loss:88.50536346435547\n",
            "step:6, loss:269.2161560058594\n",
            "step:7, loss:18.50724983215332\n",
            "step:8, loss:16.822185516357422\n",
            "step:9, loss:399.3951416015625\n",
            "step:10, loss:9.516998291015625\n",
            "step:11, loss:15.656866073608398\n",
            "step:12, loss:119.88170623779297\n",
            "step:13, loss:29.733789443969727\n",
            "step:14, loss:7.656757831573486\n",
            "step:15, loss:221.4639129638672\n",
            "step:16, loss:137.791748046875\n",
            "step:17, loss:21.532833099365234\n",
            "step:18, loss:24.29967498779297\n",
            "step:19, loss:20.626184463500977\n",
            "step:20, loss:3.9710264205932617\n",
            "step:21, loss:245.2937774658203\n",
            "step:22, loss:140.3816680908203\n",
            "step:23, loss:59.011478424072266\n",
            "step:24, loss:22.529571533203125\n",
            "step:25, loss:133.93377685546875\n",
            "step:26, loss:16.382457733154297\n",
            "step:27, loss:15.141469955444336\n",
            "step:28, loss:366.32745361328125\n",
            "step:29, loss:18.976715087890625\n",
            "step:30, loss:2702.445556640625\n",
            "step:31, loss:4536.671875\n",
            "step:32, loss:10.869146347045898\n",
            "step:33, loss:926.3928833007812\n",
            "step:34, loss:108.87033081054688\n",
            "step:35, loss:1870.142578125\n",
            "step:36, loss:5.824601173400879\n",
            "step:37, loss:59.65089797973633\n",
            "step:38, loss:7369.1728515625\n",
            "step:39, loss:18.102203369140625\n",
            "step:40, loss:8.022043228149414\n",
            "step:41, loss:77.94865417480469\n",
            "step:42, loss:5.839550971984863\n",
            "step:43, loss:7.230380535125732\n",
            "step:44, loss:20.173641204833984\n",
            "step:45, loss:32.87791442871094\n",
            "step:46, loss:30.465267181396484\n",
            "step:47, loss:7.44358491897583\n",
            "step:48, loss:7.528765678405762\n",
            "step:49, loss:284.35943603515625\n",
            "step:50, loss:36.607913970947266\n",
            "step:51, loss:207.311279296875\n",
            "step:52, loss:16.526504516601562\n",
            "step:53, loss:43.41203689575195\n",
            "step:54, loss:20.51099395751953\n",
            "step:55, loss:8.015334129333496\n",
            "step:56, loss:74.19482421875\n",
            "step:57, loss:18.924331665039062\n",
            "step:58, loss:309.331298828125\n",
            "step:59, loss:261.0606994628906\n",
            "step:60, loss:25.9180965423584\n",
            "step:61, loss:25.684410095214844\n",
            "step:62, loss:14.524779319763184\n",
            "step:63, loss:125.50291442871094\n",
            "step:64, loss:26.68618392944336\n",
            "step:65, loss:14.874103546142578\n",
            "step:66, loss:62.05345916748047\n",
            "step:67, loss:156.5113525390625\n",
            "step:68, loss:22.117979049682617\n",
            "step:69, loss:5.9416823387146\n",
            "step:70, loss:7497.2900390625\n",
            "step:71, loss:45.82648849487305\n",
            "step:72, loss:45.029136657714844\n",
            "step:73, loss:52.98442840576172\n",
            "step:74, loss:52.56184387207031\n",
            "step:75, loss:8.534324645996094\n",
            "step:76, loss:7.669300079345703\n",
            "step:77, loss:16.162717819213867\n",
            "step:78, loss:28.465242385864258\n",
            "step:79, loss:1160.0166015625\n",
            "step:80, loss:193.20538330078125\n",
            "step:81, loss:15.097841262817383\n",
            "step:82, loss:6.858617782592773\n",
            "step:83, loss:53.85584259033203\n",
            "step:84, loss:79.13871765136719\n",
            "step:85, loss:18.719545364379883\n",
            "step:86, loss:63.42864990234375\n",
            "step:87, loss:25.826749801635742\n",
            "step:88, loss:6.825809478759766\n",
            "step:89, loss:6.744818210601807\n",
            "step:90, loss:12.85755729675293\n",
            "step:91, loss:566.0335083007812\n",
            "step:92, loss:25.3963680267334\n",
            "step:93, loss:5.2289018630981445\n",
            "step:94, loss:521.376708984375\n",
            "step:95, loss:246.9413299560547\n",
            "step:96, loss:33.71058654785156\n",
            "step:97, loss:247.4770050048828\n",
            "step:98, loss:18.447731018066406\n",
            "step:99, loss:42.121864318847656\n",
            "step:100, loss:6.1446027755737305\n",
            "step:101, loss:60.00559616088867\n",
            "step:102, loss:13.430394172668457\n",
            "step:103, loss:117.04998016357422\n",
            "step:104, loss:35.15455627441406\n",
            "step:105, loss:13.785406112670898\n",
            "step:106, loss:24.217395782470703\n",
            "step:107, loss:802.8876342773438\n",
            "step:108, loss:220.42446899414062\n",
            "step:109, loss:121.39817810058594\n",
            "step:110, loss:23.754335403442383\n",
            "step:111, loss:30.38193130493164\n",
            "step:112, loss:7.646233558654785\n",
            "step:113, loss:17.93121910095215\n",
            "step:114, loss:332.1325988769531\n",
            "step:115, loss:150.05880737304688\n",
            "step:116, loss:37.025386810302734\n",
            "step:117, loss:277.0680847167969\n",
            "step:118, loss:116.44996643066406\n",
            "step:119, loss:67.93330383300781\n",
            "step:120, loss:63.19522476196289\n",
            "step:121, loss:21.257619857788086\n",
            "step:122, loss:9.224076271057129\n",
            "step:123, loss:21.85707664489746\n",
            "step:124, loss:197.3605194091797\n",
            "step:125, loss:15.604679107666016\n",
            "step:126, loss:4.413625717163086\n",
            "step:127, loss:11.92963695526123\n",
            "step:128, loss:24.387229919433594\n",
            "step:129, loss:594.3444213867188\n",
            "step:130, loss:79.56330871582031\n",
            "step:131, loss:113.04884338378906\n",
            "step:132, loss:136.03851318359375\n",
            "step:133, loss:16.426115036010742\n",
            "step:134, loss:101.21452331542969\n",
            "step:135, loss:100.45441436767578\n",
            "step:136, loss:30.1101131439209\n",
            "step:137, loss:506.98797607421875\n",
            "step:138, loss:57.07020568847656\n",
            "step:139, loss:148.5113525390625\n",
            "step:140, loss:26.410701751708984\n",
            "step:141, loss:36.47014236450195\n",
            "step:142, loss:83.28925323486328\n",
            "step:143, loss:206.48501586914062\n",
            "step:144, loss:64.8134994506836\n",
            "step:145, loss:73.84227752685547\n",
            "step:146, loss:68.34504699707031\n",
            "step:147, loss:38.958927154541016\n",
            "step:148, loss:174.28778076171875\n",
            "step:149, loss:20.736814498901367\n",
            "step:150, loss:14.327342987060547\n",
            "step:151, loss:81.67195129394531\n",
            "step:152, loss:20.174848556518555\n",
            "step:153, loss:15.650422096252441\n",
            "step:154, loss:57.380130767822266\n",
            "step:155, loss:74.02345275878906\n",
            "step:156, loss:13.029050827026367\n",
            "step:157, loss:24.800899505615234\n",
            "step:158, loss:156.91709899902344\n",
            "step:159, loss:9.317956924438477\n",
            "step:160, loss:8.035760879516602\n",
            "step:161, loss:43.79236602783203\n",
            "step:162, loss:64.58842468261719\n",
            "step:163, loss:8.479780197143555\n",
            "step:164, loss:27.23017692565918\n",
            "step:165, loss:9.669116020202637\n",
            "step:166, loss:1575.9793701171875\n",
            "step:167, loss:78.5501937866211\n",
            "step:168, loss:47.687217712402344\n",
            "step:169, loss:13.868849754333496\n",
            "step:170, loss:243.0984649658203\n",
            "step:171, loss:7.526484489440918\n",
            "step:172, loss:12.562934875488281\n",
            "step:173, loss:46.003517150878906\n",
            "step:174, loss:62.579341888427734\n",
            "step:175, loss:10.285024642944336\n",
            "step:176, loss:7.446479320526123\n",
            "step:177, loss:11.279770851135254\n",
            "step:178, loss:16.274394989013672\n",
            "step:179, loss:50.2855224609375\n",
            "step:180, loss:92.7104721069336\n",
            "step:181, loss:64.29168701171875\n",
            "step:182, loss:7.968025207519531\n",
            "step:183, loss:33.07545471191406\n",
            "step:184, loss:181.53756713867188\n",
            "step:185, loss:49.81163787841797\n",
            "step:186, loss:547.3984375\n",
            "step:187, loss:40.15576171875\n",
            "step:188, loss:11.718123435974121\n",
            "step:189, loss:65.27896881103516\n",
            "step:190, loss:157.83273315429688\n",
            "step:191, loss:34.57612609863281\n",
            "step:192, loss:14.68363094329834\n",
            "step:193, loss:171.3958282470703\n",
            "step:194, loss:10.55314826965332\n",
            "step:195, loss:10.003698348999023\n",
            "step:196, loss:12280.197265625\n",
            "step:197, loss:710.9524536132812\n",
            "step:198, loss:196.57040405273438\n",
            "step:199, loss:35.488853454589844\n",
            "step:200, loss:813.076171875\n",
            "step:201, loss:154.95834350585938\n",
            "step:202, loss:205.17962646484375\n",
            "16\n",
            "-----\n",
            "step:0, loss:11.83527660369873\n",
            "step:1, loss:53.456298828125\n",
            "step:2, loss:27.147106170654297\n",
            "step:3, loss:13.000572204589844\n",
            "step:4, loss:84.79644012451172\n",
            "step:5, loss:5.447042465209961\n",
            "step:6, loss:13.282596588134766\n",
            "step:7, loss:31.71571159362793\n",
            "step:8, loss:14.797685623168945\n",
            "step:9, loss:25.39126968383789\n",
            "step:10, loss:13.188644409179688\n",
            "step:11, loss:7.423540115356445\n",
            "step:12, loss:3.7187201976776123\n",
            "step:13, loss:112.8200454711914\n",
            "step:14, loss:13.258190155029297\n",
            "step:15, loss:25.859886169433594\n",
            "step:16, loss:24.52005958557129\n",
            "step:17, loss:701.1141357421875\n",
            "step:18, loss:670.51513671875\n",
            "step:19, loss:7.338659286499023\n",
            "step:20, loss:19.808950424194336\n",
            "step:21, loss:135.268798828125\n",
            "step:22, loss:1227.439697265625\n",
            "step:23, loss:550.3846435546875\n",
            "step:24, loss:294.0173645019531\n",
            "step:25, loss:102.80657196044922\n",
            "step:26, loss:455.35101318359375\n",
            "step:27, loss:93.69379425048828\n",
            "step:28, loss:71.26807403564453\n",
            "step:29, loss:147.86212158203125\n",
            "step:30, loss:108.48028564453125\n",
            "step:31, loss:61.769142150878906\n",
            "step:32, loss:303.099853515625\n",
            "step:33, loss:9.081110000610352\n",
            "step:34, loss:14.416908264160156\n",
            "step:35, loss:11.303863525390625\n",
            "step:36, loss:95.44184875488281\n",
            "step:37, loss:13.36209487915039\n",
            "step:38, loss:1740.089111328125\n",
            "step:39, loss:7.2989702224731445\n",
            "step:40, loss:9.135889053344727\n",
            "step:41, loss:61.7502326965332\n",
            "step:42, loss:31.29541778564453\n",
            "step:43, loss:115.26007080078125\n",
            "step:44, loss:41.115806579589844\n",
            "step:45, loss:281.8876953125\n",
            "step:46, loss:7.728977680206299\n",
            "step:47, loss:54.56321334838867\n",
            "step:48, loss:23.363370895385742\n",
            "step:49, loss:14.66877269744873\n",
            "step:50, loss:56.66569137573242\n",
            "step:51, loss:5.615328788757324\n",
            "step:52, loss:11.80927562713623\n",
            "step:53, loss:7.459560394287109\n",
            "step:54, loss:97.1622543334961\n",
            "step:55, loss:12.517560958862305\n",
            "step:56, loss:99.3443832397461\n",
            "step:57, loss:208.1421661376953\n",
            "step:58, loss:76.1029281616211\n",
            "step:59, loss:9.946525573730469\n",
            "step:60, loss:4560.17822265625\n",
            "step:61, loss:877.9884033203125\n",
            "step:62, loss:8.587858200073242\n",
            "step:63, loss:143.76634216308594\n",
            "step:64, loss:31.288990020751953\n",
            "step:65, loss:178.71981811523438\n",
            "step:66, loss:310.475341796875\n",
            "step:67, loss:22.45524024963379\n",
            "step:68, loss:743.7551879882812\n",
            "step:69, loss:6.586524963378906\n",
            "step:70, loss:7.786416053771973\n",
            "step:71, loss:102.22454833984375\n",
            "step:72, loss:150.3167724609375\n",
            "step:73, loss:22.17816925048828\n",
            "step:74, loss:17.795574188232422\n",
            "step:75, loss:5.899330139160156\n",
            "step:76, loss:10.114899635314941\n",
            "step:77, loss:13.051996231079102\n",
            "step:78, loss:13.833867073059082\n",
            "step:79, loss:53.70146179199219\n",
            "step:80, loss:12.231707572937012\n",
            "step:81, loss:119.77925109863281\n",
            "step:82, loss:10.336108207702637\n",
            "step:83, loss:14.655449867248535\n",
            "step:84, loss:16.00568389892578\n",
            "step:85, loss:31.757352828979492\n",
            "step:86, loss:10.170002937316895\n",
            "step:87, loss:68.32272338867188\n",
            "step:88, loss:76.3338623046875\n",
            "step:89, loss:87.342041015625\n",
            "step:90, loss:35.151283264160156\n",
            "step:91, loss:59.65196990966797\n",
            "step:92, loss:67.72361755371094\n",
            "step:93, loss:23.780113220214844\n",
            "step:94, loss:87.26427459716797\n",
            "step:95, loss:46.291805267333984\n",
            "step:96, loss:8.96109676361084\n",
            "step:97, loss:71.21365356445312\n",
            "step:98, loss:10.09676742553711\n",
            "step:99, loss:192.36619567871094\n",
            "step:100, loss:165.61929321289062\n",
            "step:101, loss:25.565567016601562\n",
            "step:102, loss:372.6732482910156\n",
            "step:103, loss:55.15256118774414\n",
            "step:104, loss:12.812376022338867\n",
            "step:105, loss:3.884436845779419\n",
            "step:106, loss:110.32046508789062\n",
            "step:107, loss:5.382994651794434\n",
            "step:108, loss:318.8521423339844\n",
            "step:109, loss:166.4435577392578\n",
            "step:110, loss:9.620362281799316\n",
            "step:111, loss:3031.61083984375\n",
            "step:112, loss:11.524656295776367\n",
            "step:113, loss:50.225276947021484\n",
            "step:114, loss:9.194683074951172\n",
            "step:115, loss:59.49064636230469\n",
            "step:116, loss:88.65913391113281\n",
            "step:117, loss:61.73780059814453\n",
            "step:118, loss:47.415645599365234\n",
            "step:119, loss:144.9916534423828\n",
            "step:120, loss:22.202808380126953\n",
            "step:121, loss:223.376708984375\n",
            "step:122, loss:15.778514862060547\n",
            "step:123, loss:57.75984573364258\n",
            "step:124, loss:12.834331512451172\n",
            "step:125, loss:14.942842483520508\n",
            "step:126, loss:17.48528289794922\n",
            "step:127, loss:12.96879768371582\n",
            "step:128, loss:837.0478515625\n",
            "step:129, loss:11.282064437866211\n",
            "step:130, loss:11.544179916381836\n",
            "step:131, loss:10.79135513305664\n",
            "step:132, loss:888.9727172851562\n",
            "step:133, loss:42.60235595703125\n",
            "step:134, loss:12.437126159667969\n",
            "step:135, loss:12208.9619140625\n",
            "step:136, loss:18.672700881958008\n",
            "step:137, loss:107.81444549560547\n",
            "step:138, loss:5.101395606994629\n",
            "step:139, loss:8.979933738708496\n",
            "step:140, loss:11.037546157836914\n",
            "step:141, loss:18.255508422851562\n",
            "step:142, loss:25.501203536987305\n",
            "step:143, loss:718.9886474609375\n",
            "step:144, loss:21.18202018737793\n",
            "step:145, loss:104.63827514648438\n",
            "step:146, loss:21.178808212280273\n",
            "step:147, loss:21.744400024414062\n",
            "step:148, loss:221.3502197265625\n",
            "step:149, loss:14.009984970092773\n",
            "step:150, loss:276.0751953125\n",
            "step:151, loss:5.512840747833252\n",
            "step:152, loss:98.80915069580078\n",
            "step:153, loss:13.060264587402344\n",
            "step:154, loss:37.02314758300781\n",
            "step:155, loss:134.63607788085938\n",
            "step:156, loss:15.981620788574219\n",
            "step:157, loss:220.003173828125\n",
            "step:158, loss:35.64304733276367\n",
            "step:159, loss:59.2288818359375\n",
            "step:160, loss:13.224091529846191\n",
            "step:161, loss:42.57463073730469\n",
            "step:162, loss:396.25238037109375\n",
            "step:163, loss:53.063507080078125\n",
            "step:164, loss:22.132259368896484\n",
            "step:165, loss:1581.223876953125\n",
            "step:166, loss:42.78315734863281\n",
            "step:167, loss:5.890494346618652\n",
            "step:168, loss:148.0941925048828\n",
            "step:169, loss:24.45325469970703\n",
            "step:170, loss:21.410205841064453\n",
            "step:171, loss:248.11105346679688\n",
            "step:172, loss:13.14854907989502\n",
            "step:173, loss:7274.59423828125\n",
            "step:174, loss:9.008554458618164\n",
            "step:175, loss:203.32476806640625\n",
            "step:176, loss:20.618938446044922\n",
            "step:177, loss:7607.04638671875\n",
            "step:178, loss:83.19435119628906\n",
            "step:179, loss:4.679248809814453\n",
            "step:180, loss:10.821531295776367\n",
            "step:181, loss:16.550323486328125\n",
            "step:182, loss:217.78558349609375\n",
            "step:183, loss:47.934200286865234\n",
            "step:184, loss:166.40655517578125\n",
            "step:185, loss:69.489990234375\n",
            "step:186, loss:6.491449356079102\n",
            "step:187, loss:60.642757415771484\n",
            "step:188, loss:4.866800308227539\n",
            "step:189, loss:16.785818099975586\n",
            "step:190, loss:112.01567077636719\n",
            "step:191, loss:22.26984977722168\n",
            "step:192, loss:128.3843231201172\n",
            "step:193, loss:207.48440551757812\n",
            "step:194, loss:12.03950309753418\n",
            "step:195, loss:57.61064910888672\n",
            "step:196, loss:28.650304794311523\n",
            "step:197, loss:56.059051513671875\n",
            "step:198, loss:38.37950897216797\n",
            "step:199, loss:12.547889709472656\n",
            "step:200, loss:108.62908935546875\n",
            "step:201, loss:18.178401947021484\n",
            "step:202, loss:389.17938232421875\n",
            "17\n",
            "-----\n",
            "step:0, loss:10.814727783203125\n",
            "step:1, loss:237.98095703125\n",
            "step:2, loss:124.21990966796875\n",
            "step:3, loss:757.7742309570312\n",
            "step:4, loss:107.588134765625\n",
            "step:5, loss:2287.935546875\n",
            "step:6, loss:6.049563884735107\n",
            "step:7, loss:14.116349220275879\n",
            "step:8, loss:56.480926513671875\n",
            "step:9, loss:17.504920959472656\n",
            "step:10, loss:15.407691955566406\n",
            "step:11, loss:38.66557312011719\n",
            "step:12, loss:15.7796630859375\n",
            "step:13, loss:18.535829544067383\n",
            "step:14, loss:24.847469329833984\n",
            "step:15, loss:59.02564239501953\n",
            "step:16, loss:239.1864013671875\n",
            "step:17, loss:47.52737045288086\n",
            "step:18, loss:7.477137565612793\n",
            "step:19, loss:4746.3916015625\n",
            "step:20, loss:30.387035369873047\n",
            "step:21, loss:11.195632934570312\n",
            "step:22, loss:14.916748046875\n",
            "step:23, loss:285.4326477050781\n",
            "step:24, loss:12.10635757446289\n",
            "step:25, loss:1277.3895263671875\n",
            "step:26, loss:20.238325119018555\n",
            "step:27, loss:11.427967071533203\n",
            "step:28, loss:176.54525756835938\n",
            "step:29, loss:73.56571960449219\n",
            "step:30, loss:19.64112663269043\n",
            "step:31, loss:12.280952453613281\n",
            "step:32, loss:12375.0126953125\n",
            "step:33, loss:61.551551818847656\n",
            "step:34, loss:42.41400909423828\n",
            "step:35, loss:48.587059020996094\n",
            "step:36, loss:11.050636291503906\n",
            "step:37, loss:11.458061218261719\n",
            "step:38, loss:7.225987434387207\n",
            "step:39, loss:17.946365356445312\n",
            "step:40, loss:130.38291931152344\n",
            "step:41, loss:10.66213321685791\n",
            "step:42, loss:1714.222900390625\n",
            "step:43, loss:84.27665710449219\n",
            "step:44, loss:22.142057418823242\n",
            "step:45, loss:6.5956621170043945\n",
            "step:46, loss:51.33415985107422\n",
            "step:47, loss:12.559969902038574\n",
            "step:48, loss:286.9099426269531\n",
            "step:49, loss:26.149873733520508\n",
            "step:50, loss:10.709543228149414\n",
            "step:51, loss:6.826350688934326\n",
            "step:52, loss:248.05718994140625\n",
            "step:53, loss:5.216265678405762\n",
            "step:54, loss:94.19702911376953\n",
            "step:55, loss:22.291885375976562\n",
            "step:56, loss:30.84674835205078\n",
            "step:57, loss:10.912969589233398\n",
            "step:58, loss:20.132461547851562\n",
            "step:59, loss:5.913911819458008\n",
            "step:60, loss:23.118877410888672\n",
            "step:61, loss:146.37765502929688\n",
            "step:62, loss:113.30364990234375\n",
            "step:63, loss:559.1551513671875\n",
            "step:64, loss:22.5184268951416\n",
            "step:65, loss:30.232229232788086\n",
            "step:66, loss:23.721132278442383\n",
            "step:67, loss:6.25567102432251\n",
            "step:68, loss:22.47696304321289\n",
            "step:69, loss:128.67222595214844\n",
            "step:70, loss:157.49044799804688\n",
            "step:71, loss:333.83807373046875\n",
            "step:72, loss:19.487186431884766\n",
            "step:73, loss:13.197797775268555\n",
            "step:74, loss:149.6029052734375\n",
            "step:75, loss:15.977359771728516\n",
            "step:76, loss:13.56058120727539\n",
            "step:77, loss:6.479130744934082\n",
            "step:78, loss:22.11489486694336\n",
            "step:79, loss:699.6337280273438\n",
            "step:80, loss:6.372034072875977\n",
            "step:81, loss:25.87571144104004\n",
            "step:82, loss:28.04159927368164\n",
            "step:83, loss:747.1776123046875\n",
            "step:84, loss:149.24215698242188\n",
            "step:85, loss:283.5544738769531\n",
            "step:86, loss:79.9393310546875\n",
            "step:87, loss:22.032188415527344\n",
            "step:88, loss:47.53654479980469\n",
            "step:89, loss:5.371639728546143\n",
            "step:90, loss:14.980504989624023\n",
            "step:91, loss:3.7619261741638184\n",
            "step:92, loss:16.215858459472656\n",
            "step:93, loss:35.42803955078125\n",
            "step:94, loss:18.409570693969727\n",
            "step:95, loss:9.14108657836914\n",
            "step:96, loss:132.8746337890625\n",
            "step:97, loss:31.002674102783203\n",
            "step:98, loss:6.7638397216796875\n",
            "step:99, loss:4.371953964233398\n",
            "step:100, loss:184.3656005859375\n",
            "step:101, loss:274.0827331542969\n",
            "step:102, loss:283.1465148925781\n",
            "step:103, loss:8.37167739868164\n",
            "step:104, loss:197.37002563476562\n",
            "step:105, loss:68.59446716308594\n",
            "step:106, loss:44.60569763183594\n",
            "step:107, loss:10.16605281829834\n",
            "step:108, loss:30.95684242248535\n",
            "step:109, loss:433.1494445800781\n",
            "step:110, loss:9.126696586608887\n",
            "step:111, loss:35.20126724243164\n",
            "step:112, loss:40.918617248535156\n",
            "step:113, loss:14.725356101989746\n",
            "step:114, loss:38.90537643432617\n",
            "step:115, loss:5.234793663024902\n",
            "step:116, loss:60.8476448059082\n",
            "step:117, loss:11.630268096923828\n",
            "step:118, loss:19.127635955810547\n",
            "step:119, loss:41.51289367675781\n",
            "step:120, loss:9.819963455200195\n",
            "step:121, loss:4.6573896408081055\n",
            "step:122, loss:50.980926513671875\n",
            "step:123, loss:9.477859497070312\n",
            "step:124, loss:13.672182083129883\n",
            "step:125, loss:102.15170288085938\n",
            "step:126, loss:21.381309509277344\n",
            "step:127, loss:7.746125221252441\n",
            "step:128, loss:123.98103332519531\n",
            "step:129, loss:2652.90185546875\n",
            "step:130, loss:352.9464111328125\n",
            "step:131, loss:65.44841766357422\n",
            "step:132, loss:12.447759628295898\n",
            "step:133, loss:64.11775207519531\n",
            "step:134, loss:4.4198079109191895\n",
            "step:135, loss:15.752323150634766\n",
            "step:136, loss:7307.6669921875\n",
            "step:137, loss:48.60831069946289\n",
            "step:138, loss:12.097757339477539\n",
            "step:139, loss:82.93511962890625\n",
            "step:140, loss:14.537723541259766\n",
            "step:141, loss:24.880014419555664\n",
            "step:142, loss:74.07722473144531\n",
            "step:143, loss:311.873779296875\n",
            "step:144, loss:27.855697631835938\n",
            "step:145, loss:14.198908805847168\n",
            "step:146, loss:18.518314361572266\n",
            "step:147, loss:8.446313858032227\n",
            "step:148, loss:108.56826782226562\n",
            "step:149, loss:22.424962997436523\n",
            "step:150, loss:18.901588439941406\n",
            "step:151, loss:11.771442413330078\n",
            "step:152, loss:382.548583984375\n",
            "step:153, loss:11.253473281860352\n",
            "step:154, loss:9.891664505004883\n",
            "step:155, loss:91.5956039428711\n",
            "step:156, loss:834.2900390625\n",
            "step:157, loss:22.798969268798828\n",
            "step:158, loss:101.58247375488281\n",
            "step:159, loss:6.773679733276367\n",
            "step:160, loss:12.541265487670898\n",
            "step:161, loss:34.94009017944336\n",
            "step:162, loss:277.18426513671875\n",
            "step:163, loss:7.962869167327881\n",
            "step:164, loss:153.92242431640625\n",
            "step:165, loss:25.568038940429688\n",
            "step:166, loss:20.54281997680664\n",
            "step:167, loss:94.10877227783203\n",
            "step:168, loss:11.603841781616211\n",
            "step:169, loss:124.93309020996094\n",
            "step:170, loss:184.09735107421875\n",
            "step:171, loss:24.89168930053711\n",
            "step:172, loss:100.79280090332031\n",
            "step:173, loss:51.08023452758789\n",
            "step:174, loss:201.27906799316406\n",
            "step:175, loss:73.47018432617188\n",
            "step:176, loss:15.978071212768555\n",
            "step:177, loss:7.279537677764893\n",
            "step:178, loss:85.25837707519531\n",
            "step:179, loss:69.41062927246094\n",
            "step:180, loss:7380.13427734375\n",
            "step:181, loss:100.86762237548828\n",
            "step:182, loss:33.571598052978516\n",
            "step:183, loss:89.91011047363281\n",
            "step:184, loss:85.81437683105469\n",
            "step:185, loss:83.68904876708984\n",
            "step:186, loss:36.777915954589844\n",
            "step:187, loss:38.38451385498047\n",
            "step:188, loss:69.98420715332031\n",
            "step:189, loss:6.598921775817871\n",
            "step:190, loss:11.252805709838867\n",
            "step:191, loss:36.337257385253906\n",
            "step:192, loss:175.8335418701172\n",
            "step:193, loss:13.989005088806152\n",
            "step:194, loss:6.43323278427124\n",
            "step:195, loss:9.174522399902344\n",
            "step:196, loss:291.4322814941406\n",
            "step:197, loss:52.70891571044922\n",
            "step:198, loss:13.621702194213867\n",
            "step:199, loss:5.286685943603516\n",
            "step:200, loss:73.68966674804688\n",
            "step:201, loss:8.42923355102539\n",
            "step:202, loss:313.53228759765625\n",
            "18\n",
            "-----\n",
            "step:0, loss:88.20611572265625\n",
            "step:1, loss:5.165815353393555\n",
            "step:2, loss:5.602692604064941\n",
            "step:3, loss:5.404820442199707\n",
            "step:4, loss:13.721399307250977\n",
            "step:5, loss:13.263381958007812\n",
            "step:6, loss:17.599531173706055\n",
            "step:7, loss:17.468780517578125\n",
            "step:8, loss:2.8760805130004883\n",
            "step:9, loss:18.229595184326172\n",
            "step:10, loss:10.500450134277344\n",
            "step:11, loss:71.8765640258789\n",
            "step:12, loss:36.998802185058594\n",
            "step:13, loss:33.52614974975586\n",
            "step:14, loss:28.509387969970703\n",
            "step:15, loss:42.48976135253906\n",
            "step:16, loss:7.322922229766846\n",
            "step:17, loss:117.32489013671875\n",
            "step:18, loss:27.79351043701172\n",
            "step:19, loss:22.98225975036621\n",
            "step:20, loss:194.93060302734375\n",
            "step:21, loss:41.48844909667969\n",
            "step:22, loss:90.8244857788086\n",
            "step:23, loss:14.76693344116211\n",
            "step:24, loss:9.696636199951172\n",
            "step:25, loss:122.41403198242188\n",
            "step:26, loss:23.301576614379883\n",
            "step:27, loss:35.77859878540039\n",
            "step:28, loss:14.52334976196289\n",
            "step:29, loss:7.9677042961120605\n",
            "step:30, loss:13.826284408569336\n",
            "step:31, loss:69.99290466308594\n",
            "step:32, loss:8.843987464904785\n",
            "step:33, loss:5.206236839294434\n",
            "step:34, loss:85.72068786621094\n",
            "step:35, loss:680.0762329101562\n",
            "step:36, loss:4.473801612854004\n",
            "step:37, loss:45.66270065307617\n",
            "step:38, loss:126.97246551513672\n",
            "step:39, loss:242.0526885986328\n",
            "step:40, loss:15.522265434265137\n",
            "step:41, loss:109.80087280273438\n",
            "step:42, loss:46.57171630859375\n",
            "step:43, loss:14.27983570098877\n",
            "step:44, loss:6.760656356811523\n",
            "step:45, loss:6.505405902862549\n",
            "step:46, loss:16.594995498657227\n",
            "step:47, loss:20.05904769897461\n",
            "step:48, loss:8.126997947692871\n",
            "step:49, loss:107.15365600585938\n",
            "step:50, loss:63.529273986816406\n",
            "step:51, loss:2736.81787109375\n",
            "step:52, loss:48.363983154296875\n",
            "step:53, loss:49.003013610839844\n",
            "step:54, loss:6.347896099090576\n",
            "step:55, loss:694.2723388671875\n",
            "step:56, loss:144.72195434570312\n",
            "step:57, loss:5.726074695587158\n",
            "step:58, loss:1248.0477294921875\n",
            "step:59, loss:201.2740020751953\n",
            "step:60, loss:1105.753173828125\n",
            "step:61, loss:64.53897094726562\n",
            "step:62, loss:184.9969482421875\n",
            "step:63, loss:694.3226318359375\n",
            "step:64, loss:12.388035774230957\n",
            "step:65, loss:22.342700958251953\n",
            "step:66, loss:6.792916297912598\n",
            "step:67, loss:26.545490264892578\n",
            "step:68, loss:28.045114517211914\n",
            "step:69, loss:12129.501953125\n",
            "step:70, loss:15.434656143188477\n",
            "step:71, loss:8.733333587646484\n",
            "step:72, loss:4.259376525878906\n",
            "step:73, loss:20.188337326049805\n",
            "step:74, loss:4.037290573120117\n",
            "step:75, loss:51.59143829345703\n",
            "step:76, loss:10.421533584594727\n",
            "step:77, loss:237.12896728515625\n",
            "step:78, loss:16.55599021911621\n",
            "step:79, loss:8.755575180053711\n",
            "step:80, loss:132.7053985595703\n",
            "step:81, loss:16.713382720947266\n",
            "step:82, loss:7.41587495803833\n",
            "step:83, loss:11.809091567993164\n",
            "step:84, loss:119.33601379394531\n",
            "step:85, loss:57.29398727416992\n",
            "step:86, loss:4.835855484008789\n",
            "step:87, loss:250.20556640625\n",
            "step:88, loss:335.0108642578125\n",
            "step:89, loss:17.31265640258789\n",
            "step:90, loss:30.899869918823242\n",
            "step:91, loss:85.92869567871094\n",
            "step:92, loss:6.050584316253662\n",
            "step:93, loss:354.5980224609375\n",
            "step:94, loss:231.04751586914062\n",
            "step:95, loss:35.329219818115234\n",
            "step:96, loss:221.036376953125\n",
            "step:97, loss:5.771402359008789\n",
            "step:98, loss:16.119827270507812\n",
            "step:99, loss:8.323158264160156\n",
            "step:100, loss:1712.3326416015625\n",
            "step:101, loss:539.9493408203125\n",
            "step:102, loss:5.493011474609375\n",
            "step:103, loss:17.120790481567383\n",
            "step:104, loss:21.70566177368164\n",
            "step:105, loss:22.833961486816406\n",
            "step:106, loss:37.487972259521484\n",
            "step:107, loss:28.04065704345703\n",
            "step:108, loss:16.439369201660156\n",
            "step:109, loss:115.47840881347656\n",
            "step:110, loss:60.44051742553711\n",
            "step:111, loss:11.897076606750488\n",
            "step:112, loss:7331.263671875\n",
            "step:113, loss:187.52732849121094\n",
            "step:114, loss:286.04974365234375\n",
            "step:115, loss:96.46012878417969\n",
            "step:116, loss:20.439420700073242\n",
            "step:117, loss:258.4381103515625\n",
            "step:118, loss:6.250557899475098\n",
            "step:119, loss:15.14645004272461\n",
            "step:120, loss:59.090789794921875\n",
            "step:121, loss:10.564984321594238\n",
            "step:122, loss:14.043573379516602\n",
            "step:123, loss:124.48774719238281\n",
            "step:124, loss:13.615384101867676\n",
            "step:125, loss:11869.638671875\n",
            "step:126, loss:55.388668060302734\n",
            "step:127, loss:16.056537628173828\n",
            "step:128, loss:43.38569641113281\n",
            "step:129, loss:10.34250259399414\n",
            "step:130, loss:89.56771087646484\n",
            "step:131, loss:52.2371940612793\n",
            "step:132, loss:9.621773719787598\n",
            "step:133, loss:26.201026916503906\n",
            "step:134, loss:5.897858619689941\n",
            "step:135, loss:179.95645141601562\n",
            "step:136, loss:1528.7994384765625\n",
            "step:137, loss:8.393489837646484\n",
            "step:138, loss:283.83673095703125\n",
            "step:139, loss:87.84523010253906\n",
            "step:140, loss:40.294395446777344\n",
            "step:141, loss:102.18659210205078\n",
            "step:142, loss:135.4018096923828\n",
            "step:143, loss:22.25618553161621\n",
            "step:144, loss:4.980642795562744\n",
            "step:145, loss:6.026664733886719\n",
            "step:146, loss:80.06283569335938\n",
            "step:147, loss:31.929229736328125\n",
            "step:148, loss:5.427506923675537\n",
            "step:149, loss:8.757234573364258\n",
            "step:150, loss:22.48249053955078\n",
            "step:151, loss:1530.1820068359375\n",
            "step:152, loss:9.285634994506836\n",
            "step:153, loss:25.020265579223633\n",
            "step:154, loss:10.867000579833984\n",
            "step:155, loss:16.472305297851562\n",
            "step:156, loss:27.257558822631836\n",
            "step:157, loss:24.730335235595703\n",
            "step:158, loss:219.3341827392578\n",
            "step:159, loss:56.04532241821289\n",
            "step:160, loss:5.930100440979004\n",
            "step:161, loss:160.30447387695312\n",
            "step:162, loss:36.44899368286133\n",
            "step:163, loss:15.299921035766602\n",
            "step:164, loss:391.6712951660156\n",
            "step:165, loss:31.642276763916016\n",
            "step:166, loss:155.11080932617188\n",
            "step:167, loss:17.786678314208984\n",
            "step:168, loss:48.58586883544922\n",
            "step:169, loss:31.014419555664062\n",
            "step:170, loss:9.77076530456543\n",
            "step:171, loss:32.38530349731445\n",
            "step:172, loss:6.6746320724487305\n",
            "step:173, loss:4.182136535644531\n",
            "step:174, loss:92.63279724121094\n",
            "step:175, loss:81.40397644042969\n",
            "step:176, loss:106.05229187011719\n",
            "step:177, loss:11.301700592041016\n",
            "step:178, loss:8.361623764038086\n",
            "step:179, loss:242.48814392089844\n",
            "step:180, loss:18.4455623626709\n",
            "step:181, loss:113.74546813964844\n",
            "step:182, loss:8.867565155029297\n",
            "step:183, loss:7.972663402557373\n",
            "step:184, loss:289.23822021484375\n",
            "step:185, loss:28.763729095458984\n",
            "step:186, loss:151.63841247558594\n",
            "step:187, loss:12.215200424194336\n",
            "step:188, loss:17.784683227539062\n",
            "step:189, loss:11.62038516998291\n",
            "step:190, loss:14.125588417053223\n",
            "step:191, loss:4.702046871185303\n",
            "step:192, loss:87.53594207763672\n",
            "step:193, loss:12.635233879089355\n",
            "step:194, loss:38.32469940185547\n",
            "step:195, loss:28.557327270507812\n",
            "step:196, loss:25.38054656982422\n",
            "step:197, loss:43.83967590332031\n",
            "step:198, loss:141.86729431152344\n",
            "step:199, loss:9.739447593688965\n",
            "step:200, loss:10.962528228759766\n",
            "step:201, loss:26.24664306640625\n",
            "step:202, loss:324.29931640625\n",
            "19\n",
            "-----\n",
            "step:0, loss:14.867366790771484\n",
            "step:1, loss:55.97948455810547\n",
            "step:2, loss:353.62646484375\n",
            "step:3, loss:20.517637252807617\n",
            "step:4, loss:11.177165985107422\n",
            "step:5, loss:19.171810150146484\n",
            "step:6, loss:30.47420310974121\n",
            "step:7, loss:45.545719146728516\n",
            "step:8, loss:186.32217407226562\n",
            "step:9, loss:5.592214584350586\n",
            "step:10, loss:2649.766845703125\n",
            "step:11, loss:389.6453552246094\n",
            "step:12, loss:11.263287544250488\n",
            "step:13, loss:25.313926696777344\n",
            "step:14, loss:58.232234954833984\n",
            "step:15, loss:19.51523208618164\n",
            "step:16, loss:108.90896606445312\n",
            "step:17, loss:51.422401428222656\n",
            "step:18, loss:67.45984649658203\n",
            "step:19, loss:9.647680282592773\n",
            "step:20, loss:207.2285614013672\n",
            "step:21, loss:26.017620086669922\n",
            "step:22, loss:12.616758346557617\n",
            "step:23, loss:4.545248508453369\n",
            "step:24, loss:27.205486297607422\n",
            "step:25, loss:4.1133270263671875\n",
            "step:26, loss:9.27315616607666\n",
            "step:27, loss:7.755162239074707\n",
            "step:28, loss:36.932342529296875\n",
            "step:29, loss:404.41461181640625\n",
            "step:30, loss:79.15972900390625\n",
            "step:31, loss:4.006080150604248\n",
            "step:32, loss:39.83904266357422\n",
            "step:33, loss:6.693964958190918\n",
            "step:34, loss:1531.5340576171875\n",
            "step:35, loss:14.779515266418457\n",
            "step:36, loss:4.143511772155762\n",
            "step:37, loss:28.5401611328125\n",
            "step:38, loss:4386.79052734375\n",
            "step:39, loss:11.300214767456055\n",
            "step:40, loss:12.55795955657959\n",
            "step:41, loss:10.859976768493652\n",
            "step:42, loss:9.66441535949707\n",
            "step:43, loss:13.207365989685059\n",
            "step:44, loss:6.893500328063965\n",
            "step:45, loss:58.2999382019043\n",
            "step:46, loss:56.13186264038086\n",
            "step:47, loss:11.849271774291992\n",
            "step:48, loss:66.13610076904297\n",
            "step:49, loss:195.4228515625\n",
            "step:50, loss:36.692481994628906\n",
            "step:51, loss:6.598848342895508\n",
            "step:52, loss:7398.07080078125\n",
            "step:53, loss:4.378269195556641\n",
            "step:54, loss:10.259796142578125\n",
            "step:55, loss:6.451296806335449\n",
            "step:56, loss:745.0756225585938\n",
            "step:57, loss:273.3970642089844\n",
            "step:58, loss:13.459818840026855\n",
            "step:59, loss:95.462890625\n",
            "step:60, loss:4.992523193359375\n",
            "step:61, loss:62.71250915527344\n",
            "step:62, loss:32.58234405517578\n",
            "step:63, loss:9.956869125366211\n",
            "step:64, loss:5.810428619384766\n",
            "step:65, loss:37.69686508178711\n",
            "step:66, loss:10.022989273071289\n",
            "step:67, loss:350.2254333496094\n",
            "step:68, loss:7.859673976898193\n",
            "step:69, loss:6.051967620849609\n",
            "step:70, loss:10.043581008911133\n",
            "step:71, loss:4.089137077331543\n",
            "step:72, loss:1150.6785888671875\n",
            "step:73, loss:8.597408294677734\n",
            "step:74, loss:199.1531982421875\n",
            "step:75, loss:9.984939575195312\n",
            "step:76, loss:4.583107948303223\n",
            "step:77, loss:36.39237976074219\n",
            "step:78, loss:12.306355476379395\n",
            "step:79, loss:690.9976806640625\n",
            "step:80, loss:9.679051399230957\n",
            "step:81, loss:8.457686424255371\n",
            "step:82, loss:43.71758270263672\n",
            "step:83, loss:23.205663681030273\n",
            "step:84, loss:14.276302337646484\n",
            "step:85, loss:3.8441712856292725\n",
            "step:86, loss:60.798728942871094\n",
            "step:87, loss:25.85432243347168\n",
            "step:88, loss:6.209074974060059\n",
            "step:89, loss:15.692667007446289\n",
            "step:90, loss:2151.9619140625\n",
            "step:91, loss:17.330137252807617\n",
            "step:92, loss:330.385498046875\n",
            "step:93, loss:34.03830337524414\n",
            "step:94, loss:351.7095642089844\n",
            "step:95, loss:25.154571533203125\n",
            "step:96, loss:25.14629364013672\n",
            "step:97, loss:97.6310043334961\n",
            "step:98, loss:198.82315063476562\n",
            "step:99, loss:230.6103973388672\n",
            "step:100, loss:20.554006576538086\n",
            "step:101, loss:182.69961547851562\n",
            "step:102, loss:5.5479888916015625\n",
            "step:103, loss:97.79537963867188\n",
            "step:104, loss:14.263612747192383\n",
            "step:105, loss:75.78133392333984\n",
            "step:106, loss:708.3404541015625\n",
            "step:107, loss:17.114830017089844\n",
            "step:108, loss:196.0379180908203\n",
            "step:109, loss:132.7036590576172\n",
            "step:110, loss:88.81990051269531\n",
            "step:111, loss:42.1343879699707\n",
            "step:112, loss:9.41796588897705\n",
            "step:113, loss:104.06144714355469\n",
            "step:114, loss:7.058653831481934\n",
            "step:115, loss:43.30792999267578\n",
            "step:116, loss:179.95700073242188\n",
            "step:117, loss:9.419492721557617\n",
            "step:118, loss:27.683204650878906\n",
            "step:119, loss:14.535988807678223\n",
            "step:120, loss:15.209159851074219\n",
            "step:121, loss:42.90693283081055\n",
            "step:122, loss:51.9111328125\n",
            "step:123, loss:21.941041946411133\n",
            "step:124, loss:23.753276824951172\n",
            "step:125, loss:22.61571502685547\n",
            "step:126, loss:18.675764083862305\n",
            "step:127, loss:7.274775505065918\n",
            "step:128, loss:85.19854736328125\n",
            "step:129, loss:167.6149444580078\n",
            "step:130, loss:4.434726238250732\n",
            "step:131, loss:3.970600128173828\n",
            "step:132, loss:12.230152130126953\n",
            "step:133, loss:55.53914260864258\n",
            "step:134, loss:182.93182373046875\n",
            "step:135, loss:12.226852416992188\n",
            "step:136, loss:45.83188247680664\n",
            "step:137, loss:4.006435394287109\n",
            "step:138, loss:12.875789642333984\n",
            "step:139, loss:164.4562225341797\n",
            "step:140, loss:6.390168190002441\n",
            "step:141, loss:8.709498405456543\n",
            "step:142, loss:43.283226013183594\n",
            "step:143, loss:49.33783721923828\n",
            "step:144, loss:131.01272583007812\n",
            "step:145, loss:18.99448013305664\n",
            "step:146, loss:12.543512344360352\n",
            "step:147, loss:175.9163818359375\n",
            "step:148, loss:12.030145645141602\n",
            "step:149, loss:6.214755535125732\n",
            "step:150, loss:106.5827407836914\n",
            "step:151, loss:246.30502319335938\n",
            "step:152, loss:30.259918212890625\n",
            "step:153, loss:16.711410522460938\n",
            "step:154, loss:79.88319396972656\n",
            "step:155, loss:305.0159606933594\n",
            "step:156, loss:21.514633178710938\n",
            "step:157, loss:15.253192901611328\n",
            "step:158, loss:10.129020690917969\n",
            "step:159, loss:6.005798816680908\n",
            "step:160, loss:834.267822265625\n",
            "step:161, loss:5.756230354309082\n",
            "step:162, loss:299.88897705078125\n",
            "step:163, loss:21.810546875\n",
            "step:164, loss:311.083251953125\n",
            "step:165, loss:4.338044166564941\n",
            "step:166, loss:135.37869262695312\n",
            "step:167, loss:15.894262313842773\n",
            "step:168, loss:3.1212291717529297\n",
            "step:169, loss:13.664698600769043\n",
            "step:170, loss:46.818702697753906\n",
            "step:171, loss:32.86794662475586\n",
            "step:172, loss:7259.37646484375\n",
            "step:173, loss:7.2803425788879395\n",
            "step:174, loss:10.267535209655762\n",
            "step:175, loss:13.750300407409668\n",
            "step:176, loss:11.679567337036133\n",
            "step:177, loss:146.46768188476562\n",
            "step:178, loss:38.19777297973633\n",
            "step:179, loss:77.93891906738281\n",
            "step:180, loss:44.70172882080078\n",
            "step:181, loss:199.02426147460938\n",
            "step:182, loss:706.2317504882812\n",
            "step:183, loss:43.04774856567383\n",
            "step:184, loss:44.44757080078125\n",
            "step:185, loss:3.942721128463745\n",
            "step:186, loss:114.52129364013672\n",
            "step:187, loss:2.99733567237854\n",
            "step:188, loss:20.003061294555664\n",
            "step:189, loss:223.94583129882812\n",
            "step:190, loss:87.26346588134766\n",
            "step:191, loss:17.953208923339844\n",
            "step:192, loss:15.810022354125977\n",
            "step:193, loss:96.8006362915039\n",
            "step:194, loss:12067.720703125\n",
            "step:195, loss:19.846885681152344\n",
            "step:196, loss:44.46058654785156\n",
            "step:197, loss:9.439737319946289\n",
            "step:198, loss:70.74403381347656\n",
            "step:199, loss:11.390989303588867\n",
            "step:200, loss:28.30716323852539\n",
            "step:201, loss:4.889406204223633\n",
            "step:202, loss:7.445476531982422\n",
            "20\n",
            "-----\n",
            "step:0, loss:40.177284240722656\n",
            "step:1, loss:387.8546142578125\n",
            "step:2, loss:21.500734329223633\n",
            "step:3, loss:254.89291381835938\n",
            "step:4, loss:23.650405883789062\n",
            "step:5, loss:9.558392524719238\n",
            "step:6, loss:88.17731475830078\n",
            "step:7, loss:9.71551513671875\n",
            "step:8, loss:51.854576110839844\n",
            "step:9, loss:6.486361503601074\n",
            "step:10, loss:104.88153839111328\n",
            "step:11, loss:93.33167266845703\n",
            "step:12, loss:13.740930557250977\n",
            "step:13, loss:62.98922348022461\n",
            "step:14, loss:9.375297546386719\n",
            "step:15, loss:8.441276550292969\n",
            "step:16, loss:693.950927734375\n",
            "step:17, loss:7.093708038330078\n",
            "step:18, loss:20.830257415771484\n",
            "step:19, loss:5.08997917175293\n",
            "step:20, loss:1507.09130859375\n",
            "step:21, loss:222.91770935058594\n",
            "step:22, loss:18.805683135986328\n",
            "step:23, loss:43.94316101074219\n",
            "step:24, loss:11.100045204162598\n",
            "step:25, loss:22.517051696777344\n",
            "step:26, loss:74.98199462890625\n",
            "step:27, loss:14.184308052062988\n",
            "step:28, loss:11.16671371459961\n",
            "step:29, loss:12.529923439025879\n",
            "step:30, loss:7.949298858642578\n",
            "step:31, loss:58.509033203125\n",
            "step:32, loss:62.44076919555664\n",
            "step:33, loss:3.7438488006591797\n",
            "step:34, loss:20.218294143676758\n",
            "step:35, loss:37.4898681640625\n",
            "step:36, loss:11.90906810760498\n",
            "step:37, loss:24.825794219970703\n",
            "step:38, loss:5.891267776489258\n",
            "step:39, loss:86.08531951904297\n",
            "step:40, loss:101.89969635009766\n",
            "step:41, loss:8.044492721557617\n",
            "step:42, loss:12249.5869140625\n",
            "step:43, loss:18.71619987487793\n",
            "step:44, loss:700.5484619140625\n",
            "step:45, loss:102.61536407470703\n",
            "step:46, loss:8.209545135498047\n",
            "step:47, loss:7191.60595703125\n",
            "step:48, loss:12.629714012145996\n",
            "step:49, loss:91.92948150634766\n",
            "step:50, loss:568.8934936523438\n",
            "step:51, loss:763.7999267578125\n",
            "step:52, loss:6.081045150756836\n",
            "step:53, loss:6.440103530883789\n",
            "step:54, loss:19.81182098388672\n",
            "step:55, loss:18.029953002929688\n",
            "step:56, loss:238.88217163085938\n",
            "step:57, loss:24.68643569946289\n",
            "step:58, loss:4365.4970703125\n",
            "step:59, loss:3.1814122200012207\n",
            "step:60, loss:210.96170043945312\n",
            "step:61, loss:12.275236129760742\n",
            "step:62, loss:7328.78857421875\n",
            "step:63, loss:14.174385070800781\n",
            "step:64, loss:10.87021541595459\n",
            "step:65, loss:9.224418640136719\n",
            "step:66, loss:45.866336822509766\n",
            "step:67, loss:75.61014556884766\n",
            "step:68, loss:9.418274879455566\n",
            "step:69, loss:39.6065559387207\n",
            "step:70, loss:20.8565673828125\n",
            "step:71, loss:19.037616729736328\n",
            "step:72, loss:4.703627586364746\n",
            "step:73, loss:4.716917037963867\n",
            "step:74, loss:18.146747589111328\n",
            "step:75, loss:5.656516075134277\n",
            "step:76, loss:9.591937065124512\n",
            "step:77, loss:16.971166610717773\n",
            "step:78, loss:71.61807250976562\n",
            "step:79, loss:62.916419982910156\n",
            "step:80, loss:67.78627014160156\n",
            "step:81, loss:11.967201232910156\n",
            "step:82, loss:7.068774700164795\n",
            "step:83, loss:5.667191028594971\n",
            "step:84, loss:6.167474746704102\n",
            "step:85, loss:251.85108947753906\n",
            "step:86, loss:3.9966683387756348\n",
            "step:87, loss:39.81727600097656\n",
            "step:88, loss:155.28956604003906\n",
            "step:89, loss:4.832088947296143\n",
            "step:90, loss:9.578763961791992\n",
            "step:91, loss:75.98204803466797\n",
            "step:92, loss:11.642730712890625\n",
            "step:93, loss:4.213794708251953\n",
            "step:94, loss:419.43072509765625\n",
            "step:95, loss:17.897611618041992\n",
            "step:96, loss:22.256187438964844\n",
            "step:97, loss:15.35079288482666\n",
            "step:98, loss:7.134537220001221\n",
            "step:99, loss:186.183349609375\n",
            "step:100, loss:25.856882095336914\n",
            "step:101, loss:151.2100372314453\n",
            "step:102, loss:97.47014617919922\n",
            "step:103, loss:10.223795890808105\n",
            "step:104, loss:842.56201171875\n",
            "step:105, loss:2593.529296875\n",
            "step:106, loss:152.4567108154297\n",
            "step:107, loss:23.276874542236328\n",
            "step:108, loss:73.0788345336914\n",
            "step:109, loss:7.6933417320251465\n",
            "step:110, loss:62.6571044921875\n",
            "step:111, loss:41.69516372680664\n",
            "step:112, loss:1652.3822021484375\n",
            "step:113, loss:13.269749641418457\n",
            "step:114, loss:213.19540405273438\n",
            "step:115, loss:26.94990348815918\n",
            "step:116, loss:30.092037200927734\n",
            "step:117, loss:4.829718589782715\n",
            "step:118, loss:11.84223747253418\n",
            "step:119, loss:6.4363203048706055\n",
            "step:120, loss:11.987403869628906\n",
            "step:121, loss:5.774795055389404\n",
            "step:122, loss:9.289372444152832\n",
            "step:123, loss:95.86315155029297\n",
            "step:124, loss:32.97454833984375\n",
            "step:125, loss:336.74407958984375\n",
            "step:126, loss:16.72903823852539\n",
            "step:127, loss:14.209626197814941\n",
            "step:128, loss:12.608575820922852\n",
            "step:129, loss:64.17705535888672\n",
            "step:130, loss:3.2956175804138184\n",
            "step:131, loss:6.780179977416992\n",
            "step:132, loss:202.32406616210938\n",
            "step:133, loss:7.697835922241211\n",
            "step:134, loss:197.00576782226562\n",
            "step:135, loss:22.18096923828125\n",
            "step:136, loss:375.0939636230469\n",
            "step:137, loss:41.9659538269043\n",
            "step:138, loss:5.509086608886719\n",
            "step:139, loss:12.771510124206543\n",
            "step:140, loss:27.579330444335938\n",
            "step:141, loss:44.98594665527344\n",
            "step:142, loss:123.86994934082031\n",
            "step:143, loss:24.106536865234375\n",
            "step:144, loss:3.0701000690460205\n",
            "step:145, loss:195.5488739013672\n",
            "step:146, loss:247.27316284179688\n",
            "step:147, loss:46.66834259033203\n",
            "step:148, loss:24.606266021728516\n",
            "step:149, loss:4.534311294555664\n",
            "step:150, loss:51.544071197509766\n",
            "step:151, loss:96.35398864746094\n",
            "step:152, loss:28.89437484741211\n",
            "step:153, loss:15.205735206604004\n",
            "step:154, loss:56.39786148071289\n",
            "step:155, loss:31.98201560974121\n",
            "step:156, loss:17.896560668945312\n",
            "step:157, loss:5.959072113037109\n",
            "step:158, loss:7.332945823669434\n",
            "step:159, loss:13.861806869506836\n",
            "step:160, loss:11.607828140258789\n",
            "step:161, loss:23.93402099609375\n",
            "step:162, loss:29.009984970092773\n",
            "step:163, loss:2.276665449142456\n",
            "step:164, loss:94.79324340820312\n",
            "step:165, loss:169.01400756835938\n",
            "step:166, loss:10.829680442810059\n",
            "step:167, loss:344.3514099121094\n",
            "step:168, loss:34.84222412109375\n",
            "step:169, loss:9.421880722045898\n",
            "step:170, loss:23.273914337158203\n",
            "step:171, loss:26.922218322753906\n",
            "step:172, loss:20.715614318847656\n",
            "step:173, loss:6.172364711761475\n",
            "step:174, loss:311.4408874511719\n",
            "step:175, loss:1102.4913330078125\n",
            "step:176, loss:78.79531860351562\n",
            "step:177, loss:45.18829345703125\n",
            "step:178, loss:19.012222290039062\n",
            "step:179, loss:503.9752197265625\n",
            "step:180, loss:35.95500946044922\n",
            "step:181, loss:19.049636840820312\n",
            "step:182, loss:79.99089813232422\n",
            "step:183, loss:59.92238235473633\n",
            "step:184, loss:33.18377685546875\n",
            "step:185, loss:9.102049827575684\n",
            "step:186, loss:188.31756591796875\n",
            "step:187, loss:7.060985565185547\n",
            "step:188, loss:16.40890121459961\n",
            "step:189, loss:7.355879306793213\n",
            "step:190, loss:10.407974243164062\n",
            "step:191, loss:18.730005264282227\n",
            "step:192, loss:5.570583820343018\n",
            "step:193, loss:16.72430419921875\n",
            "step:194, loss:11.590584754943848\n",
            "step:195, loss:12.28155517578125\n",
            "step:196, loss:671.2742309570312\n",
            "step:197, loss:28.503820419311523\n",
            "step:198, loss:80.24707794189453\n",
            "step:199, loss:108.28874969482422\n",
            "step:200, loss:13.731185913085938\n",
            "step:201, loss:8.461077690124512\n",
            "step:202, loss:4.202576637268066\n",
            "21\n",
            "-----\n",
            "step:0, loss:3.027440071105957\n",
            "step:1, loss:21.60576629638672\n",
            "step:2, loss:14.660964012145996\n",
            "step:3, loss:5.634404182434082\n",
            "step:4, loss:12.01730728149414\n",
            "step:5, loss:38.5917854309082\n",
            "step:6, loss:8.937687873840332\n",
            "step:7, loss:5.043571472167969\n",
            "step:8, loss:16.127412796020508\n",
            "step:9, loss:10.88315486907959\n",
            "step:10, loss:787.97119140625\n",
            "step:11, loss:1635.4425048828125\n",
            "step:12, loss:185.36801147460938\n",
            "step:13, loss:7.734345436096191\n",
            "step:14, loss:179.6357421875\n",
            "step:15, loss:9.910810470581055\n",
            "step:16, loss:212.15635681152344\n",
            "step:17, loss:33.368194580078125\n",
            "step:18, loss:254.68077087402344\n",
            "step:19, loss:228.1835479736328\n",
            "step:20, loss:45.87894058227539\n",
            "step:21, loss:94.63388061523438\n",
            "step:22, loss:6.911068439483643\n",
            "step:23, loss:47.71012878417969\n",
            "step:24, loss:15.390871047973633\n",
            "step:25, loss:6.437840461730957\n",
            "step:26, loss:26.5611629486084\n",
            "step:27, loss:7.734194278717041\n",
            "step:28, loss:18.894691467285156\n",
            "step:29, loss:67.63835144042969\n",
            "step:30, loss:8.073201179504395\n",
            "step:31, loss:41.56262969970703\n",
            "step:32, loss:101.31836700439453\n",
            "step:33, loss:6.24977445602417\n",
            "step:34, loss:251.7982177734375\n",
            "step:35, loss:14.876895904541016\n",
            "step:36, loss:10.960354804992676\n",
            "step:37, loss:4.658926010131836\n",
            "step:38, loss:11.26729965209961\n",
            "step:39, loss:142.97581481933594\n",
            "step:40, loss:16.02981948852539\n",
            "step:41, loss:4.905887603759766\n",
            "step:42, loss:198.50640869140625\n",
            "step:43, loss:50.786643981933594\n",
            "step:44, loss:66.14041137695312\n",
            "step:45, loss:6.564576148986816\n",
            "step:46, loss:7.050859451293945\n",
            "step:47, loss:64.68968963623047\n",
            "step:48, loss:16.673368453979492\n",
            "step:49, loss:115.37959289550781\n",
            "step:50, loss:27.665802001953125\n",
            "step:51, loss:6.958338737487793\n",
            "step:52, loss:71.78971862792969\n",
            "step:53, loss:56.42753601074219\n",
            "step:54, loss:65.93871307373047\n",
            "step:55, loss:5.155284881591797\n",
            "step:56, loss:5.200505256652832\n",
            "step:57, loss:4.922270774841309\n",
            "step:58, loss:21.941951751708984\n",
            "step:59, loss:13.643111228942871\n",
            "step:60, loss:38.89847946166992\n",
            "step:61, loss:162.88768005371094\n",
            "step:62, loss:12.138187408447266\n",
            "step:63, loss:7294.14599609375\n",
            "step:64, loss:4291.3984375\n",
            "step:65, loss:44.03008270263672\n",
            "step:66, loss:7.4033918380737305\n",
            "step:67, loss:43.34373474121094\n",
            "step:68, loss:3.7562830448150635\n",
            "step:69, loss:8.683526992797852\n",
            "step:70, loss:2.4786367416381836\n",
            "step:71, loss:7.136762619018555\n",
            "step:72, loss:115.41812133789062\n",
            "step:73, loss:395.34014892578125\n",
            "step:74, loss:5.764464855194092\n",
            "step:75, loss:32.090309143066406\n",
            "step:76, loss:3.6831936836242676\n",
            "step:77, loss:124.09040069580078\n",
            "step:78, loss:87.64521789550781\n",
            "step:79, loss:13.57144546508789\n",
            "step:80, loss:599.8652954101562\n",
            "step:81, loss:32.487918853759766\n",
            "step:82, loss:4.815415382385254\n",
            "step:83, loss:79.96424102783203\n",
            "step:84, loss:92.37760925292969\n",
            "step:85, loss:63.775726318359375\n",
            "step:86, loss:4.650610446929932\n",
            "step:87, loss:5.6777825355529785\n",
            "step:88, loss:3.9870331287384033\n",
            "step:89, loss:6.850235939025879\n",
            "step:90, loss:105.3699951171875\n",
            "step:91, loss:5.5766825675964355\n",
            "step:92, loss:6.83183479309082\n",
            "step:93, loss:4.620044708251953\n",
            "step:94, loss:23.223600387573242\n",
            "step:95, loss:13.106904983520508\n",
            "step:96, loss:25.50925636291504\n",
            "step:97, loss:261.1075134277344\n",
            "step:98, loss:393.4034423828125\n",
            "step:99, loss:8.806221961975098\n",
            "step:100, loss:39.13534164428711\n",
            "step:101, loss:6.5706000328063965\n",
            "step:102, loss:396.6265869140625\n",
            "step:103, loss:50.338829040527344\n",
            "step:104, loss:8.206862449645996\n",
            "step:105, loss:7.221719741821289\n",
            "step:106, loss:22.706857681274414\n",
            "step:107, loss:269.33966064453125\n",
            "step:108, loss:12.461686134338379\n",
            "step:109, loss:44.081302642822266\n",
            "step:110, loss:771.958740234375\n",
            "step:111, loss:21.183063507080078\n",
            "step:112, loss:95.86827850341797\n",
            "step:113, loss:10.523460388183594\n",
            "step:114, loss:7.104549407958984\n",
            "step:115, loss:44.21922302246094\n",
            "step:116, loss:7.204339027404785\n",
            "step:117, loss:4.2170729637146\n",
            "step:118, loss:11998.908203125\n",
            "step:119, loss:799.238525390625\n",
            "step:120, loss:7.071991443634033\n",
            "step:121, loss:4.838112831115723\n",
            "step:122, loss:28.19151496887207\n",
            "step:123, loss:350.7134704589844\n",
            "step:124, loss:87.65365600585938\n",
            "step:125, loss:22.14407730102539\n",
            "step:126, loss:22.634639739990234\n",
            "step:127, loss:6.645351409912109\n",
            "step:128, loss:172.50762939453125\n",
            "step:129, loss:20.81151580810547\n",
            "step:130, loss:1503.754150390625\n",
            "step:131, loss:6.918756008148193\n",
            "step:132, loss:49.87297439575195\n",
            "step:133, loss:37.924835205078125\n",
            "step:134, loss:6.644274711608887\n",
            "step:135, loss:79.05012512207031\n",
            "step:136, loss:113.77799987792969\n",
            "step:137, loss:29.69975471496582\n",
            "step:138, loss:5.13303279876709\n",
            "step:139, loss:10.057051658630371\n",
            "step:140, loss:7.536448955535889\n",
            "step:141, loss:9.639562606811523\n",
            "step:142, loss:13.544921875\n",
            "step:143, loss:4.288175106048584\n",
            "step:144, loss:10.401347160339355\n",
            "step:145, loss:264.3714599609375\n",
            "step:146, loss:9.283550262451172\n",
            "step:147, loss:98.55791473388672\n",
            "step:148, loss:8.859091758728027\n",
            "step:149, loss:13.663951873779297\n",
            "step:150, loss:11.187923431396484\n",
            "step:151, loss:64.90616607666016\n",
            "step:152, loss:4.610146522521973\n",
            "step:153, loss:8.629036903381348\n",
            "step:154, loss:71.58726501464844\n",
            "step:155, loss:8.244200706481934\n",
            "step:156, loss:19.796958923339844\n",
            "step:157, loss:34.63069534301758\n",
            "step:158, loss:115.85533142089844\n",
            "step:159, loss:5.838772773742676\n",
            "step:160, loss:28.059162139892578\n",
            "step:161, loss:9.699033737182617\n",
            "step:162, loss:235.13006591796875\n",
            "step:163, loss:7.14238977432251\n",
            "step:164, loss:2.6882455348968506\n",
            "step:165, loss:9.055717468261719\n",
            "step:166, loss:7.879393577575684\n",
            "step:167, loss:7149.7626953125\n",
            "step:168, loss:4.909824848175049\n",
            "step:169, loss:207.18814086914062\n",
            "step:170, loss:46.05357360839844\n",
            "step:171, loss:37.09477615356445\n",
            "step:172, loss:44.57625198364258\n",
            "step:173, loss:26.435659408569336\n",
            "step:174, loss:92.37443542480469\n",
            "step:175, loss:2652.573486328125\n",
            "step:176, loss:715.1832885742188\n",
            "step:177, loss:1101.4185791015625\n",
            "step:178, loss:16.437435150146484\n",
            "step:179, loss:91.43030548095703\n",
            "step:180, loss:26.28387451171875\n",
            "step:181, loss:62.36165237426758\n",
            "step:182, loss:5.731873512268066\n",
            "step:183, loss:5.798322677612305\n",
            "step:184, loss:12.016761779785156\n",
            "step:185, loss:8.654614448547363\n",
            "step:186, loss:171.7972412109375\n",
            "step:187, loss:6.301067352294922\n",
            "step:188, loss:18.652969360351562\n",
            "step:189, loss:5.013221740722656\n",
            "step:190, loss:12.096231460571289\n",
            "step:191, loss:159.131591796875\n",
            "step:192, loss:3.926150321960449\n",
            "step:193, loss:13.004610061645508\n",
            "step:194, loss:946.970947265625\n",
            "step:195, loss:6.189345836639404\n",
            "step:196, loss:31.701196670532227\n",
            "step:197, loss:246.5648651123047\n",
            "step:198, loss:15.010076522827148\n",
            "step:199, loss:10.101045608520508\n",
            "step:200, loss:9.98108959197998\n",
            "step:201, loss:26.901729583740234\n",
            "step:202, loss:163.6045379638672\n",
            "22\n",
            "-----\n",
            "step:0, loss:13.591095924377441\n",
            "step:1, loss:20.104476928710938\n",
            "step:2, loss:5.624355792999268\n",
            "step:3, loss:160.1336669921875\n",
            "step:4, loss:31.57284164428711\n",
            "step:5, loss:9.171472549438477\n",
            "step:6, loss:15.438726425170898\n",
            "step:7, loss:9.721208572387695\n",
            "step:8, loss:6.80584716796875\n",
            "step:9, loss:7.10011625289917\n",
            "step:10, loss:228.1161651611328\n",
            "step:11, loss:6.106379508972168\n",
            "step:12, loss:23.235519409179688\n",
            "step:13, loss:31.969932556152344\n",
            "step:14, loss:15.964990615844727\n",
            "step:15, loss:3.3178510665893555\n",
            "step:16, loss:9.256719589233398\n",
            "step:17, loss:7210.56640625\n",
            "step:18, loss:8.418966293334961\n",
            "step:19, loss:34.790706634521484\n",
            "step:20, loss:66.876708984375\n",
            "step:21, loss:6.705665588378906\n",
            "step:22, loss:404.0948486328125\n",
            "step:23, loss:10.259344100952148\n",
            "step:24, loss:44.50813674926758\n",
            "step:25, loss:4370.27587890625\n",
            "step:26, loss:27.21474838256836\n",
            "step:27, loss:10.29928970336914\n",
            "step:28, loss:6.050168037414551\n",
            "step:29, loss:269.787353515625\n",
            "step:30, loss:155.2725830078125\n",
            "step:31, loss:506.18841552734375\n",
            "step:32, loss:21.388479232788086\n",
            "step:33, loss:58.44900894165039\n",
            "step:34, loss:7.900889873504639\n",
            "step:35, loss:9.067703247070312\n",
            "step:36, loss:79.32464599609375\n",
            "step:37, loss:6.612471103668213\n",
            "step:38, loss:3.643488883972168\n",
            "step:39, loss:20.42910385131836\n",
            "step:40, loss:59.705013275146484\n",
            "step:41, loss:12.767776489257812\n",
            "step:42, loss:328.2435607910156\n",
            "step:43, loss:14.825947761535645\n",
            "step:44, loss:263.06298828125\n",
            "step:45, loss:196.6361846923828\n",
            "step:46, loss:8.314878463745117\n",
            "step:47, loss:34.55769729614258\n",
            "step:48, loss:8.745932579040527\n",
            "step:49, loss:113.67442321777344\n",
            "step:50, loss:11.424670219421387\n",
            "step:51, loss:78.86300659179688\n",
            "step:52, loss:185.8449249267578\n",
            "step:53, loss:18.306949615478516\n",
            "step:54, loss:21.600173950195312\n",
            "step:55, loss:712.969482421875\n",
            "step:56, loss:65.70467376708984\n",
            "step:57, loss:268.0094909667969\n",
            "step:58, loss:15.869375228881836\n",
            "step:59, loss:4.939586639404297\n",
            "step:60, loss:1593.1478271484375\n",
            "step:61, loss:3.191524028778076\n",
            "step:62, loss:6.298208236694336\n",
            "step:63, loss:5.868460178375244\n",
            "step:64, loss:4.245699882507324\n",
            "step:65, loss:239.4049835205078\n",
            "step:66, loss:160.620849609375\n",
            "step:67, loss:4.91892147064209\n",
            "step:68, loss:56.99224090576172\n",
            "step:69, loss:9.733774185180664\n",
            "step:70, loss:25.315885543823242\n",
            "step:71, loss:4.6091156005859375\n",
            "step:72, loss:5.667720317840576\n",
            "step:73, loss:57.27950668334961\n",
            "step:74, loss:16.469627380371094\n",
            "step:75, loss:37.306007385253906\n",
            "step:76, loss:177.23092651367188\n",
            "step:77, loss:182.6497802734375\n",
            "step:78, loss:1045.6456298828125\n",
            "step:79, loss:3.4257919788360596\n",
            "step:80, loss:15.720315933227539\n",
            "step:81, loss:6.684183120727539\n",
            "step:82, loss:7.396303653717041\n",
            "step:83, loss:6.216584205627441\n",
            "step:84, loss:4.053884983062744\n",
            "step:85, loss:3.1112425327301025\n",
            "step:86, loss:4.599370002746582\n",
            "step:87, loss:3.939565658569336\n",
            "step:88, loss:12.150782585144043\n",
            "step:89, loss:3.277276039123535\n",
            "step:90, loss:9.222986221313477\n",
            "step:91, loss:15.393424987792969\n",
            "step:92, loss:13.894848823547363\n",
            "step:93, loss:18.59820556640625\n",
            "step:94, loss:57.475799560546875\n",
            "step:95, loss:61.91670227050781\n",
            "step:96, loss:28.131053924560547\n",
            "step:97, loss:81.81853485107422\n",
            "step:98, loss:7.208615779876709\n",
            "step:99, loss:2578.65625\n",
            "step:100, loss:38.970340728759766\n",
            "step:101, loss:11.133133888244629\n",
            "step:102, loss:170.0244903564453\n",
            "step:103, loss:7.676126003265381\n",
            "step:104, loss:95.77527618408203\n",
            "step:105, loss:35.0856819152832\n",
            "step:106, loss:46.001949310302734\n",
            "step:107, loss:14.589923858642578\n",
            "step:108, loss:7.344898223876953\n",
            "step:109, loss:243.28091430664062\n",
            "step:110, loss:15.273320198059082\n",
            "step:111, loss:6.812616348266602\n",
            "step:112, loss:45.922569274902344\n",
            "step:113, loss:271.43951416015625\n",
            "step:114, loss:5.398589134216309\n",
            "step:115, loss:30.40125846862793\n",
            "step:116, loss:10.249855041503906\n",
            "step:117, loss:11.061906814575195\n",
            "step:118, loss:4.750107765197754\n",
            "step:119, loss:3.9541831016540527\n",
            "step:120, loss:4.322935581207275\n",
            "step:121, loss:37.15296936035156\n",
            "step:122, loss:2.354271411895752\n",
            "step:123, loss:12.716657638549805\n",
            "step:124, loss:108.62893676757812\n",
            "step:125, loss:217.2733917236328\n",
            "step:126, loss:21.568632125854492\n",
            "step:127, loss:10.461658477783203\n",
            "step:128, loss:46.24449157714844\n",
            "step:129, loss:36.643150329589844\n",
            "step:130, loss:52.560386657714844\n",
            "step:131, loss:16.925312042236328\n",
            "step:132, loss:160.1940460205078\n",
            "step:133, loss:46.03226852416992\n",
            "step:134, loss:48.01700973510742\n",
            "step:135, loss:124.43898010253906\n",
            "step:136, loss:13.252519607543945\n",
            "step:137, loss:50.40523147583008\n",
            "step:138, loss:8.892763137817383\n",
            "step:139, loss:9.527433395385742\n",
            "step:140, loss:44.84077835083008\n",
            "step:141, loss:641.5676879882812\n",
            "step:142, loss:3.077216148376465\n",
            "step:143, loss:7.691839218139648\n",
            "step:144, loss:4.661674976348877\n",
            "step:145, loss:8.721867561340332\n",
            "step:146, loss:6.707101821899414\n",
            "step:147, loss:4.66023063659668\n",
            "step:148, loss:180.63381958007812\n",
            "step:149, loss:6.503678321838379\n",
            "step:150, loss:39.608665466308594\n",
            "step:151, loss:11.171685218811035\n",
            "step:152, loss:18.22093963623047\n",
            "step:153, loss:5.684170246124268\n",
            "step:154, loss:83.20563507080078\n",
            "step:155, loss:598.1988525390625\n",
            "step:156, loss:7.279423713684082\n",
            "step:157, loss:1521.194091796875\n",
            "step:158, loss:60.221466064453125\n",
            "step:159, loss:254.2819366455078\n",
            "step:160, loss:89.50061798095703\n",
            "step:161, loss:204.6160888671875\n",
            "step:162, loss:5.644455909729004\n",
            "step:163, loss:680.9948120117188\n",
            "step:164, loss:55.55294418334961\n",
            "step:165, loss:7.803614616394043\n",
            "step:166, loss:126.54869079589844\n",
            "step:167, loss:92.91907501220703\n",
            "step:168, loss:34.752803802490234\n",
            "step:169, loss:28.60300064086914\n",
            "step:170, loss:9.785043716430664\n",
            "step:171, loss:6.223100662231445\n",
            "step:172, loss:17.93822479248047\n",
            "step:173, loss:11952.896484375\n",
            "step:174, loss:666.0520629882812\n",
            "step:175, loss:7.493813991546631\n",
            "step:176, loss:58.7142333984375\n",
            "step:177, loss:2.3846893310546875\n",
            "step:178, loss:42.84587097167969\n",
            "step:179, loss:18.2041015625\n",
            "step:180, loss:9.55111312866211\n",
            "step:181, loss:27.51995086669922\n",
            "step:182, loss:6.210447311401367\n",
            "step:183, loss:6.685660362243652\n",
            "step:184, loss:6.756014823913574\n",
            "step:185, loss:20.72203826904297\n",
            "step:186, loss:347.9254455566406\n",
            "step:187, loss:41.0279541015625\n",
            "step:188, loss:8.87325668334961\n",
            "step:189, loss:4.826727867126465\n",
            "step:190, loss:87.89435577392578\n",
            "step:191, loss:4.784848213195801\n",
            "step:192, loss:24.206886291503906\n",
            "step:193, loss:7.7191362380981445\n",
            "step:194, loss:11.375223159790039\n",
            "step:195, loss:5.1132917404174805\n",
            "step:196, loss:100.18115234375\n",
            "step:197, loss:7.730111598968506\n",
            "step:198, loss:7292.201171875\n",
            "step:199, loss:5.115314483642578\n",
            "step:200, loss:85.08138275146484\n",
            "step:201, loss:60.540470123291016\n",
            "step:202, loss:2564.69921875\n",
            "23\n",
            "-----\n",
            "step:0, loss:6.016689300537109\n",
            "step:1, loss:9.062936782836914\n",
            "step:2, loss:7.5587005615234375\n",
            "step:3, loss:4.1447553634643555\n",
            "step:4, loss:8.15086841583252\n",
            "step:5, loss:5.010244369506836\n",
            "step:6, loss:82.34780883789062\n",
            "step:7, loss:6.383486270904541\n",
            "step:8, loss:7.889073371887207\n",
            "step:9, loss:239.8600311279297\n",
            "step:10, loss:29.300539016723633\n",
            "step:11, loss:78.42208862304688\n",
            "step:12, loss:76.61367797851562\n",
            "step:13, loss:48.91215133666992\n",
            "step:14, loss:6.085751056671143\n",
            "step:15, loss:12.143967628479004\n",
            "step:16, loss:65.45024108886719\n",
            "step:17, loss:249.33474731445312\n",
            "step:18, loss:12.402563095092773\n",
            "step:19, loss:119.77482604980469\n",
            "step:20, loss:5.85188102722168\n",
            "step:21, loss:350.564697265625\n",
            "step:22, loss:22.803281784057617\n",
            "step:23, loss:16.641857147216797\n",
            "step:24, loss:37.172035217285156\n",
            "step:25, loss:3.3897836208343506\n",
            "step:26, loss:95.12364196777344\n",
            "step:27, loss:51.14690017700195\n",
            "step:28, loss:39.151771545410156\n",
            "step:29, loss:8.358316421508789\n",
            "step:30, loss:7.396564960479736\n",
            "step:31, loss:7.955779075622559\n",
            "step:32, loss:199.69728088378906\n",
            "step:33, loss:11880.4912109375\n",
            "step:34, loss:5.552113056182861\n",
            "step:35, loss:33.94222640991211\n",
            "step:36, loss:7.477956771850586\n",
            "step:37, loss:22.78298568725586\n",
            "step:38, loss:5.920988082885742\n",
            "step:39, loss:15.211499214172363\n",
            "step:40, loss:62.92403030395508\n",
            "step:41, loss:15.48615550994873\n",
            "step:42, loss:31.72637367248535\n",
            "step:43, loss:734.9813232421875\n",
            "step:44, loss:94.5735855102539\n",
            "step:45, loss:7.551239013671875\n",
            "step:46, loss:4.955952167510986\n",
            "step:47, loss:280.4626159667969\n",
            "step:48, loss:28.25737953186035\n",
            "step:49, loss:28.882299423217773\n",
            "step:50, loss:3.1462159156799316\n",
            "step:51, loss:33.93344497680664\n",
            "step:52, loss:8.593660354614258\n",
            "step:53, loss:204.27674865722656\n",
            "step:54, loss:5.536447525024414\n",
            "step:55, loss:3.8479726314544678\n",
            "step:56, loss:14.563925743103027\n",
            "step:57, loss:7.226756572723389\n",
            "step:58, loss:4.954259395599365\n",
            "step:59, loss:79.72154998779297\n",
            "step:60, loss:688.4692993164062\n",
            "step:61, loss:3.0957658290863037\n",
            "step:62, loss:5.593411445617676\n",
            "step:63, loss:5.67629337310791\n",
            "step:64, loss:24.805204391479492\n",
            "step:65, loss:62.16737747192383\n",
            "step:66, loss:9.46822452545166\n",
            "step:67, loss:4.954198837280273\n",
            "step:68, loss:230.0230712890625\n",
            "step:69, loss:6.905616760253906\n",
            "step:70, loss:66.44230651855469\n",
            "step:71, loss:41.106834411621094\n",
            "step:72, loss:4.5133562088012695\n",
            "step:73, loss:16.98666763305664\n",
            "step:74, loss:24.51660919189453\n",
            "step:75, loss:552.0546264648438\n",
            "step:76, loss:2.2540812492370605\n",
            "step:77, loss:78.77277374267578\n",
            "step:78, loss:73.16034698486328\n",
            "step:79, loss:7.532914161682129\n",
            "step:80, loss:9.70903205871582\n",
            "step:81, loss:4.033164024353027\n",
            "step:82, loss:3.2733092308044434\n",
            "step:83, loss:120.46676635742188\n",
            "step:84, loss:251.4995574951172\n",
            "step:85, loss:133.389892578125\n",
            "step:86, loss:2.70285701751709\n",
            "step:87, loss:4327.294921875\n",
            "step:88, loss:139.6292724609375\n",
            "step:89, loss:147.1328125\n",
            "step:90, loss:9.88541030883789\n",
            "step:91, loss:6.732282638549805\n",
            "step:92, loss:7130.74658203125\n",
            "step:93, loss:8.787328720092773\n",
            "step:94, loss:2.2917144298553467\n",
            "step:95, loss:8.167802810668945\n",
            "step:96, loss:13.861422538757324\n",
            "step:97, loss:8.813409805297852\n",
            "step:98, loss:33.42230987548828\n",
            "step:99, loss:21.126243591308594\n",
            "step:100, loss:133.58865356445312\n",
            "step:101, loss:50.632080078125\n",
            "step:102, loss:6.909019947052002\n",
            "step:103, loss:5.770271301269531\n",
            "step:104, loss:17.37396240234375\n",
            "step:105, loss:17.670635223388672\n",
            "step:106, loss:774.7267456054688\n",
            "step:107, loss:147.80715942382812\n",
            "step:108, loss:3.9583592414855957\n",
            "step:109, loss:30.414094924926758\n",
            "step:110, loss:24.857311248779297\n",
            "step:111, loss:3.3401031494140625\n",
            "step:112, loss:6.535022735595703\n",
            "step:113, loss:1829.6092529296875\n",
            "step:114, loss:72.63346099853516\n",
            "step:115, loss:12.533173561096191\n",
            "step:116, loss:6.876041889190674\n",
            "step:117, loss:60.911922454833984\n",
            "step:118, loss:33.719024658203125\n",
            "step:119, loss:51.79518127441406\n",
            "step:120, loss:5.324186325073242\n",
            "step:121, loss:6.449301719665527\n",
            "step:122, loss:13.832387924194336\n",
            "step:123, loss:111.8783950805664\n",
            "step:124, loss:5.164183616638184\n",
            "step:125, loss:6.005825042724609\n",
            "step:126, loss:84.23765563964844\n",
            "step:127, loss:8.60606575012207\n",
            "step:128, loss:7.9833526611328125\n",
            "step:129, loss:332.9783935546875\n",
            "step:130, loss:125.02422332763672\n",
            "step:131, loss:252.18080139160156\n",
            "step:132, loss:5.53900146484375\n",
            "step:133, loss:12.326196670532227\n",
            "step:134, loss:171.13131713867188\n",
            "step:135, loss:20.340621948242188\n",
            "step:136, loss:7.514066696166992\n",
            "step:137, loss:7.108537673950195\n",
            "step:138, loss:866.6273803710938\n",
            "step:139, loss:39.499691009521484\n",
            "step:140, loss:10.860239028930664\n",
            "step:141, loss:25.642419815063477\n",
            "step:142, loss:6.915844917297363\n",
            "step:143, loss:8.845338821411133\n",
            "step:144, loss:155.54296875\n",
            "step:145, loss:52.43118667602539\n",
            "step:146, loss:2.7772035598754883\n",
            "step:147, loss:43.0618896484375\n",
            "step:148, loss:2550.5595703125\n",
            "step:149, loss:326.4654541015625\n",
            "step:150, loss:10.699943542480469\n",
            "step:151, loss:29.11744499206543\n",
            "step:152, loss:165.6728057861328\n",
            "step:153, loss:40.84176254272461\n",
            "step:154, loss:40.43149185180664\n",
            "step:155, loss:78.19397735595703\n",
            "step:156, loss:15.170694351196289\n",
            "step:157, loss:7.49132776260376\n",
            "step:158, loss:1068.8798828125\n",
            "step:159, loss:12.517067909240723\n",
            "step:160, loss:164.6405029296875\n",
            "step:161, loss:5.9380903244018555\n",
            "step:162, loss:14.099361419677734\n",
            "step:163, loss:51.45305252075195\n",
            "step:164, loss:11.769643783569336\n",
            "step:165, loss:4.045316219329834\n",
            "step:166, loss:5.576436996459961\n",
            "step:167, loss:160.7963104248047\n",
            "step:168, loss:43.29484558105469\n",
            "step:169, loss:7.422290802001953\n",
            "step:170, loss:8.35481071472168\n",
            "step:171, loss:3.9416182041168213\n",
            "step:172, loss:4.63476037979126\n",
            "step:173, loss:4.2182936668396\n",
            "step:174, loss:4.913062572479248\n",
            "step:175, loss:12.289780616760254\n",
            "step:176, loss:3.180420398712158\n",
            "step:177, loss:456.74334716796875\n",
            "step:178, loss:12.782123565673828\n",
            "step:179, loss:102.02349853515625\n",
            "step:180, loss:7.644288539886475\n",
            "step:181, loss:7.231996536254883\n",
            "step:182, loss:7.165645599365234\n",
            "step:183, loss:7.050965309143066\n",
            "step:184, loss:6.156063079833984\n",
            "step:185, loss:18.594728469848633\n",
            "step:186, loss:35.168174743652344\n",
            "step:187, loss:7491.3798828125\n",
            "step:188, loss:13.342790603637695\n",
            "step:189, loss:99.81356811523438\n",
            "step:190, loss:39.151123046875\n",
            "step:191, loss:9.84708023071289\n",
            "step:192, loss:96.62025451660156\n",
            "step:193, loss:7.942365646362305\n",
            "step:194, loss:4.908697128295898\n",
            "step:195, loss:7.397434234619141\n",
            "step:196, loss:31.968772888183594\n",
            "step:197, loss:11.72291088104248\n",
            "step:198, loss:23.691184997558594\n",
            "step:199, loss:685.0261840820312\n",
            "step:200, loss:35.59232711791992\n",
            "step:201, loss:1482.4993896484375\n",
            "step:202, loss:5.197058200836182\n",
            "24\n",
            "-----\n",
            "step:0, loss:1117.518310546875\n",
            "step:1, loss:3.126725673675537\n",
            "step:2, loss:5.135158538818359\n",
            "step:3, loss:7.392844200134277\n",
            "step:4, loss:5.748908996582031\n",
            "step:5, loss:1484.360107421875\n",
            "step:6, loss:13.86832046508789\n",
            "step:7, loss:53.193843841552734\n",
            "step:8, loss:16.86989974975586\n",
            "step:9, loss:116.14771270751953\n",
            "step:10, loss:8.615459442138672\n",
            "step:11, loss:4233.92724609375\n",
            "step:12, loss:11.806474685668945\n",
            "step:13, loss:11.56965446472168\n",
            "step:14, loss:3.522078037261963\n",
            "step:15, loss:6.469708442687988\n",
            "step:16, loss:251.58486938476562\n",
            "step:17, loss:24.77812957763672\n",
            "step:18, loss:38.53177261352539\n",
            "step:19, loss:53.36152648925781\n",
            "step:20, loss:6.515403747558594\n",
            "step:21, loss:118.39832305908203\n",
            "step:22, loss:2.681356430053711\n",
            "step:23, loss:12.429018020629883\n",
            "step:24, loss:42.90574645996094\n",
            "step:25, loss:8.5037260055542\n",
            "step:26, loss:11.686934471130371\n",
            "step:27, loss:8.622730255126953\n",
            "step:28, loss:714.4974365234375\n",
            "step:29, loss:38.395809173583984\n",
            "step:30, loss:56.35246658325195\n",
            "step:31, loss:32.73374557495117\n",
            "step:32, loss:179.47763061523438\n",
            "step:33, loss:7.796617031097412\n",
            "step:34, loss:404.4608459472656\n",
            "step:35, loss:138.86932373046875\n",
            "step:36, loss:27.776887893676758\n",
            "step:37, loss:3.187242269515991\n",
            "step:38, loss:9.64752197265625\n",
            "step:39, loss:8.898000717163086\n",
            "step:40, loss:72.35676574707031\n",
            "step:41, loss:9.251420021057129\n",
            "step:42, loss:5.008608818054199\n",
            "step:43, loss:7.017827987670898\n",
            "step:44, loss:15.477787017822266\n",
            "step:45, loss:7.015180587768555\n",
            "step:46, loss:147.550537109375\n",
            "step:47, loss:2.78109073638916\n",
            "step:48, loss:56.100555419921875\n",
            "step:49, loss:11.307662963867188\n",
            "step:50, loss:9.71292495727539\n",
            "step:51, loss:16.166940689086914\n",
            "step:52, loss:65.60626220703125\n",
            "step:53, loss:12.051706314086914\n",
            "step:54, loss:19.92258644104004\n",
            "step:55, loss:3.4933788776397705\n",
            "step:56, loss:16.67142677307129\n",
            "step:57, loss:2.794285297393799\n",
            "step:58, loss:14.427309036254883\n",
            "step:59, loss:12.404573440551758\n",
            "step:60, loss:46.1048698425293\n",
            "step:61, loss:2827.854248046875\n",
            "step:62, loss:9.689228057861328\n",
            "step:63, loss:10.928969383239746\n",
            "step:64, loss:9.302864074707031\n",
            "step:65, loss:15.820839881896973\n",
            "step:66, loss:17.770267486572266\n",
            "step:67, loss:83.6811294555664\n",
            "step:68, loss:361.89013671875\n",
            "step:69, loss:3.6511728763580322\n",
            "step:70, loss:334.5621337890625\n",
            "step:71, loss:8.022689819335938\n",
            "step:72, loss:4.380231857299805\n",
            "step:73, loss:3.7479920387268066\n",
            "step:74, loss:3.018390417098999\n",
            "step:75, loss:36.55648422241211\n",
            "step:76, loss:7307.08349609375\n",
            "step:77, loss:58.35663986206055\n",
            "step:78, loss:7.045750617980957\n",
            "step:79, loss:768.7808837890625\n",
            "step:80, loss:2.9958245754241943\n",
            "step:81, loss:10.198637008666992\n",
            "step:82, loss:3.3754079341888428\n",
            "step:83, loss:12.306644439697266\n",
            "step:84, loss:27.099191665649414\n",
            "step:85, loss:78.96757507324219\n",
            "step:86, loss:253.9024200439453\n",
            "step:87, loss:163.8572540283203\n",
            "step:88, loss:239.8986358642578\n",
            "step:89, loss:4.030373573303223\n",
            "step:90, loss:481.3693542480469\n",
            "step:91, loss:7079.5703125\n",
            "step:92, loss:14.826727867126465\n",
            "step:93, loss:34.79345703125\n",
            "step:94, loss:135.60110473632812\n",
            "step:95, loss:422.4806823730469\n",
            "step:96, loss:99.58087921142578\n",
            "step:97, loss:11.44656753540039\n",
            "step:98, loss:125.23612976074219\n",
            "step:99, loss:4.903126239776611\n",
            "step:100, loss:9.84788703918457\n",
            "step:101, loss:61.68616485595703\n",
            "step:102, loss:5.273007392883301\n",
            "step:103, loss:17.392560958862305\n",
            "step:104, loss:11.817729949951172\n",
            "step:105, loss:6.305091857910156\n",
            "step:106, loss:84.27371978759766\n",
            "step:107, loss:3.8110499382019043\n",
            "step:108, loss:5.573605060577393\n",
            "step:109, loss:12.643795013427734\n",
            "step:110, loss:193.88638305664062\n",
            "step:111, loss:98.56663513183594\n",
            "step:112, loss:2.6641604900360107\n",
            "step:113, loss:41.16001892089844\n",
            "step:114, loss:51.50282287597656\n",
            "step:115, loss:93.98560333251953\n",
            "step:116, loss:188.8849334716797\n",
            "step:117, loss:12.075796127319336\n",
            "step:118, loss:173.58554077148438\n",
            "step:119, loss:11.60197639465332\n",
            "step:120, loss:63.02018356323242\n",
            "step:121, loss:26.9720458984375\n",
            "step:122, loss:4.802147388458252\n",
            "step:123, loss:4.263368129730225\n",
            "step:124, loss:7.85603141784668\n",
            "step:125, loss:5.2227582931518555\n",
            "step:126, loss:14.943744659423828\n",
            "step:127, loss:81.12586212158203\n",
            "step:128, loss:12.086074829101562\n",
            "step:129, loss:9.322938919067383\n",
            "step:130, loss:7.708463668823242\n",
            "step:131, loss:5.019434928894043\n",
            "step:132, loss:183.78558349609375\n",
            "step:133, loss:156.73245239257812\n",
            "step:134, loss:5.914929389953613\n",
            "step:135, loss:11.625875473022461\n",
            "step:136, loss:669.9833374023438\n",
            "step:137, loss:13.482845306396484\n",
            "step:138, loss:3.121901512145996\n",
            "step:139, loss:116.30636596679688\n",
            "step:140, loss:11.747068405151367\n",
            "step:141, loss:186.444580078125\n",
            "step:142, loss:10.925884246826172\n",
            "step:143, loss:8.287127494812012\n",
            "step:144, loss:3.7873034477233887\n",
            "step:145, loss:682.464599609375\n",
            "step:146, loss:15.26419448852539\n",
            "step:147, loss:20.67821502685547\n",
            "step:148, loss:14.358253479003906\n",
            "step:149, loss:9.068714141845703\n",
            "step:150, loss:41.687896728515625\n",
            "step:151, loss:50.6664924621582\n",
            "step:152, loss:1599.7144775390625\n",
            "step:153, loss:14.290233612060547\n",
            "step:154, loss:9.579169273376465\n",
            "step:155, loss:49.09244918823242\n",
            "step:156, loss:40.34514617919922\n",
            "step:157, loss:51.35616683959961\n",
            "step:158, loss:5.0821099281311035\n",
            "step:159, loss:4.036770343780518\n",
            "step:160, loss:4.803862571716309\n",
            "step:161, loss:13.151878356933594\n",
            "step:162, loss:4.478608131408691\n",
            "step:163, loss:253.298583984375\n",
            "step:164, loss:9.672094345092773\n",
            "step:165, loss:2.1608052253723145\n",
            "step:166, loss:633.8992919921875\n",
            "step:167, loss:16.84966468811035\n",
            "step:168, loss:14.565114974975586\n",
            "step:169, loss:14.741270065307617\n",
            "step:170, loss:126.81880187988281\n",
            "step:171, loss:50.33685302734375\n",
            "step:172, loss:19.8396053314209\n",
            "step:173, loss:13.818153381347656\n",
            "step:174, loss:252.4286651611328\n",
            "step:175, loss:5.405601978302002\n",
            "step:176, loss:10.905073165893555\n",
            "step:177, loss:216.60867309570312\n",
            "step:178, loss:36.09719467163086\n",
            "step:179, loss:6.7150726318359375\n",
            "step:180, loss:16.700756072998047\n",
            "step:181, loss:3.901871681213379\n",
            "step:182, loss:16.01259994506836\n",
            "step:183, loss:18.878520965576172\n",
            "step:184, loss:50.92585754394531\n",
            "step:185, loss:90.92953491210938\n",
            "step:186, loss:5.847851753234863\n",
            "step:187, loss:38.8783073425293\n",
            "step:188, loss:5.719563007354736\n",
            "step:189, loss:7.907877445220947\n",
            "step:190, loss:9.027596473693848\n",
            "step:191, loss:49.5209846496582\n",
            "step:192, loss:31.49735450744629\n",
            "step:193, loss:15.995988845825195\n",
            "step:194, loss:84.21923828125\n",
            "step:195, loss:81.7565689086914\n",
            "step:196, loss:11887.62109375\n",
            "step:197, loss:2.9064393043518066\n",
            "step:198, loss:3.9494690895080566\n",
            "step:199, loss:12.558613777160645\n",
            "step:200, loss:4.525260925292969\n",
            "step:201, loss:9.057785034179688\n",
            "step:202, loss:5.571896076202393\n",
            "25\n",
            "-----\n",
            "step:0, loss:5.676203727722168\n",
            "step:1, loss:146.06776428222656\n",
            "step:2, loss:8.56628704071045\n",
            "step:3, loss:119.26373291015625\n",
            "step:4, loss:3.9166531562805176\n",
            "step:5, loss:8.552450180053711\n",
            "step:6, loss:145.8568115234375\n",
            "step:7, loss:4.559350967407227\n",
            "step:8, loss:2531.12255859375\n",
            "step:9, loss:244.71385192871094\n",
            "step:10, loss:3.330004930496216\n",
            "step:11, loss:30.098752975463867\n",
            "step:12, loss:29.15751075744629\n",
            "step:13, loss:11959.919921875\n",
            "step:14, loss:1696.6832275390625\n",
            "step:15, loss:715.5084838867188\n",
            "step:16, loss:15.755678176879883\n",
            "step:17, loss:7.714459419250488\n",
            "step:18, loss:49.00335693359375\n",
            "step:19, loss:6.368900775909424\n",
            "step:20, loss:7.586221694946289\n",
            "step:21, loss:28.053688049316406\n",
            "step:22, loss:4.37518310546875\n",
            "step:23, loss:4.964717864990234\n",
            "step:24, loss:3.876400947570801\n",
            "step:25, loss:4.206413745880127\n",
            "step:26, loss:278.082275390625\n",
            "step:27, loss:223.50619506835938\n",
            "step:28, loss:56.7857780456543\n",
            "step:29, loss:4.637432098388672\n",
            "step:30, loss:5.440671443939209\n",
            "step:31, loss:14.347999572753906\n",
            "step:32, loss:294.35516357421875\n",
            "step:33, loss:4.695602893829346\n",
            "step:34, loss:6.214410305023193\n",
            "step:35, loss:3.6459908485412598\n",
            "step:36, loss:1631.7293701171875\n",
            "step:37, loss:9.930977821350098\n",
            "step:38, loss:88.89252471923828\n",
            "step:39, loss:200.0538330078125\n",
            "step:40, loss:11.508543014526367\n",
            "step:41, loss:1.2590714693069458\n",
            "step:42, loss:29.133625030517578\n",
            "step:43, loss:3.291680335998535\n",
            "step:44, loss:47.56507873535156\n",
            "step:45, loss:58.00809097290039\n",
            "step:46, loss:4.101984024047852\n",
            "step:47, loss:15.655550956726074\n",
            "step:48, loss:3.0261168479919434\n",
            "step:49, loss:18.195140838623047\n",
            "step:50, loss:36.90736770629883\n",
            "step:51, loss:85.0658187866211\n",
            "step:52, loss:4.332388877868652\n",
            "step:53, loss:385.3831787109375\n",
            "step:54, loss:9.249838829040527\n",
            "step:55, loss:4.913272380828857\n",
            "step:56, loss:674.0546264648438\n",
            "step:57, loss:228.42662048339844\n",
            "step:58, loss:20.284955978393555\n",
            "step:59, loss:7.360382080078125\n",
            "step:60, loss:11.665346145629883\n",
            "step:61, loss:6.886300086975098\n",
            "step:62, loss:12.892656326293945\n",
            "step:63, loss:67.7408218383789\n",
            "step:64, loss:5.342677116394043\n",
            "step:65, loss:2.72107195854187\n",
            "step:66, loss:13.580229759216309\n",
            "step:67, loss:13.91672134399414\n",
            "step:68, loss:7.8999762535095215\n",
            "step:69, loss:48.23631286621094\n",
            "step:70, loss:209.31907653808594\n",
            "step:71, loss:131.22508239746094\n",
            "step:72, loss:49.234066009521484\n",
            "step:73, loss:7.697349548339844\n",
            "step:74, loss:31.38671875\n",
            "step:75, loss:16.113536834716797\n",
            "step:76, loss:13.155982971191406\n",
            "step:77, loss:21.064250946044922\n",
            "step:78, loss:3.658618450164795\n",
            "step:79, loss:33.08113098144531\n",
            "step:80, loss:14.994240760803223\n",
            "step:81, loss:6.334162712097168\n",
            "step:82, loss:10.645818710327148\n",
            "step:83, loss:117.71675872802734\n",
            "step:84, loss:8.816494941711426\n",
            "step:85, loss:12.742518424987793\n",
            "step:86, loss:6.8566107749938965\n",
            "step:87, loss:97.32977294921875\n",
            "step:88, loss:152.2100067138672\n",
            "step:89, loss:53.3526725769043\n",
            "step:90, loss:5.773184299468994\n",
            "step:91, loss:77.13935089111328\n",
            "step:92, loss:387.4584045410156\n",
            "step:93, loss:36.75507354736328\n",
            "step:94, loss:7008.8447265625\n",
            "step:95, loss:4.3202223777771\n",
            "step:96, loss:12.20752239227295\n",
            "step:97, loss:6.963252067565918\n",
            "step:98, loss:26.308626174926758\n",
            "step:99, loss:7.1689863204956055\n",
            "step:100, loss:7.816628456115723\n",
            "step:101, loss:3.411013126373291\n",
            "step:102, loss:43.529754638671875\n",
            "step:103, loss:2.334878444671631\n",
            "step:104, loss:8.860998153686523\n",
            "step:105, loss:1453.72265625\n",
            "step:106, loss:140.09567260742188\n",
            "step:107, loss:29.51906967163086\n",
            "step:108, loss:4.978660583496094\n",
            "step:109, loss:11.844884872436523\n",
            "step:110, loss:4.618649482727051\n",
            "step:111, loss:25.898086547851562\n",
            "step:112, loss:87.36912536621094\n",
            "step:113, loss:31.890830993652344\n",
            "step:114, loss:7.683103084564209\n",
            "step:115, loss:16.29620361328125\n",
            "step:116, loss:10.802345275878906\n",
            "step:117, loss:54.45270538330078\n",
            "step:118, loss:5.998550891876221\n",
            "step:119, loss:4.932381629943848\n",
            "step:120, loss:4.9928879737854\n",
            "step:121, loss:11.018592834472656\n",
            "step:122, loss:21.883752822875977\n",
            "step:123, loss:266.1976623535156\n",
            "step:124, loss:338.3629150390625\n",
            "step:125, loss:3.896456003189087\n",
            "step:126, loss:18.919158935546875\n",
            "step:127, loss:17.11324691772461\n",
            "step:128, loss:3.9025402069091797\n",
            "step:129, loss:612.7911376953125\n",
            "step:130, loss:71.87080383300781\n",
            "step:131, loss:4.858989715576172\n",
            "step:132, loss:5.387500762939453\n",
            "step:133, loss:4.318578720092773\n",
            "step:134, loss:3.906461715698242\n",
            "step:135, loss:193.34002685546875\n",
            "step:136, loss:13.43583869934082\n",
            "step:137, loss:162.86016845703125\n",
            "step:138, loss:183.9531707763672\n",
            "step:139, loss:778.6901245117188\n",
            "step:140, loss:3.50679874420166\n",
            "step:141, loss:35.52056884765625\n",
            "step:142, loss:3.880463123321533\n",
            "step:143, loss:11.066509246826172\n",
            "step:144, loss:3.5469536781311035\n",
            "step:145, loss:50.58203125\n",
            "step:146, loss:6.68449592590332\n",
            "step:147, loss:31.238496780395508\n",
            "step:148, loss:17.32229995727539\n",
            "step:149, loss:7.627025604248047\n",
            "step:150, loss:3.9935572147369385\n",
            "step:151, loss:9.153263092041016\n",
            "step:152, loss:12.471254348754883\n",
            "step:153, loss:65.36290740966797\n",
            "step:154, loss:3.969879627227783\n",
            "step:155, loss:4.400203704833984\n",
            "step:156, loss:109.49280548095703\n",
            "step:157, loss:5.275172710418701\n",
            "step:158, loss:93.24362182617188\n",
            "step:159, loss:2.7825636863708496\n",
            "step:160, loss:62.41938400268555\n",
            "step:161, loss:16.70970344543457\n",
            "step:162, loss:87.56769561767578\n",
            "step:163, loss:14.320474624633789\n",
            "step:164, loss:4.558521270751953\n",
            "step:165, loss:36.956298828125\n",
            "step:166, loss:94.02977752685547\n",
            "step:167, loss:6.973671913146973\n",
            "step:168, loss:8.498934745788574\n",
            "step:169, loss:487.9198913574219\n",
            "step:170, loss:8.150177001953125\n",
            "step:171, loss:6.957437992095947\n",
            "step:172, loss:17.77787208557129\n",
            "step:173, loss:45.27030563354492\n",
            "step:174, loss:4.784003257751465\n",
            "step:175, loss:12.204998970031738\n",
            "step:176, loss:79.49842834472656\n",
            "step:177, loss:13.044200897216797\n",
            "step:178, loss:5.647444725036621\n",
            "step:179, loss:5.065107345581055\n",
            "step:180, loss:7219.33837890625\n",
            "step:181, loss:8.483514785766602\n",
            "step:182, loss:9.018115997314453\n",
            "step:183, loss:6.78439998626709\n",
            "step:184, loss:364.6978454589844\n",
            "step:185, loss:86.58354949951172\n",
            "step:186, loss:322.27978515625\n",
            "step:187, loss:7.302669048309326\n",
            "step:188, loss:12.507773399353027\n",
            "step:189, loss:2.8287575244903564\n",
            "step:190, loss:85.58195495605469\n",
            "step:191, loss:187.40640258789062\n",
            "step:192, loss:40.02134704589844\n",
            "step:193, loss:14.140575408935547\n",
            "step:194, loss:4246.85302734375\n",
            "step:195, loss:13.700286865234375\n",
            "step:196, loss:101.58818817138672\n",
            "step:197, loss:59.94830322265625\n",
            "step:198, loss:22.578535079956055\n",
            "step:199, loss:5.915130615234375\n",
            "step:200, loss:6.834358215332031\n",
            "step:201, loss:71.38782501220703\n",
            "step:202, loss:9.072771072387695\n",
            "26\n",
            "-----\n",
            "step:0, loss:316.7674560546875\n",
            "step:1, loss:4.993914604187012\n",
            "step:2, loss:57.52501678466797\n",
            "step:3, loss:28.53453254699707\n",
            "step:4, loss:309.11102294921875\n",
            "step:5, loss:90.92184448242188\n",
            "step:6, loss:8.676793098449707\n",
            "step:7, loss:9.363691329956055\n",
            "step:8, loss:15.95550537109375\n",
            "step:9, loss:4.652111053466797\n",
            "step:10, loss:14.690367698669434\n",
            "step:11, loss:618.269287109375\n",
            "step:12, loss:5.089260578155518\n",
            "step:13, loss:3.137660026550293\n",
            "step:14, loss:4229.9345703125\n",
            "step:15, loss:3.1685545444488525\n",
            "step:16, loss:273.7341613769531\n",
            "step:17, loss:3.951615333557129\n",
            "step:18, loss:2.4563698768615723\n",
            "step:19, loss:5.16920280456543\n",
            "step:20, loss:3.9019761085510254\n",
            "step:21, loss:5.702927112579346\n",
            "step:22, loss:23.437183380126953\n",
            "step:23, loss:25.1295166015625\n",
            "step:24, loss:80.64078521728516\n",
            "step:25, loss:52.08386993408203\n",
            "step:26, loss:34.413780212402344\n",
            "step:27, loss:5.509180068969727\n",
            "step:28, loss:4.373927116394043\n",
            "step:29, loss:3.7683749198913574\n",
            "step:30, loss:11.4267578125\n",
            "step:31, loss:104.5079345703125\n",
            "step:32, loss:4.36747932434082\n",
            "step:33, loss:79.32987976074219\n",
            "step:34, loss:1.5851027965545654\n",
            "step:35, loss:80.69452667236328\n",
            "step:36, loss:25.79560089111328\n",
            "step:37, loss:10.016641616821289\n",
            "step:38, loss:36.24594497680664\n",
            "step:39, loss:11.698073387145996\n",
            "step:40, loss:6.070554256439209\n",
            "step:41, loss:3.7704620361328125\n",
            "step:42, loss:4.551358699798584\n",
            "step:43, loss:19.541866302490234\n",
            "step:44, loss:82.91512298583984\n",
            "step:45, loss:10.353286743164062\n",
            "step:46, loss:228.42605590820312\n",
            "step:47, loss:2.7157859802246094\n",
            "step:48, loss:4.7497382164001465\n",
            "step:49, loss:35.08847427368164\n",
            "step:50, loss:15.489444732666016\n",
            "step:51, loss:188.9910125732422\n",
            "step:52, loss:7.777127742767334\n",
            "step:53, loss:4.738339424133301\n",
            "step:54, loss:133.74917602539062\n",
            "step:55, loss:306.7216796875\n",
            "step:56, loss:1034.00341796875\n",
            "step:57, loss:67.70736694335938\n",
            "step:58, loss:6.320258140563965\n",
            "step:59, loss:8.954076766967773\n",
            "step:60, loss:38.53376770019531\n",
            "step:61, loss:7.419884204864502\n",
            "step:62, loss:3.6354732513427734\n",
            "step:63, loss:6.736577033996582\n",
            "step:64, loss:4.861704349517822\n",
            "step:65, loss:8.486287117004395\n",
            "step:66, loss:192.8267822265625\n",
            "step:67, loss:24.133073806762695\n",
            "step:68, loss:24.46230125427246\n",
            "step:69, loss:674.139404296875\n",
            "step:70, loss:51.92790603637695\n",
            "step:71, loss:3.5889971256256104\n",
            "step:72, loss:3.3507137298583984\n",
            "step:73, loss:617.5089721679688\n",
            "step:74, loss:60.889225006103516\n",
            "step:75, loss:1449.1328125\n",
            "step:76, loss:52.38536071777344\n",
            "step:77, loss:6.881701946258545\n",
            "step:78, loss:74.55831146240234\n",
            "step:79, loss:11.014935493469238\n",
            "step:80, loss:112.20589447021484\n",
            "step:81, loss:123.05439758300781\n",
            "step:82, loss:154.2967987060547\n",
            "step:83, loss:3.7196154594421387\n",
            "step:84, loss:7.217026710510254\n",
            "step:85, loss:13.157132148742676\n",
            "step:86, loss:74.79933166503906\n",
            "step:87, loss:105.30329895019531\n",
            "step:88, loss:46.35358810424805\n",
            "step:89, loss:47.50248718261719\n",
            "step:90, loss:3.3696794509887695\n",
            "step:91, loss:228.31033325195312\n",
            "step:92, loss:3.8174843788146973\n",
            "step:93, loss:334.0961608886719\n",
            "step:94, loss:26.7540283203125\n",
            "step:95, loss:2.0171115398406982\n",
            "step:96, loss:3.3136444091796875\n",
            "step:97, loss:3.9584407806396484\n",
            "step:98, loss:11.486902236938477\n",
            "step:99, loss:81.90609741210938\n",
            "step:100, loss:46.641910552978516\n",
            "step:101, loss:36.94276428222656\n",
            "step:102, loss:41.8241081237793\n",
            "step:103, loss:77.95384216308594\n",
            "step:104, loss:4.966736793518066\n",
            "step:105, loss:2.6472671031951904\n",
            "step:106, loss:4.229759216308594\n",
            "step:107, loss:139.95474243164062\n",
            "step:108, loss:24.824918746948242\n",
            "step:109, loss:83.65782165527344\n",
            "step:110, loss:7.019778251647949\n",
            "step:111, loss:232.47039794921875\n",
            "step:112, loss:494.42120361328125\n",
            "step:113, loss:38.62221908569336\n",
            "step:114, loss:8.846414566040039\n",
            "step:115, loss:5.635646820068359\n",
            "step:116, loss:2.433114767074585\n",
            "step:117, loss:19.65627098083496\n",
            "step:118, loss:42.31211471557617\n",
            "step:119, loss:3.591675281524658\n",
            "step:120, loss:17.838787078857422\n",
            "step:121, loss:11.315380096435547\n",
            "step:122, loss:4.132222652435303\n",
            "step:123, loss:14.97142219543457\n",
            "step:124, loss:37.38771057128906\n",
            "step:125, loss:1.8726460933685303\n",
            "step:126, loss:8.006114959716797\n",
            "step:127, loss:842.0927734375\n",
            "step:128, loss:3.768392562866211\n",
            "step:129, loss:5.125794410705566\n",
            "step:130, loss:366.36376953125\n",
            "step:131, loss:54.4923210144043\n",
            "step:132, loss:11.351539611816406\n",
            "step:133, loss:4.915632724761963\n",
            "step:134, loss:42.1741828918457\n",
            "step:135, loss:6.926151275634766\n",
            "step:136, loss:5.906702995300293\n",
            "step:137, loss:28.020809173583984\n",
            "step:138, loss:11.142565727233887\n",
            "step:139, loss:5.305266380310059\n",
            "step:140, loss:18.803329467773438\n",
            "step:141, loss:151.7952117919922\n",
            "step:142, loss:5.761301517486572\n",
            "step:143, loss:2.9513509273529053\n",
            "step:144, loss:216.04928588867188\n",
            "step:145, loss:5.14355993270874\n",
            "step:146, loss:9.427997589111328\n",
            "step:147, loss:14.592313766479492\n",
            "step:148, loss:4.081214904785156\n",
            "step:149, loss:3.6739816665649414\n",
            "step:150, loss:17.844655990600586\n",
            "step:151, loss:25.402557373046875\n",
            "step:152, loss:238.54359436035156\n",
            "step:153, loss:5.163183212280273\n",
            "step:154, loss:85.4525146484375\n",
            "step:155, loss:9.473066329956055\n",
            "step:156, loss:2529.946044921875\n",
            "step:157, loss:78.68885040283203\n",
            "step:158, loss:89.91656494140625\n",
            "step:159, loss:678.6255493164062\n",
            "step:160, loss:3.5052385330200195\n",
            "step:161, loss:56.65599822998047\n",
            "step:162, loss:11828.9208984375\n",
            "step:163, loss:4.612326145172119\n",
            "step:164, loss:3.4656152725219727\n",
            "step:165, loss:3.2301952838897705\n",
            "step:166, loss:4.283609867095947\n",
            "step:167, loss:75.68760681152344\n",
            "step:168, loss:2.823918342590332\n",
            "step:169, loss:3.953308343887329\n",
            "step:170, loss:22.369300842285156\n",
            "step:171, loss:192.86566162109375\n",
            "step:172, loss:7156.0380859375\n",
            "step:173, loss:4.221559524536133\n",
            "step:174, loss:32.848533630371094\n",
            "step:175, loss:31.16507339477539\n",
            "step:176, loss:11.982122421264648\n",
            "step:177, loss:1737.2015380859375\n",
            "step:178, loss:4.9295573234558105\n",
            "step:179, loss:56.43901443481445\n",
            "step:180, loss:38.07968521118164\n",
            "step:181, loss:69.18630981445312\n",
            "step:182, loss:204.48770141601562\n",
            "step:183, loss:5.158292770385742\n",
            "step:184, loss:18.875320434570312\n",
            "step:185, loss:300.95355224609375\n",
            "step:186, loss:4.6075358390808105\n",
            "step:187, loss:10.394355773925781\n",
            "step:188, loss:78.82164001464844\n",
            "step:189, loss:6.157682418823242\n",
            "step:190, loss:22.376989364624023\n",
            "step:191, loss:9.02584171295166\n",
            "step:192, loss:46.279884338378906\n",
            "step:193, loss:7.673239707946777\n",
            "step:194, loss:23.76946258544922\n",
            "step:195, loss:6.248080253601074\n",
            "step:196, loss:5.565554618835449\n",
            "step:197, loss:2.3741958141326904\n",
            "step:198, loss:17.20077896118164\n",
            "step:199, loss:11.107812881469727\n",
            "step:200, loss:10.940827369689941\n",
            "step:201, loss:7.346583366394043\n",
            "step:202, loss:20452.78125\n",
            "27\n",
            "-----\n",
            "step:0, loss:3.118496894836426\n",
            "step:1, loss:36.28771209716797\n",
            "step:2, loss:13.081808090209961\n",
            "step:3, loss:7075.37109375\n",
            "step:4, loss:2515.6455078125\n",
            "step:5, loss:12.372661590576172\n",
            "step:6, loss:27.141891479492188\n",
            "step:7, loss:3.719677209854126\n",
            "step:8, loss:4.170811653137207\n",
            "step:9, loss:26.809452056884766\n",
            "step:10, loss:10.960395812988281\n",
            "step:11, loss:86.22634887695312\n",
            "step:12, loss:7.353510856628418\n",
            "step:13, loss:8.18358039855957\n",
            "step:14, loss:23.640554428100586\n",
            "step:15, loss:19.75836181640625\n",
            "step:16, loss:6.823151111602783\n",
            "step:17, loss:122.99234008789062\n",
            "step:18, loss:8.473087310791016\n",
            "step:19, loss:20.90262222290039\n",
            "step:20, loss:8.310185432434082\n",
            "step:21, loss:44.33012008666992\n",
            "step:22, loss:24.12353515625\n",
            "step:23, loss:80.9880599975586\n",
            "step:24, loss:52.624961853027344\n",
            "step:25, loss:1023.9481811523438\n",
            "step:26, loss:12.890636444091797\n",
            "step:27, loss:6.590305328369141\n",
            "step:28, loss:5.832221031188965\n",
            "step:29, loss:7.456992149353027\n",
            "step:30, loss:76.36466979980469\n",
            "step:31, loss:8.787036895751953\n",
            "step:32, loss:1.4103378057479858\n",
            "step:33, loss:50.95376968383789\n",
            "step:34, loss:5.561127185821533\n",
            "step:35, loss:45.604129791259766\n",
            "step:36, loss:604.3168334960938\n",
            "step:37, loss:84.6161117553711\n",
            "step:38, loss:2.457873821258545\n",
            "step:39, loss:8.781792640686035\n",
            "step:40, loss:19.653371810913086\n",
            "step:41, loss:18.301897048950195\n",
            "step:42, loss:161.34915161132812\n",
            "step:43, loss:367.98468017578125\n",
            "step:44, loss:4.3950581550598145\n",
            "step:45, loss:292.431396484375\n",
            "step:46, loss:77.12332153320312\n",
            "step:47, loss:4.2149271965026855\n",
            "step:48, loss:3.5791354179382324\n",
            "step:49, loss:2.5853638648986816\n",
            "step:50, loss:11.892148971557617\n",
            "step:51, loss:7.68792724609375\n",
            "step:52, loss:138.10958862304688\n",
            "step:53, loss:3.674668312072754\n",
            "step:54, loss:11.23637866973877\n",
            "step:55, loss:2.881014823913574\n",
            "step:56, loss:3.4181714057922363\n",
            "step:57, loss:6.272064685821533\n",
            "step:58, loss:145.90469360351562\n",
            "step:59, loss:3.6325786113739014\n",
            "step:60, loss:36.700439453125\n",
            "step:61, loss:998.0574340820312\n",
            "step:62, loss:3.8235538005828857\n",
            "step:63, loss:6.484731674194336\n",
            "step:64, loss:4.620335102081299\n",
            "step:65, loss:2.483236312866211\n",
            "step:66, loss:9.643937110900879\n",
            "step:67, loss:6.411212921142578\n",
            "step:68, loss:2093.264404296875\n",
            "step:69, loss:482.27099609375\n",
            "step:70, loss:12.10744857788086\n",
            "step:71, loss:4.550806045532227\n",
            "step:72, loss:2.371065616607666\n",
            "step:73, loss:8.83239459991455\n",
            "step:74, loss:11893.8720703125\n",
            "step:75, loss:65.22058868408203\n",
            "step:76, loss:32.76011657714844\n",
            "step:77, loss:4.875392436981201\n",
            "step:78, loss:54.855289459228516\n",
            "step:79, loss:16.249839782714844\n",
            "step:80, loss:15.900016784667969\n",
            "step:81, loss:3.5446181297302246\n",
            "step:82, loss:9.24609661102295\n",
            "step:83, loss:234.44052124023438\n",
            "step:84, loss:5.133700370788574\n",
            "step:85, loss:3.9662601947784424\n",
            "step:86, loss:2.9893479347229004\n",
            "step:87, loss:5.625293254852295\n",
            "step:88, loss:148.31800842285156\n",
            "step:89, loss:13.91585922241211\n",
            "step:90, loss:5.715448379516602\n",
            "step:91, loss:40.46324157714844\n",
            "step:92, loss:143.741455078125\n",
            "step:93, loss:23.231416702270508\n",
            "step:94, loss:42.93106460571289\n",
            "step:95, loss:11.228404998779297\n",
            "step:96, loss:63.33538055419922\n",
            "step:97, loss:176.87083435058594\n",
            "step:98, loss:7.37981653213501\n",
            "step:99, loss:19.027761459350586\n",
            "step:100, loss:191.6016082763672\n",
            "step:101, loss:4.085312843322754\n",
            "step:102, loss:7519.3740234375\n",
            "step:103, loss:33.784019470214844\n",
            "step:104, loss:216.43580627441406\n",
            "step:105, loss:162.2179718017578\n",
            "step:106, loss:5.215647220611572\n",
            "step:107, loss:25.49477195739746\n",
            "step:108, loss:10.133726119995117\n",
            "step:109, loss:4.872098922729492\n",
            "step:110, loss:10.492481231689453\n",
            "step:111, loss:92.70392608642578\n",
            "step:112, loss:160.75314331054688\n",
            "step:113, loss:104.62464141845703\n",
            "step:114, loss:28.709102630615234\n",
            "step:115, loss:4.167540550231934\n",
            "step:116, loss:47.18427658081055\n",
            "step:117, loss:78.06745910644531\n",
            "step:118, loss:4.813818454742432\n",
            "step:119, loss:2.3825721740722656\n",
            "step:120, loss:1.735236644744873\n",
            "step:121, loss:2.690338134765625\n",
            "step:122, loss:10.097973823547363\n",
            "step:123, loss:6.123478412628174\n",
            "step:124, loss:15.776824951171875\n",
            "step:125, loss:241.35861206054688\n",
            "step:126, loss:5.787382125854492\n",
            "step:127, loss:3.7633728981018066\n",
            "step:128, loss:7.204958438873291\n",
            "step:129, loss:205.49313354492188\n",
            "step:130, loss:3.353639841079712\n",
            "step:131, loss:13.925915718078613\n",
            "step:132, loss:150.1512451171875\n",
            "step:133, loss:6.267406463623047\n",
            "step:134, loss:4.873570442199707\n",
            "step:135, loss:38.34877395629883\n",
            "step:136, loss:51.71147918701172\n",
            "step:137, loss:102.8425064086914\n",
            "step:138, loss:4.249105930328369\n",
            "step:139, loss:76.87609100341797\n",
            "step:140, loss:3.8882429599761963\n",
            "step:141, loss:4.730059623718262\n",
            "step:142, loss:28.677305221557617\n",
            "step:143, loss:5.411532878875732\n",
            "step:144, loss:10.554073333740234\n",
            "step:145, loss:13.576266288757324\n",
            "step:146, loss:78.02894592285156\n",
            "step:147, loss:167.70565795898438\n",
            "step:148, loss:2.640491008758545\n",
            "step:149, loss:2.4220199584960938\n",
            "step:150, loss:16.60063362121582\n",
            "step:151, loss:9.835821151733398\n",
            "step:152, loss:4244.078125\n",
            "step:153, loss:48.62112045288086\n",
            "step:154, loss:8.594366073608398\n",
            "step:155, loss:7.20637321472168\n",
            "step:156, loss:2.8916568756103516\n",
            "step:157, loss:6.397906303405762\n",
            "step:158, loss:40.79533386230469\n",
            "step:159, loss:2.6859054565429688\n",
            "step:160, loss:3.8397183418273926\n",
            "step:161, loss:75.42472839355469\n",
            "step:162, loss:3.5253989696502686\n",
            "step:163, loss:11.255022048950195\n",
            "step:164, loss:12.285639762878418\n",
            "step:165, loss:237.97557067871094\n",
            "step:166, loss:64.38890075683594\n",
            "step:167, loss:2.795745849609375\n",
            "step:168, loss:669.123291015625\n",
            "step:169, loss:16.359270095825195\n",
            "step:170, loss:2.1186084747314453\n",
            "step:171, loss:11.955469131469727\n",
            "step:172, loss:194.25318908691406\n",
            "step:173, loss:12.819908142089844\n",
            "step:174, loss:2.752145290374756\n",
            "step:175, loss:9.533178329467773\n",
            "step:176, loss:158.16807556152344\n",
            "step:177, loss:27.496156692504883\n",
            "step:178, loss:87.66928100585938\n",
            "step:179, loss:641.5020751953125\n",
            "step:180, loss:13.585134506225586\n",
            "step:181, loss:229.5397186279297\n",
            "step:182, loss:23.873645782470703\n",
            "step:183, loss:14.864776611328125\n",
            "step:184, loss:7.882781028747559\n",
            "step:185, loss:4.512468338012695\n",
            "step:186, loss:1.9863691329956055\n",
            "step:187, loss:18.888036727905273\n",
            "step:188, loss:6.0099263191223145\n",
            "step:189, loss:4.842279434204102\n",
            "step:190, loss:40.96597671508789\n",
            "step:191, loss:9.79046630859375\n",
            "step:192, loss:4.986547470092773\n",
            "step:193, loss:37.833229064941406\n",
            "step:194, loss:1445.16748046875\n",
            "step:195, loss:283.9822998046875\n",
            "step:196, loss:10.037202835083008\n",
            "step:197, loss:3.8068222999572754\n",
            "step:198, loss:3.456094741821289\n",
            "step:199, loss:760.0135498046875\n",
            "step:200, loss:9.830423355102539\n",
            "step:201, loss:42.21607971191406\n",
            "step:202, loss:2.027921676635742\n",
            "28\n",
            "-----\n",
            "step:0, loss:100.0833969116211\n",
            "step:1, loss:7.856540203094482\n",
            "step:2, loss:6.497276782989502\n",
            "step:3, loss:295.72845458984375\n",
            "step:4, loss:7.790192127227783\n",
            "step:5, loss:8.994102478027344\n",
            "step:6, loss:18.179725646972656\n",
            "step:7, loss:4.203352928161621\n",
            "step:8, loss:41.25089645385742\n",
            "step:9, loss:7.363667011260986\n",
            "step:10, loss:31.76435089111328\n",
            "step:11, loss:2.817417621612549\n",
            "step:12, loss:647.3424072265625\n",
            "step:13, loss:1.8229119777679443\n",
            "step:14, loss:23.701873779296875\n",
            "step:15, loss:73.41902923583984\n",
            "step:16, loss:2.276458978652954\n",
            "step:17, loss:9.213607788085938\n",
            "step:18, loss:44.6180534362793\n",
            "step:19, loss:71.7210922241211\n",
            "step:20, loss:11.77180004119873\n",
            "step:21, loss:3.8412513732910156\n",
            "step:22, loss:13.331159591674805\n",
            "step:23, loss:68.6920166015625\n",
            "step:24, loss:81.45577239990234\n",
            "step:25, loss:22.651914596557617\n",
            "step:26, loss:4.15966272354126\n",
            "step:27, loss:4299.63427734375\n",
            "step:28, loss:7.355743408203125\n",
            "step:29, loss:49.02729034423828\n",
            "step:30, loss:9.40097427368164\n",
            "step:31, loss:161.873046875\n",
            "step:32, loss:37.19994354248047\n",
            "step:33, loss:634.6256713867188\n",
            "step:34, loss:5.961479187011719\n",
            "step:35, loss:25.430137634277344\n",
            "step:36, loss:4.671390056610107\n",
            "step:37, loss:3.7147350311279297\n",
            "step:38, loss:113.23055267333984\n",
            "step:39, loss:12.789051055908203\n",
            "step:40, loss:8.40047836303711\n",
            "step:41, loss:2.9415674209594727\n",
            "step:42, loss:2.646235227584839\n",
            "step:43, loss:11922.1875\n",
            "step:44, loss:3.5295000076293945\n",
            "step:45, loss:457.521728515625\n",
            "step:46, loss:58.73023986816406\n",
            "step:47, loss:4.783565998077393\n",
            "step:48, loss:43.416358947753906\n",
            "step:49, loss:74.03473663330078\n",
            "step:50, loss:12.274606704711914\n",
            "step:51, loss:7055.45751953125\n",
            "step:52, loss:38.45707321166992\n",
            "step:53, loss:244.28695678710938\n",
            "step:54, loss:4.476561546325684\n",
            "step:55, loss:4.3189849853515625\n",
            "step:56, loss:25.848451614379883\n",
            "step:57, loss:14.004223823547363\n",
            "step:58, loss:26.20594024658203\n",
            "step:59, loss:16.7408390045166\n",
            "step:60, loss:40.14026641845703\n",
            "step:61, loss:5.790193557739258\n",
            "step:62, loss:742.3091430664062\n",
            "step:63, loss:236.63720703125\n",
            "step:64, loss:84.64745330810547\n",
            "step:65, loss:2.324711322784424\n",
            "step:66, loss:3.384194850921631\n",
            "step:67, loss:48.88686752319336\n",
            "step:68, loss:4.529613971710205\n",
            "step:69, loss:3.250023603439331\n",
            "step:70, loss:3.0797529220581055\n",
            "step:71, loss:5.739320278167725\n",
            "step:72, loss:2.995151996612549\n",
            "step:73, loss:347.2585144042969\n",
            "step:74, loss:7.59073543548584\n",
            "step:75, loss:3.112544059753418\n",
            "step:76, loss:8.556459426879883\n",
            "step:77, loss:3.2349600791931152\n",
            "step:78, loss:79.51133728027344\n",
            "step:79, loss:143.9319610595703\n",
            "step:80, loss:142.77391052246094\n",
            "step:81, loss:21.11324119567871\n",
            "step:82, loss:79.00202178955078\n",
            "step:83, loss:8.60539722442627\n",
            "step:84, loss:17.650325775146484\n",
            "step:85, loss:13.900751113891602\n",
            "step:86, loss:183.79762268066406\n",
            "step:87, loss:6.140995025634766\n",
            "step:88, loss:5.656698226928711\n",
            "step:89, loss:6.217721939086914\n",
            "step:90, loss:13.449517250061035\n",
            "step:91, loss:29.413311004638672\n",
            "step:92, loss:29.02056121826172\n",
            "step:93, loss:6.522469520568848\n",
            "step:94, loss:222.86756896972656\n",
            "step:95, loss:1659.0552978515625\n",
            "step:96, loss:31.75191879272461\n",
            "step:97, loss:50.45951461791992\n",
            "step:98, loss:110.23448944091797\n",
            "step:99, loss:51.898284912109375\n",
            "step:100, loss:3.359556198120117\n",
            "step:101, loss:2.4409308433532715\n",
            "step:102, loss:3.0760693550109863\n",
            "step:103, loss:70.30390930175781\n",
            "step:104, loss:7151.96728515625\n",
            "step:105, loss:13.259113311767578\n",
            "step:106, loss:5.730164527893066\n",
            "step:107, loss:52.97446060180664\n",
            "step:108, loss:191.64002990722656\n",
            "step:109, loss:4.028896331787109\n",
            "step:110, loss:61.03964614868164\n",
            "step:111, loss:6.12534761428833\n",
            "step:112, loss:9.584187507629395\n",
            "step:113, loss:5.697638511657715\n",
            "step:114, loss:4.26326847076416\n",
            "step:115, loss:7.69599723815918\n",
            "step:116, loss:346.18743896484375\n",
            "step:117, loss:41.2541389465332\n",
            "step:118, loss:55.67610549926758\n",
            "step:119, loss:3.1443734169006348\n",
            "step:120, loss:150.51080322265625\n",
            "step:121, loss:377.55767822265625\n",
            "step:122, loss:5.929442405700684\n",
            "step:123, loss:4.117990493774414\n",
            "step:124, loss:3.378049373626709\n",
            "step:125, loss:1.7615654468536377\n",
            "step:126, loss:216.7138214111328\n",
            "step:127, loss:10.770971298217773\n",
            "step:128, loss:6.586828231811523\n",
            "step:129, loss:2.792104721069336\n",
            "step:130, loss:4.5418291091918945\n",
            "step:131, loss:11.310580253601074\n",
            "step:132, loss:3.9802918434143066\n",
            "step:133, loss:5.177890300750732\n",
            "step:134, loss:76.0923843383789\n",
            "step:135, loss:9.296338081359863\n",
            "step:136, loss:7.039644718170166\n",
            "step:137, loss:2510.03466796875\n",
            "step:138, loss:3.573302745819092\n",
            "step:139, loss:1026.290771484375\n",
            "step:140, loss:32.043006896972656\n",
            "step:141, loss:31.01278305053711\n",
            "step:142, loss:6.492778778076172\n",
            "step:143, loss:15.93748664855957\n",
            "step:144, loss:6.236783981323242\n",
            "step:145, loss:4.576747417449951\n",
            "step:146, loss:5.314992427825928\n",
            "step:147, loss:14.556732177734375\n",
            "step:148, loss:2.1782915592193604\n",
            "step:149, loss:7.44706916809082\n",
            "step:150, loss:604.6481323242188\n",
            "step:151, loss:9.32650375366211\n",
            "step:152, loss:19.712017059326172\n",
            "step:153, loss:5.193265438079834\n",
            "step:154, loss:12.075065612792969\n",
            "step:155, loss:98.40233612060547\n",
            "step:156, loss:29.554100036621094\n",
            "step:157, loss:49.276485443115234\n",
            "step:158, loss:4.324668884277344\n",
            "step:159, loss:28.3228759765625\n",
            "step:160, loss:5.333130836486816\n",
            "step:161, loss:13.213517189025879\n",
            "step:162, loss:6.315546035766602\n",
            "step:163, loss:3.4594602584838867\n",
            "step:164, loss:18.938724517822266\n",
            "step:165, loss:16.795072555541992\n",
            "step:166, loss:138.5765380859375\n",
            "step:167, loss:911.1212158203125\n",
            "step:168, loss:3.034067153930664\n",
            "step:169, loss:11.322334289550781\n",
            "step:170, loss:33.450069427490234\n",
            "step:171, loss:2.7517647743225098\n",
            "step:172, loss:5.672674655914307\n",
            "step:173, loss:34.29204177856445\n",
            "step:174, loss:7.24399471282959\n",
            "step:175, loss:50.84292221069336\n",
            "step:176, loss:4.422560691833496\n",
            "step:177, loss:183.59375\n",
            "step:178, loss:7.353370666503906\n",
            "step:179, loss:1555.1793212890625\n",
            "step:180, loss:88.81497192382812\n",
            "step:181, loss:31.58218002319336\n",
            "step:182, loss:96.78514862060547\n",
            "step:183, loss:8.733485221862793\n",
            "step:184, loss:14.153883934020996\n",
            "step:185, loss:1.6724721193313599\n",
            "step:186, loss:25.205177307128906\n",
            "step:187, loss:88.91739654541016\n",
            "step:188, loss:299.63421630859375\n",
            "step:189, loss:4.382458686828613\n",
            "step:190, loss:293.9364929199219\n",
            "step:191, loss:4.08242130279541\n",
            "step:192, loss:23.82716178894043\n",
            "step:193, loss:75.79170227050781\n",
            "step:194, loss:28.13418197631836\n",
            "step:195, loss:6.286544322967529\n",
            "step:196, loss:71.7689208984375\n",
            "step:197, loss:3.981553554534912\n",
            "step:198, loss:49.477603912353516\n",
            "step:199, loss:7.462085723876953\n",
            "step:200, loss:8.12064266204834\n",
            "step:201, loss:4.2117719650268555\n",
            "step:202, loss:4.773235321044922\n",
            "29\n",
            "-----\n",
            "step:0, loss:79.40692138671875\n",
            "step:1, loss:6.883689880371094\n",
            "step:2, loss:3.6564559936523438\n",
            "step:3, loss:56.380462646484375\n",
            "step:4, loss:5.458094120025635\n",
            "step:5, loss:7.4990105628967285\n",
            "step:6, loss:2.819362163543701\n",
            "step:7, loss:49.17204666137695\n",
            "step:8, loss:17.058380126953125\n",
            "step:9, loss:5.710022449493408\n",
            "step:10, loss:3.2119078636169434\n",
            "step:11, loss:4.325281143188477\n",
            "step:12, loss:15.682600975036621\n",
            "step:13, loss:27.46923065185547\n",
            "step:14, loss:145.95130920410156\n",
            "step:15, loss:16.47361183166504\n",
            "step:16, loss:94.11429595947266\n",
            "step:17, loss:10.620986938476562\n",
            "step:18, loss:8.672562599182129\n",
            "step:19, loss:85.47000885009766\n",
            "step:20, loss:8.985733985900879\n",
            "step:21, loss:16.41397476196289\n",
            "step:22, loss:2.761120319366455\n",
            "step:23, loss:4.948957920074463\n",
            "step:24, loss:6.9394707679748535\n",
            "step:25, loss:86.97823333740234\n",
            "step:26, loss:66.63288116455078\n",
            "step:27, loss:12.290205955505371\n",
            "step:28, loss:4.038116455078125\n",
            "step:29, loss:82.25639343261719\n",
            "step:30, loss:77.93789672851562\n",
            "step:31, loss:11.426192283630371\n",
            "step:32, loss:26.94173240661621\n",
            "step:33, loss:21.357995986938477\n",
            "step:34, loss:4.325335502624512\n",
            "step:35, loss:4.423223972320557\n",
            "step:36, loss:157.464111328125\n",
            "step:37, loss:287.1741943359375\n",
            "step:38, loss:179.82559204101562\n",
            "step:39, loss:1.8184559345245361\n",
            "step:40, loss:37.1491813659668\n",
            "step:41, loss:5.5096564292907715\n",
            "step:42, loss:10.461860656738281\n",
            "step:43, loss:5.745118618011475\n",
            "step:44, loss:6.65380859375\n",
            "step:45, loss:14.79186725616455\n",
            "step:46, loss:3.9023091793060303\n",
            "step:47, loss:21.50014877319336\n",
            "step:48, loss:9.938268661499023\n",
            "step:49, loss:23.973819732666016\n",
            "step:50, loss:179.83544921875\n",
            "step:51, loss:11922.3212890625\n",
            "step:52, loss:20.951812744140625\n",
            "step:53, loss:210.46493530273438\n",
            "step:54, loss:2.7408363819122314\n",
            "step:55, loss:7152.3427734375\n",
            "step:56, loss:4.295848846435547\n",
            "step:57, loss:4.4671807289123535\n",
            "step:58, loss:4.3945207595825195\n",
            "step:59, loss:77.24400329589844\n",
            "step:60, loss:109.30594635009766\n",
            "step:61, loss:316.72149658203125\n",
            "step:62, loss:3.329531192779541\n",
            "step:63, loss:36.321205139160156\n",
            "step:64, loss:6.158332347869873\n",
            "step:65, loss:2.1874165534973145\n",
            "step:66, loss:6.579705238342285\n",
            "step:67, loss:3.447091579437256\n",
            "step:68, loss:12.315511703491211\n",
            "step:69, loss:42.49099349975586\n",
            "step:70, loss:43.068359375\n",
            "step:71, loss:10.920869827270508\n",
            "step:72, loss:4.442395210266113\n",
            "step:73, loss:9.753439903259277\n",
            "step:74, loss:19.446916580200195\n",
            "step:75, loss:650.4169921875\n",
            "step:76, loss:6.4619269371032715\n",
            "step:77, loss:5.443464279174805\n",
            "step:78, loss:81.13896179199219\n",
            "step:79, loss:3.785745143890381\n",
            "step:80, loss:2.1133997440338135\n",
            "step:81, loss:316.5823974609375\n",
            "step:82, loss:6.427242279052734\n",
            "step:83, loss:603.8009033203125\n",
            "step:84, loss:3.3038339614868164\n",
            "step:85, loss:24.20646858215332\n",
            "step:86, loss:1457.101318359375\n",
            "step:87, loss:4.236458778381348\n",
            "step:88, loss:4.576346397399902\n",
            "step:89, loss:4.081114768981934\n",
            "step:90, loss:1.8489807844161987\n",
            "step:91, loss:277.9069519042969\n",
            "step:92, loss:3.4798121452331543\n",
            "step:93, loss:8.545143127441406\n",
            "step:94, loss:52.52900695800781\n",
            "step:95, loss:3.8052978515625\n",
            "step:96, loss:30.446887969970703\n",
            "step:97, loss:332.7112121582031\n",
            "step:98, loss:3.7817225456237793\n",
            "step:99, loss:226.03872680664062\n",
            "step:100, loss:13.826980590820312\n",
            "step:101, loss:73.59764099121094\n",
            "step:102, loss:8.229889869689941\n",
            "step:103, loss:7.393403053283691\n",
            "step:104, loss:4.6051836013793945\n",
            "step:105, loss:45.413429260253906\n",
            "step:106, loss:7.962785243988037\n",
            "step:107, loss:12.007820129394531\n",
            "step:108, loss:7.334806442260742\n",
            "step:109, loss:1159.050537109375\n",
            "step:110, loss:4.931145668029785\n",
            "step:111, loss:3.6778717041015625\n",
            "step:112, loss:836.6041259765625\n",
            "step:113, loss:75.70901489257812\n",
            "step:114, loss:14.389642715454102\n",
            "step:115, loss:2498.345458984375\n",
            "step:116, loss:7.315221786499023\n",
            "step:117, loss:752.9459838867188\n",
            "step:118, loss:4.333513259887695\n",
            "step:119, loss:667.3138427734375\n",
            "step:120, loss:138.7586669921875\n",
            "step:121, loss:73.84347534179688\n",
            "step:122, loss:25.630970001220703\n",
            "step:123, loss:25.495525360107422\n",
            "step:124, loss:11.100313186645508\n",
            "step:125, loss:7.829606056213379\n",
            "step:126, loss:9.98804759979248\n",
            "step:127, loss:2.5988686084747314\n",
            "step:128, loss:21.26580810546875\n",
            "step:129, loss:1628.047607421875\n",
            "step:130, loss:10.449319839477539\n",
            "step:131, loss:39.148746490478516\n",
            "step:132, loss:2.3814878463745117\n",
            "step:133, loss:5.493341445922852\n",
            "step:134, loss:3.1426069736480713\n",
            "step:135, loss:7.483977794647217\n",
            "step:136, loss:4.2891693115234375\n",
            "step:137, loss:69.83992004394531\n",
            "step:138, loss:406.10247802734375\n",
            "step:139, loss:2.480085611343384\n",
            "step:140, loss:220.34657287597656\n",
            "step:141, loss:2.9991884231567383\n",
            "step:142, loss:219.24227905273438\n",
            "step:143, loss:8.192084312438965\n",
            "step:144, loss:5.311555862426758\n",
            "step:145, loss:15.686474800109863\n",
            "step:146, loss:212.6973876953125\n",
            "step:147, loss:8.593544960021973\n",
            "step:148, loss:3.9935531616210938\n",
            "step:149, loss:5.098872661590576\n",
            "step:150, loss:2.330676555633545\n",
            "step:151, loss:19.986482620239258\n",
            "step:152, loss:1.7147326469421387\n",
            "step:153, loss:4175.341796875\n",
            "step:154, loss:59.98046875\n",
            "step:155, loss:36.832191467285156\n",
            "step:156, loss:51.84856033325195\n",
            "step:157, loss:79.39024353027344\n",
            "step:158, loss:3.341674327850342\n",
            "step:159, loss:2.2356467247009277\n",
            "step:160, loss:99.53026580810547\n",
            "step:161, loss:4.908648490905762\n",
            "step:162, loss:66.35359191894531\n",
            "step:163, loss:2.8699731826782227\n",
            "step:164, loss:9.34770393371582\n",
            "step:165, loss:13.053390502929688\n",
            "step:166, loss:86.73465728759766\n",
            "step:167, loss:2.487447738647461\n",
            "step:168, loss:8.160741806030273\n",
            "step:169, loss:94.50154876708984\n",
            "step:170, loss:2.6094653606414795\n",
            "step:171, loss:11.286643981933594\n",
            "step:172, loss:196.12037658691406\n",
            "step:173, loss:6.974541664123535\n",
            "step:174, loss:10.551265716552734\n",
            "step:175, loss:3.2654550075531006\n",
            "step:176, loss:49.423065185546875\n",
            "step:177, loss:7209.30615234375\n",
            "step:178, loss:4.563507556915283\n",
            "step:179, loss:6.719842433929443\n",
            "step:180, loss:112.29014587402344\n",
            "step:181, loss:14.735657691955566\n",
            "step:182, loss:6.369804859161377\n",
            "step:183, loss:3.2330617904663086\n",
            "step:184, loss:7.068170547485352\n",
            "step:185, loss:449.96209716796875\n",
            "step:186, loss:2.1061654090881348\n",
            "step:187, loss:305.27630615234375\n",
            "step:188, loss:105.96189880371094\n",
            "step:189, loss:6.796588897705078\n",
            "step:190, loss:7.411338806152344\n",
            "step:191, loss:2.311008930206299\n",
            "step:192, loss:10.775897979736328\n",
            "step:193, loss:2.722513198852539\n",
            "step:194, loss:15.492533683776855\n",
            "step:195, loss:7.28046989440918\n",
            "step:196, loss:5.943267822265625\n",
            "step:197, loss:44.45564651489258\n",
            "step:198, loss:31.8115177154541\n",
            "step:199, loss:2.2708096504211426\n",
            "step:200, loss:23.31025505065918\n",
            "step:201, loss:21.605478286743164\n",
            "step:202, loss:1.8035869598388672\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.utils.clip_grad import clip_grad_norm\n",
        "def train(model, optimizer, scheduler, loss_function, epochs,       \n",
        "          train_dataloader, device, clip_value=2):\n",
        "    for epoch in range(epochs):\n",
        "        print(epoch)\n",
        "        print(\"-----\")\n",
        "        best_loss = 1e10\n",
        "        model.train()\n",
        "        for step, batch in enumerate(train_dataloader):  \n",
        "            batch_inputs, batch_masks, batch_labels = \\\n",
        "                               tuple(b.to(device) for b in batch)\n",
        "            model.zero_grad()\n",
        "            outputs = model(batch_inputs, batch_masks)           \n",
        "            loss = loss_function(outputs.squeeze().float(), \n",
        "                             batch_labels.squeeze().float())\n",
        "            print(f'step:{step}, loss:{loss}') \n",
        "            loss.backward()\n",
        "            clip_grad_norm(model.parameters(), clip_value)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "                \n",
        "    return model\n",
        "model = train(model, optimizer, scheduler, loss_function, epochs, \n",
        "              train_dataloader, device, clip_value=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w1sXTNkL_Wx"
      },
      "source": [
        "#Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwuwB9jjMAbu"
      },
      "outputs": [],
      "source": [
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    for batch in dataloader:\n",
        "        batch_inputs, batch_masks, _ = \\\n",
        "                                  tuple(b.to(device) for b in batch)\n",
        "        with torch.no_grad():\n",
        "            output += model(batch_inputs, \n",
        "                            batch_masks).view(1,-1).tolist()[0]\n",
        "    return output\n",
        "\n",
        "def r2_score(outputs, labels):\n",
        "    labels_mean = torch.mean(labels)\n",
        "    ss_tot = torch.sum((labels - labels_mean) ** 2)\n",
        "    ss_res = torch.sum((labels - outputs) ** 2)\n",
        "    r2 = 1 - ss_res / ss_tot\n",
        "    return r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U_lTZJgMDkC"
      },
      "outputs": [],
      "source": [
        "val_set = val_data_fr[['description', 'demand']]\n",
        "val_set['cleaned_description'] = \\\n",
        "                val_set.description.apply(clean_text)\n",
        "encoded_val_corpus = \\\n",
        "                tokenizer(text=val_set.cleaned_description.tolist(),\n",
        "                          add_special_tokens=True,\n",
        "                          padding='max_length',\n",
        "                          truncation='longest_first',\n",
        "                          max_length=300,\n",
        "                          return_attention_mask=True)\n",
        "val_input_ids = np.array(encoded_val_corpus['input_ids'])\n",
        "val_attention_mask = np.array(encoded_val_corpus['attention_mask'])\n",
        "val_labels = val_set.demand.to_numpy()\n",
        "# val_labels = price_scaler.transform(val_labels.reshape(-1, 1))\n",
        "val_dataloader = create_dataloaders(val_input_ids, \n",
        "                         val_attention_mask, val_labels, batch_size)\n",
        "# y_pred_scaled = predict(model, val_dataloader, device)\n",
        "y_pred = predict(model, val_dataloader, device)\n",
        "y_test = val_set.demand.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ascrE7mRsH1C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "83f175f2-aab3-4842-a4cc-af18cd6a0969"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# n = data[\"comment_clean\"].str.len()\n",
        "nn, bins, patches = plt.hist(y_test,100, facecolor ='b', alpha=0.75)\n",
        "nn, bins, patches = plt.hist(pred,50, facecolor ='r', alpha=0.75)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###mean pisson deviance"
      ],
      "metadata": {
        "id": "1bYRI5Y9CdZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_poisson_deviance(y_true,y_pred):\n",
        "  prediction=y_pred.copy()\n",
        "  prediction = np.array(prediction)\n",
        "  prediction[np.where(prediction<=0)]=1e-10\n",
        "  return mean_poisson_deviance(y_true, prediction)\n",
        "from sklearn.metrics import mean_poisson_deviance\n",
        "custom_poisson_deviance(y_test, y_pred)"
      ],
      "metadata": {
        "id": "UAaaAYe9CcZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWkWjXWRsI3D"
      },
      "source": [
        "#test listing prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "043SKy-VMFop"
      },
      "outputs": [],
      "source": [
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    for batch in dataloader:\n",
        "        batch_inputs, batch_masks = \\\n",
        "                                  tuple(b.to(device) for b in batch)\n",
        "        with torch.no_grad():\n",
        "            output += model(batch_inputs, \n",
        "                            batch_masks).view(1,-1).tolist()[0]\n",
        "    return output\n",
        "\n",
        "def r2_score(outputs, labels):\n",
        "    labels_mean = torch.mean(labels)\n",
        "    ss_tot = torch.sum((labels - labels_mean) ** 2)\n",
        "    ss_res = torch.sum((labels - outputs) ** 2)\n",
        "    r2 = 1 - ss_res / ss_tot\n",
        "    return r2\n",
        "y_pred = predict(model, train_dataloader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o0nFQiV0Zfo"
      },
      "outputs": [],
      "source": [
        "df_de[\"Prediction\"] = y_pred"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "nn, bins, patches = plt.hist(y_pred,1000, facecolor ='r', alpha=0.75)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "6fJXMtAfY_k_",
        "outputId": "9164d3b0-727b-461b-8dc8-71b4450c859b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTq8MUT_M-WC"
      },
      "source": [
        "#fill data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(PATH_GDRIVE_TMP + \"/test_overall_data_with_prediction_WN.csv\")\n",
        "# data = pd.read_csv(PATH_GDRIVE_TMP + \"/test_overall_data_with_prediction_NN.csv\")"
      ],
      "metadata": {
        "id": "6AA8qP5FBlVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZQwgXUsJMI3"
      },
      "outputs": [],
      "source": [
        "data.loc[df_de.index, \"Prediction\"] = df_de[\"Prediction\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "197ACEV3v09r"
      },
      "outputs": [],
      "source": [
        "pickle.dump(data, open(PATH_GDRIVE_TMP + \"/test_data_with_prediction_WN.pkl\", \"wb\"))\n",
        "data.to_csv(PATH_GDRIVE_TMP+\"/test_overall_data_with_prediction_WN.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pickle.dump(data, open(PATH_GDRIVE_TMP + \"/test_overall_data_with_prediction_NN.pkl\", \"wb\"))\n",
        "# data.to_csv(PATH_GDRIVE_TMP+\"/test_overall_data_with_prediction_NN.csv\", index=False)"
      ],
      "metadata": {
        "id": "VhejX6J_cFmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "yQ1Qq0aKadqj",
        "outputId": "49571f1a-1c11-4a8b-b806-c849a409fe00"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# n = data[\"comment_clean\"].str.len()\n",
        "nn, bins, patches = plt.hist(y_pred, 50, facecolor ='g', alpha=0.75)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "NHsLMEXQKaVD",
        "HEe7RJQcKknj",
        "tTsl8fMh-fjF",
        "0W6EIQPmW_LQ",
        "WQ2DEXgBCBWr",
        "WZxB-jj7DRBy",
        "bLRCxvJeBugI",
        "OhIIDTTqBnOu",
        "kdpU0WF7683_"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fa752e777d2840ee97698082e4140bcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d263f897b644e2d916fe26204fff383",
              "IPY_MODEL_dccece74a3284801b04a9b6f83c397e6",
              "IPY_MODEL_f449d83865c04d0480ed380dcbad2dd1"
            ],
            "layout": "IPY_MODEL_ace743ffbbc24c49a3eb61707618323d"
          }
        },
        "0d263f897b644e2d916fe26204fff383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c78274637394660b14d5b053dfc7b40",
            "placeholder": "​",
            "style": "IPY_MODEL_c305748f69bd45afa1e2e3366dc3a1b5",
            "value": "Downloading: 100%"
          }
        },
        "dccece74a3284801b04a9b6f83c397e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09b5e0b7c4134951b7a85404430b3c64",
            "max": 254728,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49307881a6684b1dbfa571e91b132f51",
            "value": 254728
          }
        },
        "f449d83865c04d0480ed380dcbad2dd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_719fd2dafd674483b7aeca0baf6321bf",
            "placeholder": "​",
            "style": "IPY_MODEL_9a581650cb0a4260ab44c83f73f56c93",
            "value": " 255k/255k [00:00&lt;00:00, 1.66MB/s]"
          }
        },
        "ace743ffbbc24c49a3eb61707618323d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c78274637394660b14d5b053dfc7b40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c305748f69bd45afa1e2e3366dc3a1b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09b5e0b7c4134951b7a85404430b3c64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49307881a6684b1dbfa571e91b132f51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "719fd2dafd674483b7aeca0baf6321bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a581650cb0a4260ab44c83f73f56c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c31031556bce4986a8787fe0de219ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2885954765ed496c96768495fce93070",
              "IPY_MODEL_efc178e026454948b3e6b7bbbf932da9",
              "IPY_MODEL_2184e3de3e6f4b60b139c440661a0248"
            ],
            "layout": "IPY_MODEL_f79fa6ff30db44c28e8981af4643b98d"
          }
        },
        "2885954765ed496c96768495fce93070": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a336d0811f2f4ac19affc51779d0ba18",
            "placeholder": "​",
            "style": "IPY_MODEL_297d7ae09c484fe792d69dccf9852672",
            "value": "Downloading: 100%"
          }
        },
        "efc178e026454948b3e6b7bbbf932da9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_caff6a3db8cf4240aa5fc33542796eba",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec8d3e54100c4e28bb0bbb2c327d607f",
            "value": 29
          }
        },
        "2184e3de3e6f4b60b139c440661a0248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b14b2efb86343068f76d7fcdaa3570d",
            "placeholder": "​",
            "style": "IPY_MODEL_752c05b154a44bf29543a2e042be0ded",
            "value": " 29.0/29.0 [00:00&lt;00:00, 1.80kB/s]"
          }
        },
        "f79fa6ff30db44c28e8981af4643b98d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a336d0811f2f4ac19affc51779d0ba18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "297d7ae09c484fe792d69dccf9852672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "caff6a3db8cf4240aa5fc33542796eba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec8d3e54100c4e28bb0bbb2c327d607f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b14b2efb86343068f76d7fcdaa3570d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "752c05b154a44bf29543a2e042be0ded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66cd42e29b2c4d7a9749d747ca437215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d99ae2955daf49c0a1963489c7d733b0",
              "IPY_MODEL_3b634c4cc0e64ea7968f4bd2f8add018",
              "IPY_MODEL_13446137d8a549d2bda6eb547e3f4b08"
            ],
            "layout": "IPY_MODEL_59eb0e5681d344908a0204d2d3ee1ca2"
          }
        },
        "d99ae2955daf49c0a1963489c7d733b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9666abebae44bb4964565ae1475ddc6",
            "placeholder": "​",
            "style": "IPY_MODEL_c1ba18bc392d44259e72a739328c39c1",
            "value": "Downloading: 100%"
          }
        },
        "3b634c4cc0e64ea7968f4bd2f8add018": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e7ddf163b2a4042aa227f5a4cf1d42e",
            "max": 433,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b578bf0c3064d9188eafb2ffcc30b27",
            "value": 433
          }
        },
        "13446137d8a549d2bda6eb547e3f4b08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b5ab496d104418e9b8169aba60560a5",
            "placeholder": "​",
            "style": "IPY_MODEL_8d833ac3873e4575a2f3a56f3a39252d",
            "value": " 433/433 [00:00&lt;00:00, 29.8kB/s]"
          }
        },
        "59eb0e5681d344908a0204d2d3ee1ca2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9666abebae44bb4964565ae1475ddc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1ba18bc392d44259e72a739328c39c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e7ddf163b2a4042aa227f5a4cf1d42e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b578bf0c3064d9188eafb2ffcc30b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b5ab496d104418e9b8169aba60560a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d833ac3873e4575a2f3a56f3a39252d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a00d5a4e623e4e639c0187250c741266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d60c01cb114b4c33a15c23ae7c6dcb3a",
              "IPY_MODEL_b6f46243d9d742e6a6b2da24791dd6a2",
              "IPY_MODEL_ffb72a539e7b4c0e86a3487734359e2b"
            ],
            "layout": "IPY_MODEL_2c4b1a72d94f49aea8ac50978fb5bde7"
          }
        },
        "d60c01cb114b4c33a15c23ae7c6dcb3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62e42dc51f8e4c7e9d949af503e94550",
            "placeholder": "​",
            "style": "IPY_MODEL_f71d0bb6e73a4d309605d29b42dea15a",
            "value": "Downloading: 100%"
          }
        },
        "b6f46243d9d742e6a6b2da24791dd6a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6a8ed8cf5274cb4b61de39a09f3df31",
            "max": 438869143,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_931f40a152344f46ac54acd7709a7b9c",
            "value": 438869143
          }
        },
        "ffb72a539e7b4c0e86a3487734359e2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49c0f9ffe3c24a0bb4d3d5889c31d888",
            "placeholder": "​",
            "style": "IPY_MODEL_57a7e117ead843b4978df4c1ff180cc6",
            "value": " 439M/439M [00:06&lt;00:00, 76.2MB/s]"
          }
        },
        "2c4b1a72d94f49aea8ac50978fb5bde7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62e42dc51f8e4c7e9d949af503e94550": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f71d0bb6e73a4d309605d29b42dea15a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6a8ed8cf5274cb4b61de39a09f3df31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "931f40a152344f46ac54acd7709a7b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49c0f9ffe3c24a0bb4d3d5889c31d888": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57a7e117ead843b4978df4c1ff180cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}